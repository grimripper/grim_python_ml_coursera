from ConfigParser import ConfigParser as _ConfigParser
from copy import copy as _copy
from logging import getLogger as _getLogger
from re import compile as _compile
from requests import ConnectionError as _ConnectionError
from uuid import uuid4 as _random_uuid

import datetime as _datetime
import json as _json
import time as _time
import types as _types
import re as _re
import shutil as _shutil
import inspect as _inspect
import os as _os

from graphlab.util.file_util import is_s3_path as _is_s3_path, \
    is_path as _is_path, retry as _retry
import graphlab as _gl
import graphlab.canvas as _canvas
from graphlab.deploy._artifact import Artifact as _Artifact
from . import PREDICTIVE_SERVICE_SCHEMA_VERSION, INTERNAL_PREDICTIVE_OBJECT_NAMES, \
    LAST_SUPPORTED_PREDICTIVE_SERVICE_SCHEMA_VERSION

from _policy import EndpointPolicy as _EndpointPolicy

NODE_LAUNCH_LIMIT = 5

from _system_config import SystemConfig as _SystemConfig
from _predictive_service_environment import PredictiveServiceEnvironment
from _predictive_service_environment import \
    DockerPredictiveServiceEnvironment as _DockerPredictiveServiceEnvironment, \
    LocalPredictiveServiceEnvironment as _LocalPredictiveServiceEnvironment

from _predictive_service_environment import predictive_service_environment_factory
from _predictive_service_environment import PORT_DEFAULT_NUM as _PORT_DEFAULT_NUM
from _model_predictive_object import ModelPredictiveObject
from _predictive_object import PredictiveObject as _PredictiveObject
from _custom_query_predictive_object import CustomQueryPredictiveObject as _CustomQueryPredictiveObject
from _predictive_client import NonExistError as _NonExistError

_logger = _getLogger(__name__)
_name_checker = _compile('^[a-zA-Z0-9-_\ ]+$')

def _is_cache_enabled(status):
    return status.get("cache_state", "enabled") == "enabled"

def _check_endpoint_name(name):
    if not _name_checker.match(name):
        raise ValueError('endpoint name can only contain: a-z, A-Z, 0-9, hyphens and spaces.')

    if name in INTERNAL_PREDICTIVE_OBJECT_NAMES:
        raise ValueError("Cannot name endpoint to \"%s\"" % name)

_VALID_CONFIG_PARAMS = {
    "cache_max_memory_mb": (lambda v: True if isinstance(v, int) and v > 0 else False),
    "cache_ttl_on_update_secs": (lambda v: True if isinstance(v, int) and v >= 0 else False)
}

def _config_params_are_valid(params):
    for param, value in params.items():
        if param not in _VALID_CONFIG_PARAMS:
            _logger.error("Invalid configuration parameter: %s" % param)
            return False
        if not _VALID_CONFIG_PARAMS[param](value):
            _logger.error("Invalid value for configuration parameter %s: %s"
                          % (param, str(value)))
            return False
    return True

def _models_unavailable(status_sf):
    if status_sf['models'].dtype() not in (list, dict):
        msg = 'Status unavailable; '

        if "OutOfService" in {x for x in status_sf["state"].unique()}:
            msg += "nodes are unavailable"
        else:
            msg += "no endpoint deployed in any instance"

        return msg


class PredictiveService(_Artifact):
    """
    Monitor/Manage a running Predictive Service.

    Predictive Service objects should not be instantiated directly, and are
    intended to be created using :func:`graphlab.deploy.predictive_service.create`
    or loaded using :func:`graphlab.deploy.predictive_service.load`.

    See Also
    --------
    graphlab.deploy.predictive_service.create, graphlab.deploy.predictive_service.load

    """

    # State File Config Section Names
    _DEPLOYMENT_SECTION_NAME = 'Predictive Objects Service Versions'
    _PREDICTIVE_OBJECT_DOCSTRING = 'Predictive Objects Docstrings'
    _ENVIRONMENT_SECTION_NAME = 'Environment Info'
    _SERVICE_INFO_SECTION_NAME = 'Service Info'
    _SYSTEM_SECTION_NAME = 'System'
    _META_SECTION_NAME = 'Meta'

    # Directory Names
    _PREDICTIVE_OBJECT_DIR = 'predictive_objects'
    _DEPENDENCIES_DIR = 'dependencies'

    _typename = 'PredictiveService'

    def __repr__(self):
        return self.__str__()

    def __str__(self):
        if not self._environment:
            return "Predictive Service '%s' at '%s' has been terminated." % (self.name, self._state_path)

        ret = ""
        ret += 'Name                  : %s' % self.name + '\n'
        ret += 'State Path            : %s' % self._state_path + '\n'
        ret += 'Description           : %s' % self.description + '\n'
        ret += 'API Key               : %s' % self.api_key + '\n'
        ret += 'CORS origin           : %s' % self._cors_origin + '\n'
        ret += 'Global Cache State    : %s' % self._global_cache_state + '\n'
        if self._environment is not None:
            ret += 'Load Balancer DNS Name: %s' % self._environment.load_balancer_dns_name + '\n'

        ret += "\nDeployed endpoints:\n"
        for (name, po_info) in self._endpoints.iteritems():
            ret += '\tname: %s, version: %s, type: %s, cache: %s, description: %s\n' % \
            (name, po_info['version'], po_info.get('type', 'model'), \
             po_info['cache_state'], po_info['description'])

        if not self._has_pending_changes():
            ret += "\nNo Pending changes.\n"
        else:
            ret += "\nPending changes: \n"
            for (endpoint_name, info) in self._local_changes.iteritems():
                if info['action'] == 'add':
                    ret += '\tAdding: %s, type: %s, description: %s \n' % \
                    (endpoint_name, info.get('type', 'model'), info['description'])
                elif info['action'] == 'update':
                    ret += '\tUpdating: %s, type: %s, description: %s \n' % \
                    (endpoint_name, info.get('type', 'model'), info['description'])
                else:
                    ret += '\tRemoving: "%s"\n' % endpoint_name

        return ret

    @property
    def endpoints(self):
        '''
        Returns all endpoints currently defined in the Predictive Service.
        '''
        return_info = {}
        for (endpoint_name, endpoint_info) in self._endpoints.iteritems():
            return_endpoint_info = endpoint_info.copy()
            if return_endpoint_info.has_key('endpoint_obj'):
                del return_endpoint_info['endpoint_obj']
            del return_endpoint_info['docstring']
            return_info[endpoint_name] = return_endpoint_info
        return return_info

    def get_endpoints(self, endpoint_type = None):
        '''
        Returns all endpoints that are currently registered with this Predictive
        Service, not include pending endpoints.

        Parameters
        -----------
        endpoint_type : 'policy' | 'model' | 'alias', optional
            If given, returns only the endpoint of given type. Otherwise returns
            all endpoints.

        Returns
        --------
        out : SFrame
            an SFrame with one row per endpoint.

        Examples
        --------

            >>> ps.get_endpoints()
            +---------------------+--------+---------------+---------+-----------------------+
            |         name        |  type  |  cache_state  | version |      description      |
            +---------------------+--------+---------------+---------+-----------------------+
            |         blue        | model  |    enabled    |    1    | The model predicts... |
            |         green       | model  |    disabled   |    1    | The model predicts... |
            |  ratings-prediction | alias  | not supported |    3    | Alias for GreedyPoli. |
            |     GreedyPolicy    | policy | not supported |    1    |                       |
            +---------------------+--------+---------------+---------+-----------------------+

        '''
        supported_types = ['policy', 'model', 'alias']
        if (endpoint_type is not None) and (endpoint_type not in supported_types):
            raise ValueError('endpoint_type has to be one of the following: %s' % \
                                        supported_types)

        uris = self._endpoints.keys()
        fields = ['name', 'type', 'cache_state', 'version', 'description']
        field_values  = dict((k,[]) for k in fields)

        for endpoint_name, info in self._endpoints.iteritems():
            if endpoint_type is None or info.get('type', 'model') == endpoint_type:
                for f in fields:
                    if f == 'name':
                        field_values['name'].append(endpoint_name)
                    else:
                        field_values[f].append(info.get(f))

        sf = _gl.SFrame(field_values)
        return_sf = sf[fields]
        return return_sf

    @property
    def _all_endpoints(self):
        '''
        Return all endpoints that are currently registered with a
        Predictive Service, including both deployed and pending endpoints.

        Returns
        --------
        out : dict
            a dictionary with endpoint names as keys and endpoints information
            as values
        '''
        ret = {}

        for (endpoint_name, info) in self._endpoints.iteritems():
            new_info = _copy(info)
            new_info.update({'state':'deployed'})
            ret[endpoint_name] = new_info

        for (endpoint_name, info) in self._local_changes.iteritems():
            if info['action'] != 'remove':
                new_info = _copy(info)
                del new_info['action']
                new_info.update({'state':'pending'})
                ret[endpoint_name] = new_info
            else:
                del ret[endpoint_name]

        return ret

    @property
    def deployed_predictive_objects(self):
        '''
        Return all deployed Predictive Objects for a Predictive Service.

        Returns
        --------
        out : dict
            One entry for each Predictive Object. The keys are the names of the
            objects and the values are the corresponding versions.

        Examples
        --------

        >>> ps.deployed_predictive_objects
            {'recommender_one': 2, 'recommender_two': 1}

        Notes
        ------
        This method will be deprecated, use `get_endpoints` instead
        '''
        self._ensure_not_terminated()

        ret = {}
        for (endpoint_name, endpoint_info) in self._endpoints.iteritems():
            if endpoint_info.get('type', 'model') == 'model':
                info = _copy(endpoint_info)
                ret[endpoint_name] = info

        return ret

    @property
    def pending_changes(self):
        '''
        Return all currently pending updates for a Predictive Service.

        Returns
        --------
        out : dict
            Keys are the changed endpoint names, values are a
            dictionary with two keys: 'action' and 'version'. 'action' is one of
            'add', 'update', or 'remove' and 'version' is the new version
            number or None if action is 'remove'

        Examples
        --------

        >>> ps.pending_changes
        Out:
            {'recommender_one': {'action': 'remove'},
             'recommender_three': {'action': 'add', 'version': 1},
             'recommender_two': {'action': 'update', 'version': 2}}
        '''
        return self._local_changes

    def __init__(self, name, state_path, description, api_key, admin_key,
                 aws_credentials,
                 _new_service=True, cors_origin='',
                 global_cache_state='enabled', system_config=None,
                 port = _PORT_DEFAULT_NUM):
        '''
        Initialize a new Predictive Service object
        Notes
        -----
        Do not call this method directly.
        To create a new Predictive Service, use:
             graphlab.deploy.predictive_service.create(...)
        To load an existing Predictive Service, use
            graphlab.deploy.predictive_service.load(<ps-state-path>)
        '''
        if type(name) != str:
            raise TypeError("Name of Predictive Service needs to be a string")

        self.name = name
        self.description = description
        self.api_key = api_key
        self.admin_key = admin_key
        self.port = port
        if (port < 0 or port > 65535):
            raise ValueError("Port must be within a certain range, 0 to 65535.")
        if (port == 9006 or port == 19006):
            raise ValueError("Port 9006 and 19006 are reserved for cache. Please use another port.")
        self._cors_origin = cors_origin
        self._global_cache_state = global_cache_state
        self._system_config = system_config or _SystemConfig()
        self.aws_credentials = aws_credentials

        self._local_changes = {}
        self._endpoints = {}

        self._state_path = state_path
        self._session = _gl.deploy._default_session

        if _new_service:
            # Init version data
            self._revision_number = 0
            self._schema_version = PREDICTIVE_SERVICE_SCHEMA_VERSION

            # No environment yet. A launched one must be attached later.
            self._environment = None

    def describe(self, name):
        '''
        Prints doc string for the endpoint

        Parameters
        -----------
        name : str
            The name of the endpoint
        '''
        self._ensure_not_terminated()

        if name not in self._endpoints.keys():
            raise ValueError("Cannot find a endpoint with name '%s'." % name)

        print self._endpoints[name]['docstring']

    def _validate_policy_endpoint(self, policy):
        '''
        Validate the given policy. All models used in the policy
        should have been deployed

        Parameters
        -----------
        policy : EndpointPolicy
            The policy to be added to current Predictive Service
        '''
        if not isinstance(policy, _EndpointPolicy):
            raise TypeError('Policy must be a type of graphlab.deploy.EndpointPolicy')

        # We require that all models involved in the policy have been deployed.
        policy_models = policy.get_models()
        deployed_models = self.deployed_predictive_objects.keys()
        for m in policy_models:
            if m not in deployed_models:
                raise RuntimeError('Model "%s" used in policy has not been deployed.' \
                ' Use deployed_predictive_objects to get all deployed models.' \
                    % (m))

    def _get_nodes_status(self):
        '''
        Gets the status of each node in the environment.

        Examples
        --------

        >>> ps._get_nodes_status()
        +--------------------+---------------+-----------+
        |      dns_name      |  instance_id  |   state   |
        +--------------------+---------------+-----------+
        | ec2-54-186-64- ... |  i-00c5a70c   | InService |
        | ec2-54-148-131 ... |  i-01c5a70d   | InService |
        | ec2-54-186-135 ... |  i-03c5a70f   | InService |
        +--------------------+---------------+-----------+
        '''
        self._ensure_not_terminated()

        # get host information
        host_status = []
        for host in self.get_status():
            state = host['state'] if host['state'] == 'InService' else host['state'] + " - " + host['reason']
            if not host.get('cache'):
                cache_state = 'Disabled'
            else:
                cache_state = 'Healthy' if host.get('cache',{}).get('healthy') else 'Unhealthy'
            host_status.append({
                'instance_id':host['id'],
                'dns_name':host['dns_name'],
                'state': state,
                'cache': cache_state})

        if len(host_status) == 0:
            _logger.info('No nodes in the Predictive Service')
            return None
        else:
            return _gl.SArray(data=host_status).unpack('')

    def get_predictive_objects_status(self):
        '''
        Get Predictive Objects deployment status in all nodes.

        Returns
        -------
        out : SFrame
            The status for all deployed Predictive Objects represented
            in a SFrame.

        Examples
        --------

        >>> ps.get_predictive_objects_status()
        +-----------------+------------------+-------------------------+-------------------------+
        |       name      | expected version |     node.i-29397b26     |     node.i-2a397b25     |
        +-----------------+------------------+-------------------------+-------------------------+
        | Image Predictor |        1         | 1 (Loaded successfully) | 1 (Loaded successfully) |
        | Book Recommender|        2         | 2 (Loaded successfully) | 2 (Loaded successfully) |


        Notes
        ------

        This method will be deprecated., Use `get_status('endpoint')` instead
        '''
        all_status = self._get_endpoints_status()

        # filter out only for model
        return all_status[all_status['type'] == 'model']

    def _get_endpoints_status(self):
        '''
        Get realtime endpoint status from  all nodes.

        Returns
        -------
        out : SFrame
            The status for all deployed Predictive Objects represented
            in a SFrame.

        Examples
        --------

        >>> ps.get_predictive_objects_status()
        +-----------------+------------------+-------------------------+-------------------------+
        |       name      | expected version |     node.i-29397b26     |     node.i-2a397b25     |
        +-----------------+------------------+-------------------------+-------------------------+
        | Image Predictor |        1         | 1 (Loaded successfully) | 1 (Loaded successfully) |
        | Book Recommender|        2         | 2 (Loaded successfully) | 2 (Loaded successfully) |

        '''
        self._ensure_not_terminated()

        result = self.get_status()
        if len(result) == 0:
            _logger.info('No nodes in the Predictive Service')
            return None

        # result is in the following format
        # [{'dns_name': u'ec2-54-191-36-143.us-west-2.compute.amazonaws.com',
        #   'id': u'i-29397b26',
        #   'models': [{u'description': u'None',
        #     u'name': u'Image Predictor',
        #     u'status': u'Loaded successfully',
        #     u'version': 1}],
        #   'reason': u'N/A',
        #   'state': u'InService'},
        #   {...}
        #   ]
        sf = _gl.SArray(result)

        # unpack the dictionary to different columns, result looks like:
        # +------------+------------+--------+-----------+--------------------------------+
        # |  dns_name  |     id     | reason |   state   |             models             |
        # +------------+------------+--------+-----------+--------------------------------+
        # | ec2-54 ... | i-29397b26 |  N/A   | InService | {'status': 'Loaded success ... |
        # | ec2-54 ... | i-29397b26 |  N/A   | InService | {'status': 'Loaded success ... |
        sf = sf.unpack(column_name_prefix="")

        # in case no model in any instance, there is no way to infer type of 'models' column
        if sf['models'].dtype() == float:
            _logger.info('No endpoint deployed in any instance')
            return

        sf = sf.stack('models', 'models')

        # expand the "models" column, results looks like:
        # +------------+------------+--------+-----------+------------+------------+------------+---------+
        # |  dns_name  |     id     | reason |   state   | descri ... |    name    |   status   | version |
        # +------------+------------+--------+-----------+------------+------------+------------+---------+
        # | ec2-54 ... | i-29397b26 |  N/A   | InService |    None    | Image  ... | Loaded ... |    1    |
        # | ec2-54 ... | i-29397b26 |  N/A   | InService | finds  ... | shorte ... | Loaded ... |    3    |
        sf = sf.remove_column("reason")

        msg = _models_unavailable(sf)
        if msg:
            _logger.info(msg)
            return

        sf = sf.unpack('models', column_name_prefix = "")

        # Add expected and actual versions
        # filter out non modles
        expected = self._endpoints
        sf['expected version'] = sf.apply(lambda x: expected[x['name']]['version'] if expected.has_key(x['name']) else None )
        sf['type'] = sf.apply(lambda x: expected[x['name']].get('type','model') if expected.has_key(x['name']) else None )
        sf['actual'] = sf.apply(lambda x: '%s (%s)' % (x['version'], x['status']))

        # pivot the table by model and show one row for each model
        return sf \
            .groupby('name', {
                'state':_gl.aggregate.CONCAT('id', 'actual'),
                'type': _gl.aggregate.SELECT_ONE('type'),
                'expected version':_gl.aggregate.SELECT_ONE('expected version')}) \
            .unpack('state', column_name_prefix='node')

    def _get_cache_status(self):
        '''
        Get the cache status for all deployed Predictive Objects on each node.

        Examples
        --------

        >>> ps._get_cache_status()
        +-----------------+-------------------------+-------------------------+
        |       name      |     node.i-29397b26     |     node.i-2a397b25     |
        +-----------------+-------------------------+-------------------------+
        | Image Predictor |       1 (Enabled)       |      1 (Enabled)        |
        | Book Recommender|       0 (Disabled)      |      0 (Disabled)       |

        '''
        self._ensure_not_terminated()

        self.deployed_predictive_objects
        result = self.get_status()
        if len(result) == 0:
            _logger.info('No nodes in the Predictive Service')
            return None

        # result is in the following format
        # [{'dns_name': u'ec2-54-191-36-143.us-west-2.compute.amazonaws.com',
        #   'id': u'i-29397b26',
        #   'models': [{u'description': u'None',
        #     u'name': u'Image Predictor',
        #     u'cache_enabled': True,
        #     u'status': u'Loaded successfully',
        #     u'version': 1}],
        #   'reason': u'N/A',
        #   'state': u'InService'},
        #   {...}
        #   ]
        sf = _gl.SArray(result)

        # unpack the dictionary to different columns, result looks like:
        # +------------+------------+--------+-----------+--------------------------------+
        # |  dns_name  |     id     | reason |   state   |             models             |
        # +------------+------------+--------+-----------+--------------------------------+
        # | ec2-54 ... | i-29397b26 |  N/A   | InService | {'status': 'Loaded success ... |
        # | ec2-54 ... | i-29397b26 |  N/A   | InService | {'status': 'Loaded success ... |
        sf = sf.unpack(column_name_prefix="")

        # in case no model in any instance, there is no way to infer type of
        # 'models' column and the next stack operation would raise a TypeError
        msg = _models_unavailable(sf)
        if msg:
            _logger.info(msg)
            return

        sf = sf.stack('models', 'models')

        # expand the "models" column, results looks like:
        # +------------+------------+--------+-----------+------------+------------+------------+---------+
        # |  dns_name  |     id     | reason |   state   | descri ... |    name    |   status   | version |
        # +------------+------------+--------+-----------+------------+------------+------------+---------+
        # | ec2-54 ... | i-29397b26 |  N/A   | InService |    None    | Image  ... | Loaded ... |    1    |
        # | ec2-54 ... | i-29397b26 |  N/A   | InService | finds  ... | shorte ... | Loaded ... |    3    |
        sf = sf.remove_column('reason')

        msg = _models_unavailable(sf)
        if msg:
            _logger.info(msg)
            return

        sf = sf.unpack('models', column_name_prefix="")

        # Add the cache status (string)
        sf['cache_str'] = sf['cache_enabled'].apply(lambda x : "%s" % ("Enabled" if x == 1 else "Disabled"))
        sf['cache'] = sf.apply(lambda x : "%s (%s)" % (x['cache_enabled'], x['cache_str']))
        # pivot the table by model and show one row for each model
        return sf \
            .groupby('name', {'state':_gl.aggregate.CONCAT('id', 'cache')}) \
            .unpack('state', column_name_prefix='node')


    def get_status(self, view = None):
        '''
        Gets the status of the current Predictive Service.

        If parameter 'view' is given, will return an SFrame that shows specific
        aspect of the status.

        Parameters
        -----------
        view : 'cache'|'node'|'endpoint'

        Returns
        --------
        out : list | SFrame
            If `view` is not given, then return a list with each element being
            a dictionary containing status information for one node. Available
            keys are: ['dns_name', 'id', 'models', 'state']

            If 'view' is 'cache', returns an SFrame showing cache status for each
            Predictive Object deployed in each node. Columns in SFrame include
            ['name', 'node.instance-id1', 'node.instance-id2', etc.], see sample
            output below. If no Predictive Object deployed yet, then returns None.

            If 'view' is 'node', returns an SFrame showing node status for each
            node in current Predictive Service. Columns in SFrame include
            ['dns_name', 'instance-id', 'state']

            If 'view' is 'endpoint', returns an SFrame showing status for each
            deployed endpoint in each node. Columns in SFrame include
            ['name', 'expected version', 'type', 'node.instance-id1', 'node.instance-id2',
            etc...], see sample output below. If no Predictive Object deployed yet,
            then returns None.

        Examples
        ---------

        This is a sample output for a Predictive Service with two nodes and two
        Predictive Objects deployed. With one cache enabled and one disabled.
        (Note: output omitted some SFrame header information.)

        >>> ps.get_status()
            [{'dns_name': 'ec2-54-148-131...',
              'id': 'i-2a397b25',
              'state': 'InService',
              'models': [
                {
                    u'cache_enabled': True,
                    u'description': u'',
                    u'name': u'Image Predictor',
                    u'status': u'Loaded successfully',
                    u'version': 1
                }, {
                    u'cache_enabled': False,
                    u'description': u'',
                    u'name': u'Book Recommender',
                    u'status': u'Loaded successfully',
                    u'version': 2
               }],
              },
             {'dns_name': 'ec2-54-186-64-...',
              'id': 'i-29397b26',
              'state': 'InService',
              'models': [
                {
                    u'cache_enabled': True,
                    u'description': u'',
                    u'name': u'Image Predictor',
                    u'status': u'Loaded successfully',
                    u'version': 1
                }, {
                    u'cache_enabled': False,
                    u'description': u'',
                    u'name': u'Book Recommender',
                    u'status': u'Loaded successfully',
                    u'version': 2
               }],
              },
            ]


        >>> ps.get_status(view='cache')
        +-----------------+-------------------------+-------------------------+
        |       name      |     node.i-29397b26     |     node.i-2a397b25     |
        +-----------------+-------------------------+-------------------------+
        | Image Predictor |       1 (Enabled)       |      1 (Enabled)        |
        | Book Recommender|       0 (Disabled)      |      0 (Disabled)       |
        +-----------------+-------------------------+-------------------------+

        >>> ps.get_status(view='node')
        +--------------------+---------------+-----------+-----------+
        |      dns_name      |  instance_id  |   state   |   cache   |
        +--------------------+---------------+-----------+-----------+
        | ec2-54-148-131 ... |  i-2a397b25   | InService | Healthy   |
        | ec2-54-186-64- ... |  i-29397b26   | InService | Healthy   |
        +--------------------+---------------+-----------+-----------+


        >>> ps.get_status(view='endpoint')
        +-----------------+------------------+--------+-------------------------+-------------------------+
        |       name      | expected version |  type  |     node.i-29397b26     |     node.i-2a397b25     |
        +-----------------+------------------+--------+-------------------------+-------------------------+
        | Image Predictor |        1         | model  | 1 (Loaded successfully) | 1 (Loaded successfully) |
        | Book Recommender|        2         | policy | 2 (Loaded successfully) | 2 (Loaded successfully) |
        +-----------------+------------------+--------+-------------------------+-------------------------+

        '''
        self._ensure_not_terminated()

        if not view:
            return self._environment.get_status()

        if view == 'cache':
            return self._get_cache_status()

        elif view == 'node':
            return self._get_nodes_status()

        elif view == 'model':
            # This may be deprecated
            return self.get_predictive_objects_status()

        elif view == 'endpoint':
            return self._get_endpoints_status()
        else:
            raise ValueError("Supported views are: ['cache', 'node', 'model']," \
                "'%s' is not supported." % view)

    def set_query_timeout(self, timeout = 10):
        '''
        Set query timeout in seconds

        Parameters
        -----------
        timeout : int
            The timeout (in seconds) of the query to the Predictive Service.

        Examples
        ---------

            >>> deployment.set_query_timeout(30)

        '''
        self._ensure_not_terminated()

        if timeout <= 0 or not isinstance(timeout, int):
            raise ValueError('"timeout" value has to be a positive integer in seconds.')

        self.query_timeout = timeout

    def query(self, name, **kwargs):
        '''
        Submits a query to the deployed endpoint with the name specified
        by `name`.

        Parameters
        ----------
        name : str
            The name of the endpoint to query

        kwargs : kwargs
            The keyword arguments passed into endpoint query method.
            For Custom Predictive Object, the kwargs depends on custom query
            signature, for built-in Model Predictive Object, the kwargs has to
            be in the following format:

                >>> deployment.query(name, method=<method>, data = <data-dictionary>)

        Returns
        -------
        out : dict
            For successful requests, returns a dictionary containing the
            following fields:

            - "uuid": a unique request identifer string
            - "response": the actual result of the query returned by the model
            - "version": an integer corresponding to the version number of the
            model that generated the result

        See Also
        ---------
        test_query

        Examples
        ---------
        If there is a Predictive Object named 'book recommender' and we want to
        recommend some books for users, the following query may be used:

          >>> deployment.query('book recommender',
          ...                   method = 'recommend',
          ...                   data = {
          ...                       'users': [
          ...                           {'user_id':12345, 'book_id':2},
          ...                           {'user_id':12346, 'book_id':3},
          ...                       ],
          ...                       'k': 5
          ...                   }
          ...                 )


        '''
        self._ensure_not_terminated()

        if not self._endpoints.has_key(name):
            raise ValueError('Endpoint "%s" can not be found.' % name)

        try:
            timeout = self.query_timeout if hasattr(self, 'query_timeout') else 10
            return self._environment.query(name, self.api_key, timeout=timeout, **kwargs)
        except _NonExistError:
            return "Endpoint '%s' can not be found. If you just deployed "\
                "the endpoint, it may take a short while for all Predictive " \
                "Service nodes to be up-to-date. Please use get_predictive_objects_status() " \
                "to get most current state." % name

    def test_query(self, name, **kwargs):
        '''
        Used for simulating querying the given endpoint locally without
        going to actual Predictive Service nodes.

        This method queries the specified endpoint with the input and
        validates the end to end query to this endpoint locally.

        Unlike the `query` method, which submits a query to a deployed
        Predictive Service cluster, this method runs entirely locally.

        The purpose is to validate the endpoint logic before actually
        deploying the endpoint to production.

        Parameters
        ----------
        name : str
            The name of the endpoint to query

        kwargs : Any
            The keyword arguments passed into endpoint query method

        Returns
        -------
        out : dict
            For successful requests, returns a dictionary containing the
            following fields:

            - "uuid": a unique request identifer string
            - "response": the actual result of the query returned by the model
            - "version": an integer corresponding to the version number of the
            model that generated the result

        See Also
        ---------
        query

        Examples
        ---------
        If there is a deployed endpoint named 'book recommender' and we want to
        validates our input query with this Predictive object, the following query may
        be used:

          >>> deployment.test_query('book recommender',
          ...                        method = 'recommend',
          ...                        data = {
          ...                          'users': [
          ...                                {'user_id':12345, 'book_id':2},
          ...                                {'user_id':12346, 'book_id':3},
          ...                          ],
          ...                          'k': 5
          ...                        }
          ...                      )

        '''
        self._ensure_not_terminated()

        uuid = unicode(_random_uuid())

        # simulate server side logging
        def custom_logger(**kwargs):
            _logger.info(_json.dumps({'uri':name, 'version':1, 'uuid': uuid, 'data':kwargs}))

        # serialize then deserialize data
        try:
            validated_data = _json.loads(_json.dumps(kwargs))
        except:
            raise ValueError("input data is not JSON serializable.")
        _logger.info("Input data serializable.")

        unpickled_po = None

        # load endpoint from local changes
        temp_dir = _gl.util._make_temp_filename(prefix='predictive_object_')
        try:
            # Load the endpoint object
            endpoint_name = name
            while True:
                endpoint_info = self._all_endpoints.get(endpoint_name)

                if not endpoint_info:
                    raise ValueError('Cannot find endpoint with name "%s".' % endpoint_name)

                _logger.info("Trying to serve %s" % endpoint_name)

                if endpoint_info.get('type', 'model') == 'policy':
                    policy = self._load_endpoint(endpoint_name)
                    endpoint_name = policy.select_model(**kwargs)
                    _logger.info('Redirecting to %s' % endpoint_name)
                elif endpoint_info.get('type', 'model') == 'alias':
                    endpoint_name = endpoint_info['endpoint_obj']
                    _logger.info('Redirecting to %s' % endpoint_name)
                else:
                    break

            # model, persist local and then load it
            if endpoint_info['state'] == 'pending':
                po = endpoint_info['endpoint_obj']
                po.save(temp_dir)  # pickle and save
                unpickled_po = _PredictiveObject.load(temp_dir, endpoint_info['schema_version'])
            else:
                unpickled_po = self._load_endpoint(endpoint_name)

            # test the query
            if isinstance(unpickled_po, _CustomQueryPredictiveObject):
                unpickled_po.custom_query.log = custom_logger

            result = unpickled_po.query(**validated_data)

            # serialize the result
            try:
                _json.dumps(result)
            except:
                raise ValueError("Result cannot be serialized to JSON format, please check "
                                "your Model or your Custom Predictive Object to make sure "
                                " the result is in JSON format.")
            _logger.info("Query results serializable.")

            return {u'uuid':uuid, u'response':result, u'version':1}
        finally:
            if _os.path.isdir(temp_dir):
                _shutil.rmtree(temp_dir)

    def feedback(self, request_id, **kwargs):
        '''
        Submits feedback data corresponding to a particular query result.

        The feedback method is intended to allow PredictiveService clients to
        associate arbitrary data with the results returned from a particular
        query. For example, you could store click data (user ID, clicked result
        rank, etc.) in order to evaluate your model and train the next version.

        Parameters
        ----------
        request_id : str
            The unique request identifer associated with a particular query and
            result (ie. the ```uuid``` field).

        kwargs : kwargs
            Named attribute and value pairs to associate with a query and result.
            The benefit of keyword arguments is their flexibility; users can
            record any arbitrary key-value pairs with a particular query-result
            pair for future analysis.

        See Also
        ---------
        get_feedback_logs, get_query_logs, get_result_logs

        Examples
        ---------
        The following examples assume the existence of an autocomplete model,
        where the model is queried to produce suggested completions for a user's
        text input. The user then either accepts or rejects the suggestion,
        which serves as the basis for a feedback call.

          >>> deployment.feedback('e8f13b17-173a-402d-835d-cc816eba626f',
                                  search_term='anomoly',
                                  suggested='anomaly detection',
                                  suggestion_accepted=True)

          >>> deployment.feedback('b7a620c8-14a9-4d66-a6a0-9c3308dffe1b',
                                  search_term='accomodat',
                                  suggested='accommodations',
                                  suggestion_accepted=False,
                                  next_search_term='accommodate')

        When it comes time to train a new model, you could use rows in the
        feedback logs as training data. For illustration purposes, the example
        above contains more data than is strictly required. You could exclude
        some of the fields above (eg. 'search_term', 'suggested') from the
        feedback, since it could just as easily be collected from the query and
        result logs, which can be joined with the feedback log on the ```uuid```
        field.

        '''
        self._ensure_not_terminated()

        try:
            timeout = self.query_timeout if hasattr(self, 'query_timeout') else 10
            result = self._environment.feedback(request_id, self.api_key,
                                                timeout=timeout, **kwargs)
            result.raise_for_status()
            return result.json()
        except Exception as e:
            _logger.error("Error submitting feedback: %s" % e.message)

    def alias(self, alias, existing_endpoint_name, description = None):
        '''
        Create a new endpoint to redirect to an existing endpoint, or update an
        existing alias to point to a different existing endpoint.

        You cannot update an exiting endpoint that is not already an alias.

        Parameters
        -----------
        alias : str
            The new endpoint name or an existing alias endpoint name.

        existing_endpoint_name : str
            A name of an existing endpoint to redirect the alias to.

        '''
        if self._schema_version < 5:
            raise RuntimeError('"Alias" functionality is not supported for '
                'Predictive Service with schema version 4.')

        if not isinstance(alias, basestring):
            raise TypeError('"alias" parameter has to be a string')

        # check for invalid PO names
        _check_endpoint_name(alias)

        if not description:
            description = 'Alias for %s' % existing_endpoint_name

        if not self._endpoints.has_key(existing_endpoint_name):
            raise ValueError("Endpoint '%s' does not exist." % existing_endpoint_name)

        # Can only overwrite existing alias
        existing_endpoint = self._endpoints.get(alias)
        if existing_endpoint:
            if existing_endpoint.get('type', 'model') != 'alias':
                raise RuntimeError('Name "%s" is already in use by another endpoint.' % alias)
            action = 'update'
            version = existing_endpoint['version'] + 1
        else:
            action = 'add'
            version = 1

        self._local_changes[alias] = {
            'action'        : action,
            'version'       : version,
            'schema_version': 1,
            'description'   : description,
            'type'          : 'alias',
            'endpoint_obj'  : existing_endpoint_name,
            'cache_state'   : 'disabled',
            'docstring'     : 'Alias for %s' % existing_endpoint_name
        }

        _logger.info("New alias '%s' added, use apply_changes() to deploy all pending changes, or continue other modification." % alias)

    def add(self, name, obj, description=''):
        '''
        Add a new endpoint to the Predictive Service.

        The endpoint may be backed by one of the following:

           * a GraphLab Create model
           * a user-defined function, or
           * a group of models tied together by an `EndpointPolicy`

        This operation will not take effect until `apply_changes` is called.

        Parameters
        ----------
        name : str
            A unique identifier for the endpoint. This name is used
            when querying the Predictive Service.

        obj :  `graphlab.Model` | str | func | `EndpointPolicy`
            The actual object that backs the endpoint.

            If 'obj' is a str, it is interpreted as a path (local/S3/HDFS) to where
            a GraphLab Create Model is persisted.

            If 'obj' is a user-defined function, the query can have any signature.
            However both input and output of the query need to be JSON serializable

            If 'obj' is an EndpointPolicy, it has to be one of the following
            supported policies:

                * ProbabilityPolicy
                * EpsilonGreedyPolicy

        description : str, optional
            The description for the endpoint

        See Also
        --------
        apply_changes, update, remove, endpoints, get_endpoints

        Notes
        -----
        This operation will not take effect until `apply_changes` is called.

        Examples
        --------
        To add a GraphLab Create Model:

            >>> model = graphlab.recommender.create(...)
            >>> ps.add('recommender', model)

        To add a GraphLab Create Model saved locally / S3:

            >>> ps.add('recommender', '~/saved_models/recommender')
            >>> ps.add('recommender2', 's3://model-archive/recommender')

        To add an endpoint backed by a user-defined function:

            >>> def add(a, b):
            >>>     return a + b
            >>>
            >>> ps.add('add_two_numbers', add)
            >>> ps.apply_changes()
            >>> ps.query('add_two_numbers', a = 1, b = 2)

        To add a user-defined function that has GraphLab Create model dependency:

            >>> recommender = graphlab.recommender.create(...)
            >>> def my_recommender(user_id):
            >>>
            >>>     if not isinstance(user_id, str):
            >>>         raise ValueError('"user_id" has to be a string')
            >>>     return recommender.recommend([user_id])
            >>>
            >>> ps.add('custom-recommender', my_recommender)

            >>> ps.apply_changes()
            >>> ps.query('custom-recommender', user_id = 'abc')

        If the custom query depends on other Python package(s), the dependendent
        package(s) may be declared using @required_packages decorator:

            >>> from graphlab.deploy import required_packages
            >>> @required_packages(["names", ...])
            >>> def generate_names(num_names):
            >>>     import names
            >>>     return gl.SArray([names.get_full_name() for i in range(num_items)])

        To add an endpoint backed by `EndpointPolicy`:

            >>> from graphlab.deploy import ProbabilityPolicy
            >>> p = ProbabilityPolicy({'m1':0.8, 'm2':0.2})
            >>> ps.add('endpoint-name', p)
        '''
        self._ensure_not_terminated()

        if not isinstance(name, basestring):
            raise TypeError("Endpoint name must be a string or unicode")

        if name == None or name == '':
            raise TypeError("Endpoint name cannot be empty")

        if name in self._endpoints.keys():
            raise ValueError("There is already and endpoint with name '%s'." % name)

        self._add_or_update_endpoint('add', name, obj, description)

        _logger.info("Endpoint '%s' is added. Use apply_changes() to deploy "
            "all pending changes, or continue with other modification." % (name))

    def update(self, name, obj, description = ''):
        '''
        Updates an existing endpoint.
        Checkout `add` for more detailed examples of adding different endpoints.

        Parameters
        ----------
        name : str
            The endpoint name

        obj :  `graphlab.Model` | str | func | `EndpointPolicy`
            The actual object that backs the endpoint.

            If 'obj' is a str, it is interpreted as a path (local/S3/HDFS) to where
            a GraphLab Create Model is persisted.

            If 'obj' is a user-defined function, the query can have any signature.
            But input and output of the query needs to be JSON serializable

            If 'obj' is an EndpointPolicy, it has to be one of the following
            supported policy:

                * ProbabilityPolicy
                * EpsilonGreedyPolicy

        description : str, optional
            The description for the endpoint

        See Also
        --------
        apply_changes, add, remove, endpoints, get_endpoints

        Notes
        -----
        This operation will not take effect until `apply_changes` is called.

        Examples
        --------
        To update a endpoint named 'recommender':

            >>> new_recommender = graphlab.recommender.create(...)
            >>> ps.update('recommender', new_recommender)

        '''
        self._ensure_not_terminated()

        if not self._endpoints.has_key(name):
            if not self.pending_changes.has_key(name):
                raise ValueError("Cannot find a predictive object with name '%s'." % name)
            action = 'add'
        else:
            action = 'update'

        self._add_or_update_endpoint(action, name, obj, description)

        _logger.info("Endpoint '%s' is updated. Use apply_changes to deploy all pending "
            "changes, or continue other modification." % (name))

    def _add_or_update_endpoint(self, action, name, obj, description):
        '''
        Add or update an endpoint.

        Parameters
        ----------
        action : str
            The action to do, can be 'add' or 'update'

        name : str
            Endpoint name to add or update

        obj :  | `graphlab.Model` | str | func | `EndpointPolicy`
            Object that backs the endpoint

        description : str
            Description of the endpoint

        '''
        # check for invalid PO names
        _check_endpoint_name(name)

        endpoint_object = None
        endpoint_type = 'model'
        if isinstance(obj, _types.FunctionType):
            endpoint_object = self._create_custom_po(obj, description)

        elif isinstance(obj, _gl.Model) or \
             isinstance(obj, _gl.CustomModel) or \
             _is_path(obj):
            endpoint_object = ModelPredictiveObject(model=obj)

        elif isinstance(obj, _PredictiveObject):
            endpoint_object = obj

        elif isinstance(obj, _EndpointPolicy):
            # policy is supported only in version 5 of Predictive Service
            if self._schema_version < 5:
                raise RuntimeError('Endpoint policy is not supported for Predictive Service'
                    ' with schema version less than 5.')

            self._validate_policy_endpoint(obj)
            endpoint_object = obj
            endpoint_type = 'policy'
        else:
            raise TypeError("'obj' parameter can only be one of the following: "
                            "an instance of GraphLab Create Model, "
                            "a path to a saved GraphLab Create Model, "
                            "an user-defined function, "
                            "or an instance of EndpointPolicy")

        # extract doc string
        docstring = endpoint_object.get_doc_string().strip()

        if action == 'add':
            # disable cache for policy by default
            cache_state = self._global_cache_state if endpoint_type == 'model' else 'disabled'
            version = 1
        else:
            version = self._endpoints[name]['version'] + 1
            cache_state = self._endpoints[name]['cache_state']

        self._local_changes[name] = {
            'action'        : action,
            'version'       : version,
            'schema_version': endpoint_object.schema_version,
            'description'   : description,
            'type'          : endpoint_type,
            'cache_state'   : cache_state,
            'endpoint_obj'  : endpoint_object,
            'docstring'     : docstring
        }

    def remove(self, name):
        '''
        Remove the alias, custom method, or model that has the specified name.
        This operation will not take effect until `apply_changes` is called.

        Parameters
        ----------
        name : str
            The name of the endpoint to be removed.

        Notes
        -----
        This could fail if the endpoint does not exist, or if the endpoint is
        in use by other endpoints like alias or policy. To check all endpoints
        that are depending on this endpoint, use `get_endpoint_dependencies`.

        See Also
        --------
        apply_changes, add, update, get_endpoint_dependencies
        '''
        self._ensure_not_terminated()

        # Cannot remove an endpoint if it is referenced by other endpoint
        dependent_endpoints = self.get_endpoint_dependencies(name)
        if len(dependent_endpoints) > 0:
            raise RuntimeError("Cannot remove endpoint '%s' because it is used by "
            'the following endpoint(s): %s' % (name, dependent_endpoints))

        found = False
        if self._endpoints.has_key(name):
            self._local_changes[name] = {'action': 'remove'}
            found = True

        elif self._local_changes.has_key(name):
            del self._local_changes[name]
            found = True

        if not found:
            raise ValueError('Endpoint "%s" does not exist.' % name)

        _logger.info("Endpoint '%s' is scheduled to be removed, use apply_changes() "
            "to deploy all pending changes, or continue with other modification." % name)

    def get_endpoint_dependencies(self, endpoint_name = None):
        '''
        Get all endpoints that depend on the given endpoint.

        parameter
        ----------
        endpoint_name : str, optional
            The name of the endpoint to find dependendent endpoints. If not given,
            find all dependendent endpoints for all endpoints

        Returns
        --------
        dependendent endpoints : [str] | dict(str, list[str])
            if endpoint_name is given, returns a list of endpoint names that depend
            on the given endpoint.

            if endpoint_name is not given, returns a dictionary where key is the
            endpoint name and value is a list of endpoints that depend on the
            endpoint specified by the key.

        '''
        all_dependencies = {}

        def update_dependency(src, dependent):
            if all_dependencies.has_key(src):
                all_dependencies[src].append(dependent)
            else:
                all_dependencies[src] = [dependent]

        for (current_name, endpoint_info) in self._all_endpoints.iteritems():
            endpoint_type = endpoint_info.get('type', 'model')
            if endpoint_type == 'model':
                continue
            elif endpoint_type == 'policy':
                policy = self._load_endpoint(current_name)
                models = policy.get_models()

                for m in models:
                    update_dependency(m, current_name)
            elif endpoint_type == 'alias':
                alias_src = endpoint_info['endpoint_obj']
                update_dependency(alias_src, current_name)

        if endpoint_name:
            return all_dependencies.get(endpoint_name, [])
        else:
            return all_dependencies

    def apply_changes(self):
        '''
        Applies all pending changes to this Predictive Service deployment.

        If apply_changes returns success, then all pending changes have been
        staged to the predictive objects path associated with the Predictive
        Service. Each of the nodes in the Predictive Service will pick up
        the state eventually.

        Use get_status() to check the status of each node.
        Use get_status('endpoint') to check the loading status of all endpoints.

        See Also
        --------
        add, update, remove, alias, pending_changes
        '''
        self._ensure_not_terminated()

        if not self._has_pending_changes():
            _logger.info("There are no pending changes. No action is taken.")
            return

        # persist endpoints to service state
        self._persist_pending_changes()

        # update the service state.ini file
        self._save_state()

        # Now update my local state
        self._update_local_state()

        # Tell nodes to update themselves
        try:
            self._environment.poke()
        except _ConnectionError as e:
            _logger.warn("Unable to connect to running Predictive Service: %s" %
                         (e.message))

    def clear_pending_changes(self):
        '''
        Clears all changes which have not been applied.

        Clears all actions done using `add`, `update`, `remove` and 'alias',
        since the last time `apply_changes` was called.
        '''
        self._ensure_not_terminated()

        _logger.info('Clearing all pending changes.')
        self._local_changes = {}

    def save_client_config(self, file_path, predictive_service_cname):
        '''
        Create the config file that can be used by applications accessing the
        Predictive Service.

        The file is stored in an .ini format, with the specific information
        necessary to query this deployment.

        Parameters
        ----------
        file_path : str
            The path where the config file will be saved.

        predictive_service_cname : str
            The CNAME for the Predictive Service endpoint. It is *highly recommended*
            that all client connect through a CNAME record that you have created.
            If this value is set to None, the "A" record will be used.
            Using the "A" record is only advisable for testing, since this value
            may change over time.
        '''
        self._ensure_not_terminated()

        out = _ConfigParser(allow_no_value=True)
        out.optionxform = str
        out.add_section(PredictiveService._SERVICE_INFO_SECTION_NAME)
        out.set(PredictiveService._SERVICE_INFO_SECTION_NAME, 'api key', self.api_key)

        if hasattr(self._environment, 'certificate_name') and self._environment.certificate_name:
            out.set(PredictiveService._SERVICE_INFO_SECTION_NAME, 'verify certificate',
                    not(self._environment.certificate_is_self_signed))
            schema = 'https://'
        else:
            schema = 'http://'

        if predictive_service_cname:
            out.set(PredictiveService._SERVICE_INFO_SECTION_NAME, 'endpoint',
                    schema + predictive_service_cname)
        else:
            _logger.warn('Creating client config using the "A" Record name. These types of records'
                         ' often change over time. It is strongly recommended that you create a'
                         ' CNAME record for this load balancer and use that. It would be crazy'
                         ' not to use CNAME records for all production purposes.')
            out.set(PredictiveService._SERVICE_INFO_SECTION_NAME, 'endpoint',
                    schema + self._environment.load_balancer_dns_name)

        with open(file_path, 'w') as f:
            out.write(f)

    def _recover_cache(self, max_retries=3):
        '''
        Restart the cache and restore the cache state to the cache settings
        in place before the restart.

        Parameters
        ----------
        max_retries : int
            The number of times we should re-attempt to enable the cache if it
            doesn't start cleanly.
        '''
        def toggle(is_enabled, name):
            if is_enabled:
                self._environment.cache_enable(name)
            else:
                self._environment.cache_disable(name)

        # capture cache state before disabling/enabling
        cache_state = {name: _is_cache_enabled(status) for
                       (name, status) in self._endpoints.items()}

        # disable the cache
        self._environment.cache_disable(None)
        self._environment.cache_enable(None, True)

        # if failed, retry as many as num_retries attempts
        num_retries = 0
        healthy = "healthy"

        while not self._environment._is_cache_ok(healthy) and \
          num_retries < max_retries:
            _logger.warn("Cluster cache unstable; restarting")
            self._environment.cache_enable(None, True)
            num_retries += 1
            _time.sleep(2)

        # reset the cache state
        if self._environment._is_cache_ok(healthy):
            for name, is_enabled in cache_state.items():
                toggle(is_enabled, name)
        else:
            _logger.error("Predictive service cluster is unstable: cache is " \
                          "unhealthy")

    def _remove_nodes(self, instance_ids, cache_restart=True):
        '''
        Terminate one or more nodes in a Predictive Service cluster by their
        instance IDs, and remove them from the cluster.

        Parameters
        ----------
        cache_restart : bool
            A boolean indicating whether we should restart the cache. This
            defaults to True. Restarting the cache results in the clearing of
            any existing cached results. However, after adding or removing
            nodes from the cluster, the cache will not function without a
            restart.
        '''
        if not isinstance(instance_ids, list):
            instance_ids = [instance_ids]

        if not all([type(i) in [str, unicode] for i in instance_ids]):
            raise TypeError('The "instance_ids" parameter must be either a ' \
                            'string or a list of strings')

        instance_ids = list(set(instance_ids)) # ensure unique elements

        node_ids = {x.id for x in self._environment._get_all_hosts()}
        num_nodes_deployed = len(node_ids)

        # ensure we're not removing any nodes outside this predictive service
        if not all([instance_id in node_ids for instance_id in instance_ids]):
            raise ValueError("Cannot remove a node that is not in this " \
                             "PredictiveService cluster.")

        if num_nodes_deployed == len(instance_ids):
            cache_restart = False # there will be no instances left
            _logger.warn("This action will terminate all of the nodes in " \
                         "your predictive service cluster.")

        self._environment.terminate_instances(instance_ids)

        if self._global_cache_state == "enabled" and cache_restart:
            self._recover_cache()

    def remove_nodes(self, instance_ids):
        '''
        Terminate one or more nodes in a Predictive Service cluster by their
        instance IDs, and remove them from the cluster.

        This is useful in the event of a node failure. You can query the
        Predictive Service status with `get_status`. If there are unresponsive
        nodes, capture their instance IDs and pass them as the sole parameter
        to this function.

        Parameters
        ----------
        instance_ids : list[str] | str
            The instance IDs for the nodes to terminate.

        See Also
        --------
        get_status, add_nodes, cache_enable

        Notes
        -----
        Calling this method will clear out any existing data in the distributed
        cache.

        Examples
        --------
        To remove a node in the current Predictive Service:

          >>> print ps.get_status('node')
          +---------+-------------------------------+-------------+-----------+
          |  cache  |            dns_name           | instance_id |   state   |
          +---------+-------------------------------+-------------+-----------+
          | Healthy | ec2-12-11-12-121.us-west-2... |  i-4399eb8a | InService |
          | Healthy | ec2-21-21-24-221.us-west-2... |  i-ccc4663a | InService |
          +---------+-------------------------------+-------------+-----------+
          [2 rows x 4 columns]
          >>> ps.remove_nodes(['i-ccc4663a'])
          >>> print ps.get_status('node')
          +---------+-------------------------------+-------------+-----------+
          |  cache  |            dns_name           | instance_id |   state   |
          +---------+-------------------------------+-------------+-----------+
          | Healthy | ec2-12-11-12-121.us-west-2... |  i-4399eb8a | InService |
          +---------+-------------------------------+-------------+-----------+
          [1 rows x 4 columns]

        '''
        self._raise_error_if_on_premise_ps()
        self._ensure_not_terminated()
        self._remove_nodes(instance_ids)

    def _add_nodes(self, num_nodes, instance_type=None,
                   security_group_name=None, CIDR_rule=None, tags=None,
                   cache_restart=True):
        '''
        Add one or more nodes to a Predictive Service Cluster.

        Parameters
        ----------
        num_nodes : int
            The number of additional nodes to add to the cluster. The maximum
            number of nodes that can be launched at one time is 5. If the
            `num_nodes` parameter exceeds that limit, a ValueError is thrown.

        instance_type : str, optional
            The type of instance to launch. If specified, this must match the
            instance type used when the service was originally launched.

        security_group : str, optional
            The name of the security group for the EC2 instance to use. If
            specified, this must match the instance type used when the service
            was originally launched.

        CIDR_rule : string or list[string], optional
            The Classless Inter-Domain Routing rule(s) to use for the instance.
            Useful for restricting the IP Address Range for a client. Default is
            no restriction. If you specify CIDR_rule(s), you must also specify a
            security group to use.

        tags : dict, optional
            A dictionary containing the name/value tag pairs to be assigned to
            the instance. If you want to create only a tag name, the value for
            that tag should be the empty string (i.e. ''). In addition to these
            specified tags, a 'GraphLab' tag will also be assigned.

        cache_restart : bool
            A boolean indicating whether we should restart the cache. This
            defaults to True. Restarting the cache results in the clearing of
            any existing cached results. However, after adding or removing
            nodes from the cluster, the cache will not function without a
            restart.
        '''

        if num_nodes <= 0:
            _logger.warn("The num_nodes parameter must have a positive " \
                         "value for this method to have any effect.")
            return

        if num_nodes > NODE_LAUNCH_LIMIT:
            raise ValueError("You cannot launch more than %d nodes. If this " \
                             "limit is problematic, please contact " \
                             "support@dato.com" % NODE_LAUNCH_LIMIT)

        if not instance_type:
            attrs = self._environment._get_instance_attributes()
            instance_type = attrs['instance_type']
            security_group_name = attrs['security_group_name']
            tags = attrs['tags']

        self._environment.add_instances(self._state_path, num_nodes,
                                        instance_type, security_group_name,
                                        tags, CIDR_rule=CIDR_rule,
                                        additional_port_to_open=self.port)

        if self._global_cache_state == "enabled" and cache_restart:
            self._recover_cache()

    def add_nodes(self, num_nodes):
        '''
        Add one or more nodes to a Predictive Service Cluster.

        Parameters
        ----------
        num_nodes : int
            The number of additional nodes to add to the cluster. The maximum
            number of nodes that can be launched at one time is 5. If the
            `num_nodes` parameter exceeds that limit, a ValueError is thrown.

        Notes
        -----
        Calling this method will clear out any existing data in the distributed
        cache.

        Examples
        --------
        To add three new nodes to the current Predictive Service:

          >>> ps.add_nodes(3)

        '''
        self._raise_error_if_on_premise_ps()
        self._ensure_not_terminated()
        self._add_nodes(num_nodes)

    def replace_nodes(self, instance_ids):
        '''
        Replace each of the nodes specified in `instance_ids` with a new node.

        For each node terminated, add a new one in its place. After launching
        new nodes, terminate those specified by the `instance_ids` parameter.

        Parameters
        ----------
        instance_ids : list[str] | str
            The instance IDs for the nodes to terminate.

        See Also
        --------
        add_nodes, remove_nodes

        Notes
        -----
        Calling this method will clear out any existing data in the distributed
        cache.

        Examples
        --------
        To replace a node in the current Predictive Service with a new node:

          >>> print ps.get_status('node')
          +---------+-------------------------------+-------------+-----------+
          |  cache  |            dns_name           | instance_id |   state   |
          +---------+-------------------------------+-------------+-----------+
          | Healthy | ec2-12-11-12-121.us-west-2... |  i-4399eb8a | InService |
          | Healthy | ec2-21-21-24-221.us-west-2... |  i-ccc4663a | InService |
          +---------+-------------------------------+-------------+-----------+
          [2 rows x 4 columns]
          >>> ps.replace_nodes(['i-ccc4663a'])

        '''
        self._raise_error_if_on_premise_ps()
        self._ensure_not_terminated()

        if not isinstance(instance_ids, list):
            instance_ids = [instance_ids]

        if not all([type(i) in [str, unicode] for i in instance_ids]):
            raise TypeError('The "instance_ids" parameter must be either a ' \
                            'string or a list of strings')

        num_nodes_deployed = len(self._environment._get_all_hosts())
        num_replacements = len(instance_ids)

        if num_replacements == num_nodes_deployed:
            _logger.warn("Replacing all %d existing nodes in cluster" \
                         % num_replacements)

        # bring up replacement nodes
        _logger.info("Adding %d nodes to cluster", num_replacements)

        num_replaced = 0
        while num_replaced < num_replacements:
            n = min(num_replacements - num_replaced, NODE_LAUNCH_LIMIT)
            self._add_nodes(n, cache_restart=False)
            num_replaced += n

        # terminate specified nodes
        self._remove_nodes(instance_ids, cache_restart=False)

        # try re-enabling cache
        if self._global_cache_state == "enabled":
            self._recover_cache()

        # check that cache is healthy, and warn if not
        expected_cache_status = "healthy" if self._global_cache_state else "disabled"
        if self._environment._is_cache_ok(expected_cache_status):
            _logger.info("Predictive service cluster is healthy")

    def terminate_service(self, remove_logs=False, remove_state=True):
        '''
        Terminates the Predictive Service deployment.

        This will terminate all EC2 hosts and delete the load balancer. The
        Predictive Service object is not usable after terminate_service is
        called.

        **This operation can not be undone.**

        Parameters
        ----------
        remove_logs : bool
            Delete all logs associated with this Predictive Service Deployment remotely.

        remove_state : bool
            Delete all state data associated with this Predictive Service Deployment remotely.
        '''

        self._raise_error_if_on_premise_ps()
        self._ensure_not_terminated()

        # Terminate hosts and delete load balancer.
        self._environment.terminate(remove_logs)

        # Remove state file, stored endpoints and dependencies
        if remove_state:
            self._environment.remove_state(self._state_path,
                                            PredictiveService._DEPENDENCIES_DIR,
                                            PredictiveService._PREDICTIVE_OBJECT_DIR)

        # Remove environment object
        self._environment = None

        # Delete local predictive service endpoint
        _gl.deploy.predictive_services.delete(self.name)

    def _get_latest_state(self):
        '''
        Update the endpoints from the latest remote state file.

        Parameters
        ----------
        config : config, optional
            The configuration file given manually to update to the latest state.

        Returns
        -------
        out : dict
            Returns a summary of what has changed. There is one entry in the dictionary for each
            endpoint that has changed. The keys are the name of the endpoint. The
            values are tuples: (model_version_number, path_to_new_version).
        '''
        # TODO: make sure we're not "ahead" of remote state file due to eventual consistency

        self._ensure_not_terminated()

        remote_cfg = PredictiveServiceEnvironment._get_state_from_file(self._state_path, self.aws_credentials)

        if remote_cfg.has_option(PredictiveService._META_SECTION_NAME, 'Schema Version'):
            self._schema_version = remote_cfg.getint(PredictiveService._META_SECTION_NAME, 'Schema Version')
        else:
            # if missing schema version, fail
            raise ValueError('Invalid remote state configuration file; missing ' \
                             '"Schema Version"')

        # if remote schema version is 1, fail
        if self._schema_version == 1:
            raise ValueError("The Predictive Service that you are trying to " \
                             "load is running version 1, which is no " \
                             "longer supported. Please re-create your " \
                             "Predictive Service using your current version " \
                             "of GraphLab Create.")

        # if remote schema version is > than ours, fail
        if self._schema_version > PREDICTIVE_SERVICE_SCHEMA_VERSION:
            raise ValueError("Your GraphLab Create only supports Predictive " \
                             "Services with schema version up to '%s', " \
                             "while the Predictive Service you are trying " \
                             "to load has schema version '%s'. Please " \
                             "upgrade GraphLab Create to the latest " \
                             "version." % (PREDICTIVE_SERVICE_SCHEMA_VERSION,
                                           self._schema_version))

        # warn if there's a version mismatch
        if self._schema_version != PREDICTIVE_SERVICE_SCHEMA_VERSION:
            _logger.warn("There is a version mismatch between what is " \
                         "expected by this version of GraphLab Create " \
                         "(%d) and your deployed Predictive Service (%d)" \
                         ". Some client functionality may not be supported " \
                         "by the server." % (
                             PREDICTIVE_SERVICE_SCHEMA_VERSION,
                                   self._schema_version))

        # update global cache metadata and cors_origin metadata
        if remote_cfg.has_option(PredictiveService._SERVICE_INFO_SECTION_NAME, 'CORS Origin'):
            self._cors_origin = remote_cfg.get(PredictiveService._SERVICE_INFO_SECTION_NAME, 'CORS Origin')
        if remote_cfg.has_option(PredictiveService._SERVICE_INFO_SECTION_NAME, 'Global Cache State'):
            self._global_cache_state = remote_cfg.get(PredictiveService._SERVICE_INFO_SECTION_NAME, 'Global Cache State')

        updated_deployment_versions = {}
        endpoint_names = remote_cfg.options(PredictiveService._DEPLOYMENT_SECTION_NAME)
        for endpoint_name in endpoint_names:
            endpoint_info = _json.loads(remote_cfg.get(PredictiveService._DEPLOYMENT_SECTION_NAME,
                                                    endpoint_name))

            updated_deployment_versions[endpoint_name] = endpoint_info

        if remote_cfg.has_section(PredictiveService._PREDICTIVE_OBJECT_DOCSTRING):
            for endpoint_name in endpoint_names:
                docstring = remote_cfg.get(PredictiveService._PREDICTIVE_OBJECT_DOCSTRING,
                                                     endpoint_name)
                updated_deployment_versions[endpoint_name]['docstring'] = docstring.decode('string_escape')

        if remote_cfg.has_section(PredictiveService._ENVIRONMENT_SECTION_NAME):
            # Create and attach the environment.
            environment_info = dict(remote_cfg.items(PredictiveService._ENVIRONMENT_SECTION_NAME))
            if hasattr(self._environment, 'aws_credentials'):
                environment_info['aws_credentials'] = self._environment.aws_credentials
            self._environment = predictive_service_environment_factory(environment_info)
        else:
            self._environment = None

        diff = {}
        for (endpoint_name, endpoint_info) in updated_deployment_versions.items():
            existing_endpoint = self._endpoints.get(endpoint_name)
            if (existing_endpoint is None) or \
                        endpoint_info['version'] != existing_endpoint['version']:
                # Either a new endpoint or new endpoint version
                path_to_new_version = self._get_predictive_object_save_path(endpoint_name,
                                                                            endpoint_info['version'])
                endpoint_type = endpoint_info.get('type', 'model')
                diff[endpoint_name] = (endpoint_type, endpoint_info['version'], path_to_new_version)
            else:
                # not change to endpoint, keep the endpoint_obj so that its state
                # can be maintained
                updated_deployment_versions[endpoint_name].update(
                    {'endpoint_obj': existing_endpoint.get('endpoint_obj')})

        # add removed models too
        for endpoint_name in self._endpoints.keys():
            if endpoint_name not in updated_deployment_versions.keys():
                endpoint_type = self._endpoints[endpoint_name].get('type', 'model')
                diff[endpoint_name] = (endpoint_type, None, None)

        self._revision_number = remote_cfg.getint(PredictiveService._META_SECTION_NAME, 'Revision Number')

        self._endpoints = updated_deployment_versions
        self._local_changes = {}

        return diff

    def get_metrics_url(self):
        '''
        Return a URL to Amazon CloudWatch for viewing the metrics assoicated with
        the service

        Returns
        -------
        out : str
            A URL for viewing Amazon CloudWatch metrics
        '''
        self._raise_error_if_on_premise_ps()
        self._ensure_not_terminated()

        _logger.info("retrieving metrics from predictive service...")
        try:
            return self._environment.get_metrics_url(self.name)
        except Exception as e:
            _logger.error("Error retrieving metrics: %" % e.message)

    def _str_to_datetime(self, now, time_string):
        '''
        Convert relative timestamp in string to absolute timestamp
        in DateTime
        '''
        try:
            time_string = time_string.lower()
            time_v = int(time_string[:-1])
            time_p = time_string[-1]
        except:
            raise ValueError('Invalid time format %s' % time_string)

        if time_p == 'h':
            timestamp = now - _datetime.timedelta(hours=time_v)
        elif time_p == 'm':
            timestamp = now - _datetime.timedelta(minutes=time_v)
        elif time_p == 'd':
            timestamp = now - _datetime.timedelta(days=time_v)
        else:
            raise ValueError('Invalid time format %s' % time_string)

        return timestamp

    def get_metrics(self, name=None, start_time='12h', end_time=None, period=None):
        '''
        Get the metrics associated with the Predictive Service instance. The metrics include
        number of requests, latency, number of healthy hosts, number of endpoints queryable,
        number of endpoints registered, cache information, and number of requests and
        latency of specific endpoints.

        Parameters
        ----------
        name : str, optional
            The name of the metric to be requested. If None, the requests and latency
            metrics for the entire Predictive Service are returned as a dictionary
            of SFrames.

            Accepted metric names:

            - 'requests': the number of requests that were sent to the Predictive
              Service in a given time frame.
            - 'latency': the latency of the requests that were sent to the Predictive
              Service in a given time frame.
            - 'cache::hits': the amount of cache hits in a given time frame.
            - 'cache::misses': the amount of cache misses in a given time frame.
            - 'cache::latency': the latency of the cache operations.
            - 'cache::num_keys': the number of keys in the cache.
            - 'num_hosts_in_cluster': the number of hosts in this Predictive Service.
            - 'num_objects_queryable': the number of endpoints that can be queried
              on this Predictive Service.
            - 'num_objects_registered': the number of endpoints that are uploaded
              and registered with this Predictive Service.
            - endpoint name: the requests and latency metrics of a specific endpoint.

        start_time : datetime or str, optional
            The start time to query metrics. This parameter can be either
            a string or a DateTime object. The default value is '12h', which is
            12 hours ago from now.

            If given as a string, the parameter specifies the start time as the
            current time minus the given period.
            The string needs to conform to a specific syntax: Supported time
            ranges are 'm' for minutes, 'h' for hours, and 'd' for days.
            Examples: '12h' = 12 hours ago from now, '15m' = 15 min ago from now.

            If given as a DateTime object, the DateTime will be used as the absolute
            start time to query the metrics. It is interpreted as local time.

        end_time : datetime or str, optional
            The end time to query metrics. This parameter can be either a
            string or a DateTime object. If not specified, the default value is now.

            If given as a string, the parameter specifies the end time as the
            current time minus the given period.
            The string needs to conform to a specific syntax: Supported time
            ranges are 'm' for minutes, 'h' for hours, and 'd' for days.
            Examples: '12h' = 12 hours ago from now, '15m' = 15 min ago from now.

            If given as a DateTime object, the DateTime will be used as the absolute
            end time to query the metrics. It is interpreted as local time.

            Note: Both start_time and end_time must be the same type.

        period : int, optional
            The sample frequency of data points in seconds. Default is 5 minutes.

        Returns
        -------
        out : dict (SFrames)
            Returns a dictionary of SFrames containing the SFrames for each metric.

        Notes
        -----
        If none of the parameters are given, this function would return the requests
        and latency metrics for the last 12 hours.

        The DateTime values in the result are UTC times, while the parameters given
        to the method are interpreted as local times.

        Examples
        --------
        To get the default metrics (last 12 hours of metrics):

        >>> ps.get_metrics()
        {'requests':
        +------------------+---------------------------+-------------------+
        |       sum        |            time           |        unit       |
        +------------------+---------------------------+-------------------+
        |       8.0        | 2014-11-13 00:31:00+00:00 |       Count       |
        |       2.0        | 2014-11-13 00:36:00+00:00 |       Count       |
        |       7.0        | 2014-11-13 00:41:00+00:00 |       Count       |
        |     24707.0      | 2014-11-13 00:46:00+00:00 |       Count       |
        |       5.0        | 2014-11-13 00:51:00+00:00 |       Count       |
        ......
        'latency':
        +------------------+---------------------------+-------------------+
        |     average      |            time           |        unit       |
        +------------------+---------------------------+-------------------+
        |    0.0229513     | 2014-11-13 00:31:00+00:00 |      Seconds      |
        |    0.0231056     | 2014-11-13 00:36:00+00:00 |      Seconds      |
        |    0.0221893     | 2014-11-13 00:41:00+00:00 |      Seconds      |
        |    0.0578591     | 2014-11-13 00:46:00+00:00 |      Seconds      |
        |    0.0225744     | 2014-11-13 00:51:00+00:00 |      Seconds      |
        ......

        To get the last 6 hours of metrics for endpoint "test":

        >>> ps.get_metrics(name="test", start_time='6h')
        {'requests':
        +------------------+---------------------------+-------------------+
        |       sum        |            time           |        unit       |
        +------------------+---------------------------+-------------------+
        |       1.0        | 2014-11-13 05:52:00+00:00 |       Count       |
        |       1.0        | 2014-11-13 05:54:00+00:00 |       Count       |
        |       1.0        | 2014-11-13 05:55:00+00:00 |       Count       |
        |       1.0        | 2014-11-13 06:04:00+00:00 |       Count       |
        |       1.0        | 2014-11-13 06:06:00+00:00 |       Count       |
        |       1.0        | 2014-11-13 06:19:00+00:00 |       Count       |
        ......
        'latency':
        +------------------+---------------------------+-------------------+
        |     average      |            time           |        unit       |
        +------------------+---------------------------+-------------------+
        |    0.0521337     | 2014-11-13 05:52:00+00:00 |      Seconds      |
        |    0.0509784     | 2014-11-13 05:54:00+00:00 |      Seconds      |
        |    0.0512019     | 2014-11-13 05:55:00+00:00 |      Seconds      |
        |    0.0525574     | 2014-11-13 06:04:00+00:00 |      Seconds      |
        |    0.0519456     | 2014-11-13 06:06:00+00:00 |      Seconds      |
        |    0.0518503     | 2014-11-13 06:19:00+00:00 |      Seconds      |
        ......

        '''
        self._ensure_not_terminated()

        # check if po_name is valid
        if name and not self._environment._check_metrics_name(name) and \
                    not self._endpoints.has_key(name):
            raise ValueError('Cannot find metric with name "%s".' % name)

        # convert timestamps to datetimes if necessary
        now = _datetime.datetime.now()
        if not start_time:
            start_time = now - _datetime.timedelta(hours=12)
        elif isinstance(start_time, basestring): # relative
            start_time = self._str_to_datetime(now, start_time)
        # convert to UTC format
        start_time = int(_time.mktime(start_time.timetuple()))

        if not end_time:
            end_time = now
        elif isinstance(end_time, basestring): #relative
            end_time = self._str_to_datetime(now, end_time)
        # convert to UTC format
        end_time = int(_time.mktime(end_time.timetuple()))

        # default period to 5 min
        if not period or type(period) != int:
            period = 300

        # get metrics
        metrics = self._environment._get_metrics(name, self.name,
                                    self._endpoints.has_key(name),
                                    start_time, end_time, period)

        if not metrics or len(metrics.keys()) == 0:
            return None

        return metrics

    def get_query_logs(self, start_time = None, end_time = None):
        """
        Fetch query logs from state path corresponding to time window specified by
        [start_time, end_time] and return in an SFrame.

        If both start_time and end_time are not given, default to past one hour

        Parameters
        ----------
        start_time : datetime or str
            Fetch logs no older than this timestamp.

        end_time : datetime or str
            Fetch logs only as recent as this timestamp.

        Returns
        -------
        out : SFrame
            An SFrame containing two columns: datetime and log, with size equal
            to the number of log entries from all nodes during the specified
            time interval.

        See Also
        --------
        get_result_logs, get_feedback_logs, get_server_logs, get_custom_logs,
        get_graphlab_service_logs, graphlab.deploy.job.create

        Notes
        -----
        If datetimes are specified as a str, we expect them to be ISO-8601-
        formatted (eg. "2015-03-23T11:23:13") with microseconds and timezone
        optional. If no timezone is provided, we assume system localtime.

        For large time windows, downloading the log files can take a very long
        time. You can set start_time and end_time to None if you would like to
        download all query logs, but you probably don't want to. If you have an
        operation that you wish to run over the entire set of logs, you should
        launch it as a remote `Job`.

        Examples
        --------
        Get query logs from past 24 hours:

            >>> from datetime import datetime, timedelta
            >>> start_time = datetime.now() - timedelta(days=1)
            >>> logs = ps.get_query_logs(start_time=start_time, end_time=None)

        Get query logs for the day before yesterday:

            >>> start_time = (datetime.now() - timedelta(days=2)).replace(
                    hour=0, minute=0, second=0, microsecond=0)
            >>> end_time = start_time + timedelta(days=1)
            >>> logs = ps.get_query_logs(start_time=start_time, end_time=end_time)

        Get query logs for the past week:

            >>> start_time = datetime.now() - timedelta(days=7)
            >>> logs = ps.get_query_logs(start_time=start_time, end_time=None)
        """
        return self._environment._get_logs("query", start_time, end_time)

    def get_result_logs(self, start_time, end_time):
        """
        Fetch result logs from state path corresponding to time window specified by
        [start_time, end_time] and return in an SFrame.

        If both start_time and end_time are not given, default to past one hour

        Parameters
        ----------
        start_time : datetime or str
            Fetch logs no older than this timestamp.

        end_time : datetime or str
            Fetch logs only as recent as this timestamp.

        Returns
        -------
        out : SFrame
            An SFrame containing two columns: datetime and log, with size equal
            to the number of log entries from all nodes during the specified
            time interval.


        See Also
        --------
        get_query_logs, get_feedback_logs, get_server_logs, get_custom_logs,
        get_graphlab_service_logs.

        Notes
        -----
        If datetimes are specified as a str, we expect them to be ISO-8601-
        formatted (eg. "2015-03-23T11:23:13") with microseconds and timezone
        optional. If no timezone is provided, we assume system localtime.

        For large time windows, downloading the log files can take a very long
        time. You can set start_time and end_time to None if you would like to
        download all result logs, but you probably don't want to. If you have an
        operation that you wish to run over the entire set of logs, you should
        launch it as a remote `Job`.

        Examples
        --------
        Get result logs from past 24 hours:

            >>> from datetime import datetime, timedelta
            >>> start_time = datetime.now() - timedelta(days=1)
            >>> logs = ps.get_result_logs(start_time=start_time, end_time=None)

        Get result logs for the day before yesterday:

            >>> start_time = (datetime.now() - timedelta(days=2)).replace(
                    hour=0, minute=0, second=0, microsecond=0)
            >>> end_time = start_time + timedelta(days=1)
            >>> logs = ps.get_result_logs(start_time=start_time, end_time=end_time)

        Get result logs for the past week:

            >>> start_time = datetime.now() - timedelta(days=7)
            >>> logs = ps.get_result_logs(start_time=start_time, end_time=None)
        """
        return self._environment._get_logs("result", start_time, end_time)

    def get_feedback_logs(self, start_time = None, end_time = None):
        """
        Fetch feedback logs from state path corresponding to time window specified by
        [start_time, end_time] and return in an SFrame.

        If both start_time and end_time are not given, default to past one hour

        Parameters
        ----------
        start_time : datetime or str
            Fetch logs no older than this timestamp.

        end_time : datetime or str
            Fetch logs only as recent as this timestamp.

        Returns
        -------
        out : SFrame
            An SFrame containing two columns: datetime and log, with size equal
            to the number of log entries from all nodes during the specified
            time interval.

        See Also
        --------
        get_query_logs, get_result_logs, get_server_logs, get_custom_logs,
        get_graphlab_service_logs.

        Notes
        -----
        If datetimes are specified as a str, we expect them to be ISO-8601-
        formatted (eg. "2015-03-23T11:23:13") with microseconds and timezone
        optional. If no timezone is provided, we assume system localtime.

        For large time windows, downloading the log files can take a very long
        time. You can set start_time and end_time to None if you would like to
        download all feedback logs, but you probably don't want to. If you have
        an operation that you wish to run over the entire set of logs, you
        should launch it as a remote `Job`.

        Examples
        --------
        Get feedback logs from past 24 hours:

            >>> from datetime import datetime, timedelta
            >>> start_time = datetime.now() - timedelta(days=1)
            >>> logs = ps.get_feedback_logs(start_time=start_time, end_time=None)

        Get feedback logs for the day before yesterday:

            >>> start_time = (datetime.now() - timedelta(days=2)).replace(
                    hour=0, minute=0, second=0, microsecond=0)
            >>> end_time = start_time + timedelta(days=1)
            >>> logs = ps.get_feedback_logs(start_time=start_time, end_time=end_time)

        Get feedback logs for the past week:

            >>> start_time = datetime.now() - timedelta(days=7)
            >>> logs = ps.get_feedback_logs(start_time=start_time, end_time=None)
        """
        return self._environment._get_logs("feedback", start_time, end_time)

    def get_server_logs(self, start_time = None, end_time = None):
        """
        Fetch server logs from state path corresponding to time window specified by
        [start_time, end_time] and return in an SFrame.

        If both start_time and end_time are not given, default to past one hour

        Parameters
        ----------
        start_time : datetime or str
            Fetch logs no older than this timestamp.

        end_time : datetime or str
            Fetch logs only as recent as this timestamp.

        Returns
        -------
        out : SFrame
            An SFrame containing two columns: datetime and log, with size equal
            to the number of log entries from all nodes during the specified
            time interval.

        See Also
        --------
        get_query_logs, get_result_logs, get_feedback_logs, get_custom_logs,
        get_graphlab_service_logs.

        Notes
        -----
        If datetimes are specified as a str, we expect them to be ISO-8601-
        formatted (eg. "2015-03-23T11:23:13") with microseconds and timezone
        optional. If no timezone is provided, we assume system localtime.

        For large time windows, downloading the log files can take a very long
        time. You can set start_time and end_time to None if you would like to
        download all server logs, but you probably don't want to. If you have an
        operation that you wish to run over the entire set of logs, you should
        launch it as a remote `Job`.

        Examples
        --------
        Get server logs from past 24 hours:

            >>> from datetime import datetime, timedelta
            >>> start_time = datetime.now() - timedelta(days=1)
            >>> logs = ps.get_server_logs(start_time=start_time, end_time=None)

        Get server logs for the day before yesterday:

            >>> start_time = (datetime.now() - timedelta(days=2)).replace(
                    hour=0, minute=0, second=0, microsecond=0)
            >>> end_time = start_time + timedelta(days=1)
            >>> logs = ps.get_server_logs(start_time=start_time, end_time=end_time)

        Get server logs for the past week:

            >>> start_time = datetime.now() - timedelta(days=7)
            >>> logs = ps.get_server_logs(start_time=start_time, end_time=None)
        """
        return self._environment._get_logs("server", start_time, end_time)

    def get_custom_logs(self, start_time = None, end_time = None):
        """
        Fetch custom logs from state path corresponding to time window specified by
        [start_time, end_time] and return in an SFrame.

        If both start_time and end_time are not given, default to past one hour

        Parameters
        ----------
        start_time : datetime or str
            Fetch logs no older than this timestamp.

        end_time : datetime or str
            Fetch logs only as recent as this timestamp.

        Returns
        -------
        out : SFrame
            An SFrame containing two columns: datetime and log, with size equal
            to the number of log entries from all nodes during the specified
            time interval.

        See Also
        --------
        get_query_logs, get_result_logs, get_feedback_logs, get_custom_logs,
        get_graphlab_service_logs.

        Notes
        -----
        If datetimes are specified as a str, we expect them to be ISO-8601-
        formatted (eg. "2015-03-23T11:23:13") with microseconds and timezone
        optional. If no timezone is provided, we assume system localtime.

        For large time windows, downloading the log files can take a very long
        time. You can set start_time and end_time to None if you would like to
        download all custom logs, but you probably don't want to. If you have an
        operation that you wish to run over the entire set of logs, you should
        launch it as a remote `Job`.

        Examples
        --------
        Get custom logs from past 24 hours:

            >>> from datetime import datetime, timedelta
            >>> start_time = datetime.now() - timedelta(days=1)
            >>> logs = ps.get_custom_logs(start_time=start_time, end_time=None)

        Get custom logs for the day before yesterday:

            >>> start_time = (datetime.now() - timedelta(days=2)).replace(
                    hour=0, minute=0, second=0, microsecond=0)
            >>> end_time = start_time + timedelta(days=1)
            >>> logs = ps.get_custom_logs(start_time=start_time, end_time=end_time)

        Get custom logs for the past week:

            >>> start_time = datetime.now() - timedelta(days=7)
            >>> logs = ps.get_custom_logs(start_time=start_time, end_time=None)
        """
        return self._environment._get_logs("custom", start_time, end_time)

    def get_graphlab_service_logs(self, start_time = None, end_time = None):
        """
        Fetch graphlab service logs from S3 corresponding to time window specified by
        [start_time, end_time] and return in an SFrame.

        If both start_time and end_time are not given, default to past one hour

        Parameters
        ----------
        start_time : datetime or str
            Fetch logs no older than this timestamp.

        end_time : datetime or str
            Fetch logs only as recent as this timestamp.

        Returns
        -------
        out : SFrame
            An SFrame containing two columns: datetime and log, with size equal
            to the number of log entries from all nodes during the specified
            time interval.

        See Also
        --------
        get_query_logs, get_result_logs, get_feedback_logs, get_server_logs, get_custom_logs.

        Notes
        -----
        If datetimes are specified as a str, we expect them to be ISO-8601-
        formatted (eg. "2015-03-23T11:23:13") with microseconds and timezone
        optional. If no timezone is provided, we assume system localtime.

        For large time windows, downloading the log files can take a very long
        time. You can set start_time and end_time to None if you would like to
        download all query logs, but you probably don't want to. If you have an
        operation that you wish to run over the entire set of logs, you should
        launch it as a remote `Job`.

        Examples
        --------
        Get graphlab service logs from past 24 hours:

            >>> from datetime import datetime, timedelta
            >>> start_time = datetime.now() - timedelta(days=1)
            >>> logs = ps.get_graphlab_service_logs(start_time=start_time, end_time=None)

        Get graphlab service logs for the day before yesterday:

            >>> start_time = (datetime.now() - timedelta(days=2)).replace(
                    hour=0, minute=0, second=0, microsecond=0)
            >>> end_time = start_time + timedelta(days=1)
            >>> logs = ps.get_graphlab_service_logs(start_time=start_time, end_time=end_time)

        Get graphlab service logs for the past week:

            >>> start_time = datetime.now() - timedelta(days=7)
            >>> logs = ps.get_graphlab_service_logs(start_time=start_time, end_time=None)
        """
        return self._environment._get_logs("graphlab_service", start_time, end_time)

    def show(self):
        """
        Visualize the Predictive Service with GraphLab Canvas. This function
        starts Canvas if it is not already running.

        Returns
        -------
        view : graphlab.canvas.view.View
            An object representing the GraphLab Canvas view

        See Also
        --------
        canvas
        """
        self._ensure_not_terminated()

        _canvas.get_target().state.set_selected_variable(('Predictive Services', self.name))
        return _canvas.show()

    def save(self):
        """
        Saves the Predictive Service information to disk. Information saved
        contains: Name, State path, and AWS credentials. Note that only metadata
        of the Predictive Service is stored, it does not impact the actual
        deployed Predictive Service.

        This information can be useful later to access the Predictive Service,
        for example:

            >>> import graphLab
            >>> my_ps = graphlab.deploy.predictive_services[<name>]

        """
        self._ensure_not_terminated()
        self._session.save(self, typename="PredictiveService")

    def cache_enable(self, name=None, restart=True, max_retries=3):
        """
        Enable caching for a Predictive Service.

        Parameters
        ----------
        name : str, optional
            A unique identifier for the Predictive Object. If empty or None, the
            cache will be enabled for the entire service.

        restart : bool, optional (defaults to True)
            A boolean indicating whether the underlying cache process should be
            restarted (and all data erased).
        """
        self._ensure_not_terminated()
        self._ensure_no_local_changes()
        self._disallow_cache_operation_for_non_model(name)

        # update and save cache state
        self._update_and_save_cache_status(name, 'enabled')

        # send control plane query to enable cache
        self._environment.cache_enable(name, restart)

        num_retries = 0
        while not self._environment._is_cache_ok("healthy") and \
          num_retries < max_retries:
            self._environment.cache_enable(name, restart)
            num_retries += 1
            _time.sleep(2)

        if not self._environment._is_cache_ok("healthy"):
            _logger.warn("Cluster cache unstable; disabling")
            self.cache_disable()
        else:
            _logger.info('Cache enabled successfully!')

    def cache_disable(self, name=None):
        """
        Disable caching for a Predictive Service.

        Parameters
        ----------
        name : str, optional
            A unique identifier for the Predictive Object. If empty or None, the
            cache will be disabled for the entire service.
        """
        self._ensure_not_terminated()
        self._ensure_no_local_changes()
        self._disallow_cache_operation_for_non_model(name)

        # update and save cache state
        self._update_and_save_cache_status(name, 'disabled')

        # send control plane query to disable cache
        self._environment.cache_disable(name)

        _logger.info('Cache disabled successfully!')

    def cache_clear(self, name=None):
        """
        Clear a Predictive Service's cache.

        Parameters
        ----------
        name : str, optional
            A unique identifier for the Predictive Object. If a valid name is
            specified, only cache entries associated with that particular
            Predictive Object will be removed. Otherwise, if name is empty or
            None, the entire cache is cleared.
        """
        self._ensure_not_terminated()
        self._ensure_no_local_changes()
        self._disallow_cache_operation_for_non_model(name)

        self._environment.cache_clear(name)
        _logger.info('Cache cleared successfully!')

    def _disallow_cache_operation_for_non_model(self, name):
        if name is not None:
            endpoint = self._endpoints.get(name)
            if not endpoint:
                raise RuntimeError('Endpoint "%s" does not exist.' % name)
            if endpoint.get('type', 'model') != 'model':
                raise RuntimeError('Cache operation is not allowed for endpoint'
                                        ' of type "%s".' % endpoint.get('type'))

    def flush_logs(self):
        """
        Force a Predictive Service to ship its logs to its environment's log path.

        Logs are shipped periodically, but you may find it useful for diagnosing
        unexpected behavior to ship the logs on demand. The logs will be shipped
        to the location specified in the `EC2Environment` at the time the
        Predictive Service was created.
        """
        self._ensure_not_terminated()
        self._environment.flush_logs()

    def set_CORS(self, cors_origin):
        """
        Sets the CORS origin for the Predictive Service

        Parameters
        ----------
        cors_origin : str
            The string value to use as HTTP header Access-Control-Allow-Origin,
            in order to support Cross-Origin Resource Sharing as described in
            https://developer.mozilla.org/en-US/docs/Web/HTTP/Access_control_CORS.
            If '' (empty String) is specified, CORS support will be disabled.
            If the string value is '*', CORS support will be enabled for all URIs.
            If the string value is 'https://dato.com', CORS support will be
            enabled for 'https://dato.com' only.

        Notes
        -----
        This operation will be applied immediately and will take effect as
        soon as all nodes are up to date.

        Examples
        --------
        Allow requests from dato.com to access this Predictive Service:

            >>> ps.set_CORS('https://dato.com')

        Allow requests from any origin to access this Predictive Service:

            >>> ps.set_CORS('*')

        Disable CORS for this Predictive Service:

            >>> ps.set_CORS(None)
        """
        self._ensure_not_terminated()
        self._ensure_no_local_changes()

        # set cors_origin
        if not cors_origin:
            cors_origin = ''
        if not isinstance(cors_origin, basestring):
            raise TypeError('CORS origin must be a String.')
        self._cors_origin = cors_origin

        try:
            self._save_state()
            self._environment.poke()
            _logger.info("CORS origin configuration has been changed, please wait a "\
                "while for all nodes to take effect.")
        except _ConnectionError as e:
            _logger.warn("Unable to connect to running Predictive Service: %s" %
                         (e.message))

    _BAD_PARAM_RE = _re.compile("an unexpected keyword argument '(.*?)'")

    def _set_state_from_params_dict(self, params):
        try:
            self._system_config = _SystemConfig(**params)
        except TypeError as e:
            if "an unexpected keyword argument" in e.message:
                bad_param_name = \
                  PredictiveService._BAD_PARAM_RE.search(e.message).group(1)
                raise ValueError("Invalid configuration parameter specified: %s"
                                 % bad_param_name)
            else:
                raise

    def reconfigure(self, params):
        """
        Updates specified system configuration parameters for the Predictive
        Service.

        Parameters
        ----------
        params : dict
            A dictionary of configuration parameter names and the corresponding
            values to which they will be set. Possible configuration parameters
            are
                ```cache_max_memory_mb```
                ```cache_ttl_on_update_secs```

        Examples
        --------
        Update the maximum memory allocated to the cache to be 4GB:

            >>> ps.reconfigure({"cache_max_memory_mb": 4000})

        Update the TTL for the cache entries on model updates:

            >>> ps.reconfigure({"cache_ttl_on_update_secs": 120})

        """
        self._ensure_not_terminated()
        self._ensure_no_local_changes()

        if not _config_params_are_valid(params):
            raise RuntimeError("Invalid configuration parameters")

        self._set_state_from_params_dict(params)

        try:
            self._save_state()
            self._environment.reconfigure(self._system_config)
            _logger.info("Configuration updated")
        except ValueError:
            raise
        except _ConnectionError as e:
            _logger.error("Unable to connect to running Predictive Service: %s" %
                         (e.message))
        except Exception as e:
            _logger.error("Unable to reconfigure Predictive Service: %s"
                          % e.message)

    def _load_endpoint(self, name):
        '''
        Load an endpoint object. We lazily load the actual endpoint policy
        object into the Predictive Service. On self._get_latest_state(), we
        do not load any Predictive Object or Policy.

        If the endpoint is not depoyed yet, we return the endpoint from pending
        change list.
        '''
        self._ensure_not_terminated()

        # For pending endpoint changes, use the pending one
        if name in self._local_changes:
            if self._local_changes[name]['action'] == 'remove':
                raise RuntimeError('Endpoint "%s" is schedueld for removal.' % name)
            else:
                return self._local_changes[name]['endpoint_obj']

        if name not in self._endpoints:
            raise ValueError('Cannot find policy with name "%s".' % name)

        endpoint_info = self._endpoints[name]

        # already loaded case
        if endpoint_info.get('endpoint_obj') is not None:
            return endpoint_info.get('endpoint_obj')

        endpoint_type = endpoint_info.get('type', 'model')
        path = self._get_predictive_object_save_path(name, endpoint_info['version'])
        if endpoint_type == 'model':
            endpoint_info['endpoint_obj'] = _PredictiveObject.load(
                                        path, endpoint_info['schema_version'])
        elif endpoint_type == 'policy':
            endpoint_info['endpoint_obj'] = _EndpointPolicy.load(
                                        path, endpoint_info['schema_version'])
        else:
            assert endpoint_type == 'alias'

        return endpoint_info.get('endpoint_obj')

    @_retry()
    def _persist_pending_changes(self):
        # Save endpoint changes
        if len(self._local_changes) > 0:
            for (endpoint_name, info) in self._local_changes.iteritems():
                if info['action'] != 'remove' and \
                (info.get('type', 'model') == 'policy' or info.get('type', 'model') == 'model'):
                    save_path = self._get_predictive_object_save_path(endpoint_name, info['version'])
                    if _is_s3_path(save_path):
                        info['endpoint_obj'].save(save_path, self._environment.aws_credentials)
                    else:
                        info['endpoint_obj'].save(save_path)

    def _save_state(self):
        # Dump immutable state data to a config
        state = _ConfigParser(allow_no_value=True)
        state.optionxform = str
        state.add_section(PredictiveService._SERVICE_INFO_SECTION_NAME)
        state.set(PredictiveService._SERVICE_INFO_SECTION_NAME, 'Name', self.name)
        state.set(PredictiveService._SERVICE_INFO_SECTION_NAME, 'Description', self.description)
        state.set(PredictiveService._SERVICE_INFO_SECTION_NAME, 'API Key', self.api_key)
        state.set(PredictiveService._SERVICE_INFO_SECTION_NAME, 'CORS Origin', self._cors_origin)
        state.set(PredictiveService._SERVICE_INFO_SECTION_NAME, 'Global Cache State', self._global_cache_state)

        # Save environment, if we have one
        if self._environment:
            state.add_section(PredictiveService._ENVIRONMENT_SECTION_NAME)
            for (key, value) in self._environment._get_state(self._schema_version).iteritems():
                state.set(PredictiveService._ENVIRONMENT_SECTION_NAME, key, value)

        # Save deployment version data to config
        state.add_section(PredictiveService._DEPLOYMENT_SECTION_NAME)
        state.add_section(PredictiveService._PREDICTIVE_OBJECT_DOCSTRING)
        current_endpoints = _copy(self._all_endpoints)
        for (endpoint_name, info) in current_endpoints.iteritems():
            # Save docstring in a separate section
            state.set(PredictiveService._PREDICTIVE_OBJECT_DOCSTRING, endpoint_name, \
                                 info['docstring'].encode('string_escape'))

            # save metadata
            endpoint_metadata = self._get_endpoint_metadata(info)
            state.set(PredictiveService._DEPLOYMENT_SECTION_NAME, endpoint_name, \
                _json.dumps(endpoint_metadata))

        if self._has_remote_state_changed():
            raise IOError("Can not save changes. The Predictive Service has changed. Please "
                          "reload your Predictive Service first")

        # Update the revision number after we have successfully written all endpoints
        self._revision_number += 1
        state.add_section(self._META_SECTION_NAME)
        state.set(self._META_SECTION_NAME, 'Revision Number', self._revision_number)
        state.set(self._META_SECTION_NAME, 'Schema Version', self._schema_version)

        # Save system config. params
        state.add_section(PredictiveService._SYSTEM_SECTION_NAME)
        self._system_config.set_state_in_config(
            state, PredictiveService._SYSTEM_SECTION_NAME)

        # Write state file to state path
        self._environment._write_state_config(state, self._state_path)

    def _get_endpoint_metadata(self, endpoint_info):
        endpoint_metadata = ['cache_state', 'description', 'schema_version', 'version']

        # type and endpoint_obj is introduced in schema 5
        if self._schema_version > 4:
            endpoint_metadata.append('type')
            if endpoint_info.get('type', 'model') == 'alias':
                endpoint_metadata.append('endpoint_obj')

        metadata = {}
        for m in endpoint_metadata:
            metadata[m] = endpoint_info.get(m)

        return metadata

    def _update_local_state(self):
        # Update our state
        current_endpoints = _copy(self._all_endpoints)
        self._local_changes = {}
        self._endpoints = dict(zip(current_endpoints.keys(),
            [{
                'version': info['version'],
                'type': info.get('type', 'model'),
                'description': info['description'],
                'schema_version': info['schema_version'],
                'docstring': info['docstring'],
                'cache_state': info['cache_state'],
                'endpoint_obj' : info.get('endpoint_obj')
             } for info in current_endpoints.values()]))

    def _get_predictive_object_save_path(self, predictive_object_name, version_number):
        return self._environment._get_root_path(self._state_path) + \
                '/'.join([PredictiveService._PREDICTIVE_OBJECT_DIR,
                          predictive_object_name, str(version_number)])

    def _get_dependency_save_path(self, predictive_object_name, version_number):
        return self._environment._get_root_path(self._state_path) + \
                '/'.join([PredictiveService._DEPENDENCIES_DIR,
                          predictive_object_name, str(version_number)])

    def _ensure_not_terminated(self):
        if not self._environment:
            raise RuntimeError("This operation is not supported because Predictive Service is already terminated")

    def _ensure_no_local_changes(self):
        if self._has_pending_changes():
            raise RuntimeError("There are local pending changes to this " \
                               "Predictive Service. Please apply or remove " \
                               "all local changes before performing the " \
                               "operation.")

    def _update_and_save_cache_status(self, name, cache_status="enabled"):
        # update cache state
        if not name or name == '':
            # enable global cache state
            self._global_cache_state = cache_status

            # enable all Predictive Object cache states directly in our internal state,
            # since this is not a local PO change.
            for k in self._endpoints:
                if self._endpoints[k].get('type', 'model') == 'model':
                    self._endpoints[k]['cache_state'] = cache_status
        else:
            # enable cache for Predictive Object defined in 'name'
            if name in self._endpoints:
                self._endpoints[name]['cache_state'] = cache_status
            else:
                # name does not exist
                raise ValueError('Cannot find Predictive Object with name "%s".' % name)

        # save cache state
        self._save_state()

    def _create_custom_po(self, query, description):
        return _CustomQueryPredictiveObject(
            query = query,
            description = description)

    # The following methods are for savng artifacts in local session:
    def __getstate__(self):
        state = {
            'name': self.name,
            'schema_version': self._schema_version,
            'state_path': self._state_path,
            'description': self.description,
        }
        if hasattr(self._environment, 'aws_credentials'):
            state['aws_credentials'] = self._environment.aws_credentials
        return state

    def _get_version(self):
        return PREDICTIVE_SERVICE_SCHEMA_VERSION

    def _get_metadata(self):
        """
        Get the metadata that is managed by the session. This gets displayed
        in the Scoped session and is stored in the session's index file.

        """
        status = 'Yes' if self._modified_since_last_saved is True else 'No'
        return {'Name': self.name,
                'State_path': self._state_path,
                'Type': type(self).__name__,
                'Unsaved changes?' : status,
                'Creation date': self._session_registration_date}

    @classmethod
    def _load_version(cls, unpickler, version):
        """
        An abstract function to implement save for the object in consideration.

        Parameters
        ----------
        unpickler : A GLUnpickler file handle.

        version : Version number as maintained by the class.

        """
        # Load the dump.
        ps_info = unpickler.load()
        # We had issues loading scheme that were before version 3, so we do not support it.
        if ps_info.schema_version < LAST_SUPPORTED_PREDICTIVE_SERVICE_SCHEMA_VERSION:
            raise RuntimeError("Do not support loading version %s of Predictive Service from local session." \
                % ps_info.schema_version)

        aws_credentials = ps_info.aws_credentials if hasattr(ps_info, 'aws_credentials') else {}

        return _gl.deploy.predictive_service._load_imp(
            state_path = ps_info.state_path,
            aws_access_key_id = aws_credentials.get('aws_access_key_id', None) if aws_credentials else None,
            aws_secret_access_key = aws_credentials.get('aws_secret_access_key', None) if aws_credentials else  None)

    def _has_remote_state_changed(self):
        '''
        Returns whether any changes have been made to this Predictive Service remotely.
        '''
        if self._revision_number == 0:
            return False
        config = PredictiveServiceEnvironment._get_state_from_file(self._state_path, self.aws_credentials)
        saved_version_num = config.getint(self._META_SECTION_NAME, 'Revision Number')
        return (self._revision_number != saved_version_num)

    def _raise_error_if_on_premise_ps(self):
        '''
        Raise an exception if the environment is docker.
        '''
        if  isinstance(self._environment, _DockerPredictiveServiceEnvironment) or \
            isinstance(self._environment, _LocalPredictiveServiceEnvironment):
            raise NotImplementedError("The function " + _inspect.stack()[1][3] + " is disabled on the" +
                                      " client side for on-premises Predictive Service. Please check with your" +
                                      " systems admin for further instructions.")

    def _has_pending_changes(self):
        return len(self._local_changes) > 0
