"""
This module contains the churn prediction high-level toolkit
"""

import graphlab as _gl
import graphlab.connect as _mt
import types as _types
import datetime
import time
import logging as _logging
# Utils
from graphlab.util import _raise_error_if_not_of_type
from graphlab.toolkits._internal_utils import _toolkit_repr_print, \
                                              _precomputed_field, \
                                              _raise_error_if_not_sframe
from graphlab.util import _raise_error_if_not_of_type
from graphlab.toolkits._main import ToolkitError
from graphlab.toolkits._model import _get_default_options_wrapper
from graphlab.toolkits._model import SDKModel as _SDKModel




import datetime as _datetime
import time as _time
from graphlab.toolkits._main import ToolkitError as _ToolkitError

def create(observation_data,
           user_id='user_id', timestamp='timestamp',
           user_data=None,
           time_unit=0,
           features=[],
           time_aggregate=_datetime.timedelta(days=1),
           lookback_feature_periods=[7, 14, 21, 60, 90],
           time_boundaries=[],
           verbose=True):
    """
    Create a model of type
    :class:`~graphlab.churn_predictor.ChurnPredictor` that performs
    churn analysis on provided user activity logs.

    The Churn Prediction toolkit allows predicting which users
    will churn (stop using) a product or website given user activity logs.

    Training datasets should contain columns with user id, time stamp, and user
    events. Given the same, or a different data set, the toolkit will compute
    the probability of a user churning.

    For instance, given a dataset of the form:

    .. sourcecode:: python

        +-------------------------------+---------+---------------+------------+
        |            user_id            | action  |   timestamp   | product_id |
        +-------------------------------+---------+---------------+------------+
        | ONE                           | open    | 04/15/1981    | 205075200  |
        | ONE                           | bought  | 04/15/1981    |  88441100  |
        | ONE                           | clicked | 04/17/1981    | 205075200  |
        | TWO                           | clicked | 09/01/2015    | 205075200  |
        | TWO                           | bought  | 09/21/2015    |  88414900  |
        +-------------------------------+---------+---------------+------------+

    If we are looking at this data set, the last time stamp is September 21st
    2015. Given that last date, user TWO is unlikely to churn. Whereas user ONE,
    who was a customer in 1981 and has not come back for 34 years is likely to
    churn (not come back).

    The toolkit does not use current wall time, it uses the last provided time
    stamp for predictions (unless another time stamp is specified).

    This toolkit will look at users, time and activity types, form an internal
    feature set based on user behavior, and train a model to predict user churn.

    Predicted churn is provided as a probability, where 0% means the user will
    definitely churn, and 100% means the user will definitely stay.

    The prediction is set by default to execute on the latest time stamp
    provided in the prediction set, but can also be set manually to some other
    date.

    Since, internally, training requires generating training labels, the last
    10% of the data is not used for training. Therefore, it is safe to reuse
    the training data set as a prediction data set to know who is likely to
    churn. A trained model can also be used to predict on new data set safely.

    Parameters
    ----------
    observation_data : SFrame
        The dataset to use for training the model. It must contain a column of
        user ids, a column of timestamps, along with one or more activity
        columns. Each row represents a user action at a given time.

        A user activity column can contain numeric data (length of visit in
        seconds, number of items in cart) or categorical data (item purchased,
        page visited).

        The user id must be of type 'int' or 'str'. The time column must be of
        type int or datetime.

        User activity columns of type 'int' and 'str' will be considered
        categorical. Columns of type 'float' will be considered numerical.

    user_id : string, optional
        The name of the column in `observation_data` that corresponds to the
        user id.
        Default: user_id

    timestamp : string, optional
        The name of the column in `observation_data` that corresponds to the
        timestamp.

        The column can be of datetime.datetime type, or int type. If the column
        contains ints the time_unit parameter can be used to define the unit of
        time represented by the column.
        Default: timestamp

    user_data : SFrame, optional
        Side information for the users.  This SFrame must have a column with
        the same name as what is specified by the `user_id` input parameter.
        `user_data` can provide any amount of additional user-specific
        information. The join performed is an inner join.

    features : List of string, optional
        If specified, only the features in the list will be used. Columns of
        type Integer and String will be treated as categorical, columns of type
        Float will be treated as numeric.

    time_unit : int, optional
        If the timestamp column is of integral type, this sets the unit interval
        of the column. For instance, if the value of the timestamp column is in
        milliseconds, the value would be 1000. If the timestamp column is in
        seconds, the value would be 1.
        Default: 0 (auto-detect)

    time_aggregate : datetime.timedelta, optional
        Internal time frame to roll up user actions. In order to make the inner
        computation efficient, the user actions are rolled-up by time. The
        default is to aggregate by day. This can be shortened if hourly rates of
        actions are of importance, or stretched if only weekly rates matter.
        The larger the roll-up, the faster the model will run. The timedelta
        must be positive.
        Default: 1 day

    lookback_feature_periods : list of int, optional
        Interval of time to look back for feature generation. Each number is in
        the unit of time_aggregate (the default being days as defined above).
        For instance, if the list contains [7, 14], it will generate features
        for weekly patterns and biweekly patterns. If time_aggregate was set
        to hours, [7, 14] would generate for 7 hours and 14 hours patterns.
        Default: [7, 14, 21, 60, 90]

    time_boundaries : list of datetime.datetime, optional
        List of time boundaries used to compute the training set. At each time
        boundary, users that are present before the boundary will be used to
        compute features, and their presence after the boundary will make up the
        label. By having multiple time boundaries, more training data can be
        generated. If an empty list is specified (default), 10 evenly separated
        boundaries are used based on the first and last timestamp of the
        observation_data.
        Default: 10 evenly separated boundaries are used based on the first and
        last timestamp of the observation_data.

    verbose: boolean, optional
        When set to true, more status output is generated
        Default: True

    Returns
    -------
      out : ChurnPredictor
          A trained model of type
          :class:`~graphlab.churn_predictor.ChurnPredictor`.

    See Also
    --------
    ChurnPredictor


    Examples
    --------

    .. sourcecode:: python

        # Load a data set. The d ata set has 3 columns: user_id, timestamp, event
        # and must contain at least 100 rows of user activity.
        >>> sf = gl.SFrame('~/data/churn/actions_top_k.csv')

        # We set time_unit to 1000 because our data has timestamps in milliseconds.
        # Otherwise, we would ommit it (or set it to 1).
        >>> model = gl.churn_predictor.create(sf,
                                              user_id="user_id",
                                              timestamp="timestamp")

        # For simplicity, we will predict on the input data set
        >>> model.predict(sf)

        # Output is in the form:
        +-------------------------------+----------------------+
        |            user_id            |   stay_probability   |
        +-------------------------------+----------------------+
        | ONE                           | 0.001                |
        | TWO                           | 99.99                |
        +-------------------------------+----------------------+

    """

    _mt._get_metric_tracker().track('{}.create'.format("toolkit.churn_predictor.create"))

    _raise_error_if_not_sframe(observation_data, "observation_data")
    _raise_error_if_not_of_type(user_id, [str])
    _raise_error_if_not_of_type(timestamp, [str])
    if (user_data): _raise_error_if_not_sframe(user_data, "observation_data")

    time_aggregate_int = int(time_aggregate.total_seconds())
    if (time_aggregate_int <= 0):
        raise _ToolkitError("time_aggregate must be a positive time delta")

    if (observation_data.num_rows() < 100):
        raise _ToolkitError("This toolkit requires at least 100 rows of activity")

    # Cheap way to determine time units if not user-specified
    if time_unit == 0:
        # Set to 1 in case of using datetimes or other non-int formats
        time_unit = 1
        first_timestamp = observation_data[timestamp][0]
        if isinstance(first_timestamp, int):
            if (verbose): print("PROGRESS: Determining timestamp unit")
            max_timestamp = observation_data[timestamp].max()
            if len(str(max_timestamp)) >= 11:
                if (verbose): print("PROGRESS: Assuming timestamps are in milliseconds since 01/01/1970")
                time_unit = 1000
            else:
                if (verbose): print("PROGRESS: Assuming timestamps are in seconds since 01/01/1970")
                time_unit = 1

    if (verbose): print("PROGRESS: Initializing churn predictor")
    proxy = _gl.extensions._ChurnPredictor()
    proxy.define_columns(observation_data, timestamp, user_id, time_unit)
    proxy.define_columns3(observation_data, features)
    proxy.define_lookback_feature_periods(lookback_feature_periods)
    proxy.define_default_time_aggregate(time_aggregate_int)

    if (verbose): print("PROGRESS: Sorting input data by time order")
    sorted_data = proxy.sort_by_time(observation_data, "")

    if (verbose): print("PROGRESS: Aggregating input data by groups of " + str(time_aggregate))
    aggregated_by_time = proxy.aggregate_by_time(sorted_data, True, 0, "", "")

    if (time_boundaries):
        unix_timestamps = []
        for dt in time_boundaries: unix_timestamps.append(int(_time.mktime(dt.timetuple())))
        time_boundaries = unix_timestamps

    stpcnt = 10
    if (not time_boundaries and isinstance(sorted_data[timestamp][0], _datetime.datetime)):
        min_time = sorted_data[timestamp][0]
        max_time = sorted_data[timestamp][sorted_data[timestamp].size() - 1]
        step = (max_time - min_time) / stpcnt
        if (verbose): print("PROGRESS: No time boundaries specified, computing 10 boundaries from " + str(min_time) + " to " + str(max_time))
        time_boundaries = [(i+1) * step + min_time for i in range(stpcnt - 1)]

    if (not time_boundaries):
        min_time = sorted_data[timestamp][0] / time_unit
        max_time = sorted_data[timestamp][sorted_data[timestamp].size() - 1] / time_unit
        step = (max_time - min_time) / stpcnt
        if step <= 0:
            raise _ToolkitError("Not enough time in the training data. There should be more than " + str(stpcnt) + " units of time.")
        if (verbose): print("PROGRESS: No time boundaries specified, computing 10 boundaries from " + str(_datetime.datetime.fromtimestamp(min_time)) + " to " + str(_datetime.datetime.fromtimestamp(max_time)))
        time_boundaries = range(min_time + step, max_time, step - 1)

    if (not time_boundaries):
        raise _ToolkitError("Not enough time boundaries defined, at least one must be defined")

    big_user_aggregate = None
    for time_boundary in time_boundaries:

        if isinstance(time_boundary, _datetime.datetime):
            time_boundary = int(_time.mktime(time_boundary.timetuple()))

        if (verbose): print("PROGRESS: Generating user data for aggregate " + str(_datetime.datetime.fromtimestamp(time_boundary)))
        user_aggregate = proxy.per_user_aggregate(aggregated_by_time, time_boundary, "", lookback_feature_periods)

        if (not big_user_aggregate):
            big_user_aggregate = user_aggregate
        else:
            big_user_aggregate = big_user_aggregate.append(user_aggregate)

    if (user_data):
        if (verbose): print("PROGRESS: Joining with user data")
        big_user_aggregate = big_user_aggregate.join(user_data, on=user_id, how="inner")

    if (verbose): print("PROGRESS: Training model")
    proxy.train_model(big_user_aggregate, "")

    if (verbose): print("PROGRESS: All done!")
    return ChurnPredictor(proxy)

_DEFAULT_OPTIONS = {
}

get_default_options = _get_default_options_wrapper(
    '_ChurnPredictor', 'churn_predictor', 'ChurnPredictor', True)

class ChurnPredictor(_SDKModel):
    """
    Churn prediction is used to determine whether a user will stop using a
    service, given their behavior. The input data is in the form of usage
    logs (user id, time stamp, action(s)), as well as supplemental data about
    the users (user id, supplemental data). The toolkit then performs different
    time-based aggregations and computations to build a feature set representing
    the user behavior. This feature set is the used to train an internal model.

    The same transformations are applied to the prediction data, and the trained
    model used to predict user churn.

    User churn is defined as a user who was present (any actions) before a given
    time boundary, and no longer present after said boundary. The time
    boundaries for training can be specified, or chosen automatically by the
    toolkit. For prediction, the last time stamp of the provided data is used
    by default.

    This model cannot be constructed directly.  Instead, use
    :func:`graphlab.churn_predictor.create` to create
    an instance of this model. A detailed list of parameter options and
    code samples are available in the documentation for the create function.

    See Also
    --------
    create
    """

    def __init__(self, model_proxy):
        self.__proxy__ = model_proxy

    def _get_wrapper(self):
        _class = self.__proxy__.__class__
        proxy_wrapper = self.__proxy__._get_wrapper()
        def model_wrapper(unity_proxy):
            model_proxy = proxy_wrapper(unity_proxy)
            return ChurnPredictor(model_proxy)
        return model_wrapper


    def __str__(self):
        """
        Return a string description of the model to the ``print`` method.

        Returns
        -------
        out : string
            A description of the model.
        """
        return self.__repr__()

    def _get_summary_struct(self):
        """
        Returns a structured description of the model, including (where relevant)
        the schema of the training data, description of the training data,
        training statistics, and model hyperparameters.

        Returns
        -------
        sections : list (of list of tuples)
            A list of summary sections.
              Each section is a list.
                Each item in a section list is a tuple of the form:
                  ('<label>','<field>')
        section_titles: list
            A list of section titles.
              The order matches that of the 'sections' object.
        """

        fields = [
            ("Time column", 'time_column'),
            ("User id columns", 'user_id_column'),
            ("Time unit", 'time_unit'),
            ("Time aggregate", 'default_time_aggregate'),
            ("Lookback feature periods", 'lookback_feature_periods'),
            ("Features", 'features'),
        ]

        section_titles = [ 'Model fields' ]
        return ([fields], section_titles)


    def __repr__(self):
        """
        Print a string description of the model, when the model name is entered
        in the terminal.
        """

        (sections, section_titles) = self._get_summary_struct()

        return _toolkit_repr_print(self, sections, section_titles, width=30)

    def get(self, field):
        """
        Return the value of a given field. The list of all queryable fields is
        detailed below, and can be obtained programmatically with the
        :func:`~graphlab.churn_predictor.ChurnPredictor.list_fields`
        method.

        Parameters
        ----------
        field : string
            Name of the field to be retrieved.

        Returns
        -------
        out
            Value of the requested fields.

        See Also
        --------
        list_fields
        """

        _mt._get_metric_tracker().track(
            'toolkits.churn_predictor.get')
        return self.__proxy__.get(field)

    def get_current_options(self):
        """
        Return a dictionary with the options used to define and train the model.

        Returns
        -------
        out : dict
            Dictionary with options used to define and train the model.

        See Also
        --------
        get_default_options, list_fields, get
        """

        _mt._get_metric_tracker().track(\
                  'toolkits.churn_predictor.get_current_options')
        return self.__proxy__.get_current_options()

    def list_fields(self):
        """
        List the fields stored in the model, including data, model, and training
        options. Each field can be queried with the ``get`` method.

        Returns
        -------
        out : list
            List of fields queryable with the ``get`` method.

        See Also
        --------
        get
        """

        _mt._get_metric_tracker().track(
            'toolkits.churn_predictor.list_fields')
        return self.__proxy__.list_fields()

    def predict(self,
                dataset,
                user_data = None,
                prediction_time = datetime.datetime(2100, 1, 1),
                verbose = True):
        """
        Use the trained model to obtain churn prediction for each user presented
        in the ``dataset``.

        Predictions are returned as an SFrame with three columns:
        `user_id`, `predictions`.

        Parameters
        ----------
        dataset : SFrame
            A dataset that has the same columns that were used during training.

        user_data : SFrame, optional
            Side information for the users.  This SFrame must have a column with
            the same name as what is specified by the `user_id` input parameter.
            `user_data` can provide any amount of additional user-specific
            information. The join performed is an inner join.

        prediction_time : int or datetim, optional
            Time to use as a boundary for prediction. By default the last
            time stamp of the `dataset` is used.

        verbose: boolean, optional
            When set to true, more status output is generated
            Default: True

        Returns
        -------
        out : SFrame
            An SFrame with model predictions. A prediction of 100% means the
            user is likely to stay, whereas a prediction of 0% means the
            user is likely to leave (churn).

        See Also
        --------
        create
        """

        _mt._get_metric_tracker().track(
                      'toolkits.churn_predictor.predict')

        proxy = self.__proxy__

        if (verbose): print("PROGRESS: Sorting input data by time order")
        sorted_data = proxy.sort_by_time(dataset, "")

        if (verbose): print("PROGRESS: Aggregating input data by groups")
        aggregated_by_time = proxy.aggregate_by_time(sorted_data, False, 0, "", "")


        prepare_time = int(time.mktime(prediction_time.timetuple()))
        if prediction_time == datetime.datetime(2100, 1, 1):
            if (verbose): print("PROGRESS: Generating user data for aggregate")
        else:
            if (verbose): print("PROGRESS: Generating user data for aggregate " + str(datetime.datetime.fromtimestamp(prepare_time)))

        eval_data = proxy.per_user_aggregate_with_defaults(aggregated_by_time, prepare_time, "")

        if (user_data):
            if (verbose): print("PROGRESS: Joining with user data")
            user_aggregate = user_aggregate.join(user_data, on=user_id, how="inner")

        if (verbose): print("PROGRESS: Performing predictions")
        evaluation_data = proxy.predict(eval_data)

        if (verbose): print("PROGRESS: All done!")
        return evaluation_data

    @classmethod
    def _get_queryable_methods(cls):
        """
        Returns a list of method names that are queryable through
        Predictive Services
        """
        return {'predict': {'dataset': 'sframe', 'user_data': 'sframe'}}
