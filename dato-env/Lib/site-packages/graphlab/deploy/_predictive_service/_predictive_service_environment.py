import atexit
from copy import copy
import functools
import logging as _logging
import os as _os
from requests import get, post
from time import sleep, time
from datetime import datetime, timedelta
from dateutil import parser
from ConfigParser import ConfigParser as _ConfigParser
from StringIO import StringIO as _StringIO
import tempfile as _tempfile
import shutil as _shutil
from subprocess import Popen, STDOUT
import boto.ec2.elb
import boto.ec2.cloudwatch
from boto.exception import BotoServerError
import boto.iam.connection
from urllib import quote_plus
import json

try:
    from subprocess import DEVNULL  # py3k
except ImportError:
    DEVNULL = open(_os.devnull, 'wb')

from boto import connect_s3
from boto.s3.key import Key

import graphlab as _gl
from graphlab.util import _dt_to_utc_timestamp, _utc
from graphlab.util import file_util as _file_util
from graphlab.util.file_util import parse_s3_path, s3_delete_key, s3_recursive_delete, retry
from graphlab.connect.aws._ec2 import _ec2_factory, _get_ec2_instances, _stop_instances, _ProductType

from _predictive_client import PredictiveServiceClient
from . import _MAX_CREATE_TIMEOUT_SECS

_logger = _logging.getLogger(__name__)

ENV_TYPE = 'environment type'
# The maximum number of datapoints CloudWatch will return for a given query
CW_MAX_DATA_POINTS = 1440
# The minimum allowable period (seconds) for CloudWatch
CW_MIN_PERIOD = 60
PORT_DEFAULT_NUM = 9005

# disable spurious output from requests library
_logging.getLogger("requests").setLevel(_logging.WARNING)

def predictive_service_environment_factory(env_info):
    env_type = env_info[ENV_TYPE]
    del env_info[ENV_TYPE]

    assert(env_type in ['TestStubEnv', 'LocalPredictiveServiceEnvironment', 'DockerPredictiveServiceEnvironment', 'Ec2PredictiveServiceEnvironment'])

    if(env_type == 'LocalPredictiveServiceEnvironment'):
        return LocalPredictiveServiceEnvironment(**env_info)
    elif(env_type == 'Ec2PredictiveServiceEnvironment'):
        return Ec2PredictiveServiceEnvironment(**env_info)
    elif(env_type == 'DockerPredictiveServiceEnvironment'):
        return DockerPredictiveServiceEnvironment(**env_info)

class PredictiveServiceEnvironment:
    def __init__(self, admin_key, port, aws_credentials):
        self.admin_key = admin_key
        self.certificate_name = None
        self._request_schema = None
        self._should_verify_certificate = None
        self.port = port
        self.aws_credentials = aws_credentials

    def _get_all_hosts(self):
        return []

    def _poke_host(self, host_name):
        _logger.info("Notifying: %s" % host_name)
        data = json.dumps({'admin_key': self.admin_key})
        url = 'http://%s:%s/control/poke' % (host_name, self.port)
        self._post(url, data=data)

    def get_status(self):
        pass

    def launch(self):
        pass

    def poke(self):
        pass

    def query(self, po_name, api_key, timeout, **kwargs):
        if not hasattr(self, 'client_connection'):
            if self.certificate_name:
                schema = 'https://'
            else:
                schema = 'http://'

            endpoint = schema + self.load_balancer_dns_name
            verify_cert = hasattr(self, 'certificate_is_self_signed') and not self.certificate_is_self_signed
            self.client_connection = PredictiveServiceClient(endpoint= endpoint, api_key=api_key,
                                                             should_verify_certificate = verify_cert)

        self.client_connection.set_query_timeout(timeout)
        return self.client_connection.query(po_name, **kwargs)

    def feedback(self, request_id, api_key, timeout, **kwargs):
        if not hasattr(self, 'client_connection'):
            if self.certificate_name:
                schema = 'https://'
            else:
                schema = 'http://'

            endpoint = schema + self.load_balancer_dns_name
            verify_cert = hasattr(self, 'certificate_is_self_signed') and not self.certificate_is_self_signed

            self.client_connection = PredictiveServiceClient(
                endpoint=endpoint, api_key=api_key,
                should_verify_certificate=verify_cert)

        self.client_connection.set_query_timeout(timeout)
        return self.client_connection.feedback(request_id, kwargs)

    @staticmethod
    def _get_default_log_time_range(start_time, end_time):
        '''
        Get default time range for getting Predictive Service logs

        The default is past one hour
        '''
        if not start_time and not end_time:
            start_time = (datetime.now() - timedelta(hours=1)).replace(
                        hour=0, minute=0, second=0, microsecond=0)
            end_time = datetime.utcnow()
        return (start_time, end_time)

    def get_metrics_url(self, ps_name):
        pass

    def _get_metrics(self, name, po_name, start_time, end_time, period):
        pass

    def terminate(self, delete_logs):
        pass

    def _post(self, url, data):
        if not self._should_verify_certificate:
            self._should_verify_certificate = self.__should_verify_certificate()

        headers = {'content-type': 'application/json'}
        return post(url=url, data=data, headers=headers,
                    verify=self._should_verify_certificate, timeout=10)

    def _get(self, url, params=None):
        if not self._should_verify_certificate:
            self._should_verify_certificate = self.__should_verify_certificate()

        return get(url=url, params=params, verify=self._should_verify_certificate, timeout=10)

    def __make_url(self, host_dns_name, endpoint):
        return "http://%s:%s/%s" % (host_dns_name,
                                    self.port,
                                    endpoint)

    def __cache_op_params(self, name, restart):
        data = {"admin_key": self.admin_key}

        if name:
            data.update({"name": name})

        if restart:
            data.update({"restart": restart})

        return data

    def _host_addrs(self):
        return ["localhost"]

    def _get_logs(self, log_affix, start_time=None, end_time=None):
        """
        Fetch log files of type specified by log_affix and corresponding to time
        window specified by [start_time, end_time] and return in an SFrame.
        """
        # 1. get a list of all files in log path matching log_type pattern
        # 2. filter those based on specified date range

        (start_time, end_time) = \
            PredictiveServiceEnvironment._get_default_log_time_range(start_time, end_time)

        if start_time and isinstance(start_time, str):
            start_time = parser.parse(start_time)

        if end_time and isinstance(end_time, str):
            end_time = parser.parse(end_time)

        assert(isinstance(start_time, datetime) or start_time is None)
        assert(isinstance(end_time, datetime) or end_time is None)

        start_time = _dt_to_utc_timestamp(start_time) if start_time else None
        end_time = _dt_to_utc_timestamp(end_time) if end_time else None

        if _file_util.is_s3_path(self.log_path):
            conn = connect_s3(**self.aws_credentials)
            s3_bucket_name, s3_key_name = parse_s3_path(self.log_path)
            bucket = conn.get_bucket(s3_bucket_name, validate=False)
            bucket_path = "s3://" + s3_bucket_name + "/"
            log_files = [bucket_path + log.name for log in \
                         bucket.list(prefix=s3_key_name) if \
                         PredictiveServiceEnvironment._in_time_window(\
                            log.name, start_time, end_time) and log_affix in log.name]
            _logger.info("Fetching %d log files from S3..." % len(log_files))
        elif _file_util.is_hdfs_path(self.log_path):
            # Get all HDFS files
            files = [log['path'] for log in _file_util.list_hdfs(self.log_path) \
                        if log_affix in log['path'] ]

            log_files = [log for log in files if \
                         PredictiveServiceEnvironment._in_time_window(\
                            log, start_time, end_time) ]
            _logger.info("Fetching %d log files from HDFS..." % len(log_files))
        elif _os.path.exists(self.log_path):
            log_files = [_os.path.join(self.log_path, log) for log in \
                     _os.listdir(self.log_path) if \
                     PredictiveServiceEnvironment._in_time_window(\
                        log, start_time, end_time) and log_affix in log]
            _logger.info("Fetching %d log files" % len(log_files))
        else:
            _logger.error("Log path '%s' is either not exist or is not a supported path."\
                 % self.log_path)

        if len(log_files) > 0:
            # for readability, partially apply read function
            read_log_file = functools.partial(_gl.SFrame.read_csv,
                                              header=False,
                                              column_type_hints=[str, dict])

            # create dummy first row to ensure types are correct for initializer
            init = _gl.SFrame({"X1": [""], "X2": [{}]})

            # fold each item onto the initial empty SFrame via SFrame.append
            start_time = time()
            gls_log_sf = reduce(lambda sf, f: sf.append(read_log_file(f)),
                                log_files, init)

            # remove dummy init row required for reduce
            gls_log_sf = gls_log_sf[1:]

            # convert datetime column to datetime type
            gls_log_sf["datetime"] = gls_log_sf["X1"]\
                .str_to_datetime("%Y-%m-%dT%H:%M:%S.%fZ")

            gls_log_sf.swap_columns("X1", "datetime")
            gls_log_sf.remove_column("X1")
            gls_log_sf.rename({"X2": "log"})

            # sort by timestamp
            gls_log_sf = gls_log_sf.sort("datetime")

            time_elapsed = time() - start_time
            _logger.info("Read, parsed, and merged %d log files in %d seconds" \
                         % (len(log_files), time_elapsed))

            return gls_log_sf

        return _gl.SFrame()

    @staticmethod
    def _in_time_window(log_filename, start, end):
        """
        Check whether log file with name specified log_filename was created
        within the time window specified in the outer function.
        """
        _, log_filename = _os.path.split(log_filename)
        timestamp_str = log_filename.split(".")[0]

        # convert h-m-s to h:m:s
        date_time = timestamp_str.split("T")
        timestamp_str = date_time[0] + "T" + date_time[1].replace('-', ':')

        timestamp = parser.parse(timestamp_str)
        timestamp = timestamp.replace(tzinfo=_utc)
        timestamp = _dt_to_utc_timestamp(timestamp)

        if start and end:
            return timestamp >= start and timestamp <= end
        elif start:
            return timestamp >= start
        elif end:
            return timestamp <= end

        return True

    @staticmethod
    def _verify_state_path(state_path, aws_credentials = None):
        # Verify we're not overriding another predictive service.
        state_key = _os.path.join(state_path, 'state.ini')

        if _file_util.exists(state_key, aws_credentials):
           raise IOError("There is already a Predictive Service at the specified " \
                         "location. Use a different state path. If you want to load " \
                         "an existing Predictive Service, call 'load(...)'.")

    @staticmethod
    def _get_root_path(state_path):
        if _file_util.is_s3_path(state_path):
            bucket_name, key_name = parse_s3_path(state_path)
            if key_name:
                return "s3://%s/%s/" % (bucket_name, key_name)
            else:
                return "s3://%s/" % bucket_name
        else:
            if state_path[-1] != '/':
                return state_path + '/'
            else:
                return state_path

    def _write_state_config(self, state, state_path):
        state_key = _os.path.join(state_path, 'state.ini')

        if _file_util.is_s3_path(state_key):
            return self._write_s3_state_config(state, state_path)

        # for HDFS, create a local file and then upload
        is_remote = not _file_util.is_local_path(state_key)
        if is_remote:
            tmp_dir = _tempfile.mkdtemp(prefix='predictive_service_env')
            tmp_state_file = _os.path.join(tmp_dir, 'state.ini')
        else:
            tmp_state_file = state_key

        with open(tmp_state_file, 'w') as f:
            state.write(f)

        if _file_util.is_hdfs_path(state_key):
            _file_util.upload_to_hdfs(tmp_state_file, state_key, force=True)

        if is_remote:
            _shutil.rmtree(tmp_dir)

    def _write_s3_state_config(self, state, state_path):
        bucket_name, key_name = parse_s3_path(state_path)
        state_key = key_name + '/state.ini'

        # Write state file to S3
        state_fp = _StringIO()
        state.write(state_fp)
        state_fp.flush()
        state_fp.seek(0)
        conn = connect_s3(**self.aws_credentials)
        bucket = conn.get_bucket(bucket_name, validate=False)
        key = Key(bucket)
        key.key = state_key
        key.set_contents_from_file(state_fp)
        state_fp.close()

    @staticmethod
    def _get_s3_state_config(state_path, aws_credentials):
        bucket_name, key_name = parse_s3_path(state_path)
        state_key = key_name + '/state.ini'
        conn = connect_s3(**aws_credentials)
        bucket = conn.get_bucket(bucket_name, validate=False)
        key = bucket.get_key(state_key)

        if not key:
            raise IOError("No Predictive Service at the specified location.")

        contents = key.get_contents_as_string()
        config = _ConfigParser(allow_no_value=True)
        config.optionxform = str
        cont_fp = _StringIO(contents)
        cont_fp.seek(0)
        config.readfp(cont_fp)
        cont_fp.close()

        return config

    @staticmethod
    def _get_state_from_file(state_path, aws_credentials):
        state_key = _os.path.join(state_path, 'state.ini')

        if _file_util.is_s3_path(state_path):
            return PredictiveServiceEnvironment._get_s3_state_config(state_path, aws_credentials)

        # Download if it is remote
        is_remote = not _file_util.is_local_path(state_key)
        if is_remote:
            tmp_dir = _tempfile.mkdtemp(prefix='predictive_service_env')
            tmp_state_file = _os.path.join(tmp_dir, 'state.ini')
            if _file_util.is_hdfs_path(state_key):
                _file_util.download_from_hdfs(state_key, tmp_state_file)
            else:
                if not _os.path.isdir(state_path):
                    raise RuntimeError('%s must be a local folder.' % state_path)
        else:
            tmp_state_file = state_key

        config = _ConfigParser(allow_no_value=True)
        config.optionxform = str
        config.read(tmp_state_file)

        if is_remote:
            _shutil.rmtree(tmp_dir)

        return config

    def remove_state(self, state_path, deps_path, po_path):
        _logger.info('Deleting state data.')

        if _file_util.is_s3_path(state_path):
            self._remove_s3_state(state_path, deps_path, po_path)
        elif _file_util.is_hdfs_path(state_path):
            self._remove_hdfs_state(state_path, deps_path, po_path)
        else:
            self._remove_local_state(state_path, deps_path, po_path)

    def _remove_s3_state(self, state_path, deps_path, po_path):
        _logger.info('Deleting s3 state data.')
        bucket_name, key_name = parse_s3_path(state_path)
        state_key = key_name + '/state.ini'
        # make sure we have a valid s3 path
        if key_name:
            s3_root_path = "s3://%s/%s/" % (bucket_name, key_name)
        else:
            s3_root_path = "s3://%s/" % bucket_name
        # remove necessary folders and files
        try:
            s3_recursive_delete(s3_root_path + deps_path,
                                self.aws_credentials)
            s3_recursive_delete(s3_root_path + po_path,
                                self.aws_credentials)
            s3_delete_key(bucket_name, state_key, self.aws_credentials)
        except:
            _logger.error("Could not delete predictive object data from S3. "
                          "Please manually delete data under: %s" %
                          s3_root_path)

    def _remove_hdfs_state(self, state_path, deps_path, po_path):
        deps = _os.path.join(state_path, deps_path)
        po = _os.path.join(state_path, po_path)
        state_key = _os.path.join(state_path, 'state.ini')

        try:
            _file_util.remove_hdfs(deps)
            _file_util.remove_hdfs(po)
            _file_util.remove_hdfs(state_key)
        except Exception as e:
            _logger.error("Could not delete predictive object data from HDFS. %s" % e)

    def _remove_local_state(self, state_path, deps_path, po_path):
        if not _os.path.exists(state_path) or not _os.path.isdir(state_path):
            raise IOError("Cannot remote state at state path: %s" % state_path)

        deps = _os.path.join(state_path, deps_path)
        po = _os.path.join(state_path, po_path)
        state_key = _os.path.join(state_path, 'state.ini')
        # remove necessary folders and files
        try:
            _shutil.rmtree(deps)
            _shutil.rmtree(po)
            _os.remove(state_key)
        except:
            _logger.error("Could not delete predictive object data from state path. "
                          "Please manually delete data under: %s" %
                          state_path)

    @retry()
    def cache_enable(self, name, restart=True):
        for host in self._host_addrs():
            try:
                url = self.__make_url(host, "control/cache_enable")

                params = self.__cache_op_params(name, restart)

                self._post(url, json.dumps(params))
            except Exception as e:
                _logger.error('Could not enable cache on %s: %s' %
                              (host, e.message))

    @retry()
    def cache_disable(self, name):
        for host in self._host_addrs():
            try:
                url = self.__make_url(host, "control/cache_disable")

                params = self.__cache_op_params(name, False)

                self._post(url, json.dumps(params))
            except Exception as e:
                _logger.error('Could not disable cache on %s: %s' %
                              (host, e.message))

    @retry()
    def cache_clear(self, name):
        for host in self._host_addrs():
            try:
                url = self.__make_url(host, "control/cache_clear")

                params = self.__cache_op_params(name, False)

                self._post(url, json.dumps(params))
            except Exception as e:
                _logger.error('Could not clear cache on %s: %s' %
                              (host, e.message))

    @retry()
    def flush_logs(self):
        for host in self._host_addrs():
            try:
                url = self.__make_url(host, "control/flush_logs")
                self._post(url, json.dumps({"admin_key": self.admin_key}))
            except Exception as e:
                _logger.error("Could not flush logs on %s: %s" %
                              (host, e.message))

    def reconfigure(self, system_conf):
        for host in self._host_addrs():
            try:
                url = self.__make_url(host, "control/reconfigure")
                post_body = system_conf.for_json()
                post_body.update({"admin_key": self.admin_key})
                self._post(url, json.dumps(post_body))
            except Exception as e:
                _logger.error("Could not reconfigure host %s: %s"
                              % (host, e.message))
                raise

    def get_node_status(self, host_addr):
        url = self.__make_url(host_addr, "control/status")

        try:
            response = self._post(url, json.dumps(
                {"admin_key": self.admin_key}))
        except Exception as e:  # TimeoutError, ConnectionError, etc.
            return {"error": "Cannot get status for host %s, error: %s"
                    % (host_addr, e.message)}

        try:
            _logger.debug(response.text)
            data = json.loads(response.text)
        except Exception as e:
            return {"error": "Cannot get status for host %s, error: %s"
                    % (host_addr, e.message)}

        data.update({"public_dns_name": host_addr})
        return data

    def __is_using_certificates(self):
        return hasattr(self, 'certificate_name') and self.certificate_name

    def __get_schema(self):
        if not self._request_schema:
            self._request_schema = 'https' if self.__is_using_certificates() else 'http'
        return self._request_schema

    def __should_verify_certificate(self):
        return self.__is_using_certificates() and not (hasattr(self, 'certificate_is_self_signed') and self.certificate_is_self_signed)

    def _is_cache_ok(self, expected_cache_status, status=None):
        """
        Checks the state of the cache.

        Parameters
        ----------
        expected_cache_status : str ("healthy" or "disabled")
            The expected state of the system

        status : list[dict], optional
            The deployment's status as returned in a call to `get_status`. The
            list has as many elements as there are nodes in the deployment. Each
            element should contain the following fields: "cache", "dns_name",
            "id", "models", "reason", and "state". The "cache field is expected
            to be a dictionary. If left unspecfied (or set to None), we make a
            control-plane (/control/status) request to each node in the cluster.
        """
        status = status or self.get_status()

        # is cache either healthy or disabled on all nodes?
        def healthy_or_disabled(cache_status):
            if isinstance(cache_status, dict):
                if cache_status == {}:
                    cache_status = "disabled"
                elif cache_status.get("healthy"):
                    cache_status = "healthy"
                else:
                    cache_status = "unhealthy"

            if type(cache_status) in [str, unicode]:
                return cache_status

            raise RuntimeError("Unexpected value for cache_status: %s",
                               str(cache_status))

        node_cache_statuses = [healthy_or_disabled(x.get("cache", {})) \
                               for x in status]

        if len(node_cache_statuses) == 0:
            return True

        if len(set(node_cache_statuses)) > 1 \
          or "unhealthy" in node_cache_statuses:
            return False

        # is expected cache status the same as actual cache status?
        if expected_cache_status != node_cache_statuses[0]:
            return False

        # if cache is disabled globally, is it disabled for all models?
        for node_index, node_status in enumerate(status):
            node_cache_status = node_cache_statuses[node_index]
            model_caches = [model_status.get("cache_enabled") for model_status \
                            in node_status.get("models", [])]

            if node_cache_status == "disabled" and any(model_caches):
                return False

        # are all nodes using the same type of cache?
        cache_statuses = [x["cache"] for x in status]
        node_cache_types = {x.get("type", "nocache") if isinstance(x, dict) else "nocache" \
                            for x in cache_statuses}

        if len(node_cache_types) > 1:
            return False

        return True

class LocalPredictiveServiceEnvironment(PredictiveServiceEnvironment):
    def __init__(self, log_path, admin_key, aws_credentials=None, num_hosts = 1, redis_manager = None,
                 web_server = None, graphlab_service = None,
                 port = PORT_DEFAULT_NUM, **kwargs):
        PredictiveServiceEnvironment.__init__(self, admin_key, port, aws_credentials)
        self.log_path = log_path
        self.num_hosts = int(num_hosts)
        self.redis_manager = redis_manager
        self.web_server = web_server
        self.graphlab_service = graphlab_service
        self.load_balancer_dns_name = 'localhost:%s' % port
        self.certificate_is_self_signed = True

    def _get_state(self, schema_version):
        result = {}
        result[ENV_TYPE] = 'LocalPredictiveServiceEnvironment'
        result['num_hosts'] = self.num_hosts
        result['log_path'] = self.log_path
        result['admin_key'] = self.admin_key
        result['port'] = self.port
        return result

    def get_status(self):
        node_status = self.get_node_status('localhost')
        return [{
            'id': 'localhost',
            'dns_name': 'localhost',
            'state':'InService',
            'reason': 'N/A',
            'cache': node_status.get('cache') if not node_status.has_key('error') else None,
            'models': node_status.get('models') if not node_status.has_key('error') else None,
            }]

    def _is_cache_ok(self, expected_cache_status, status=None):
        # we should implement this
        return True

    def terminate(self, delete_logs):
        _logger.info('Terminating service.')
        try:
            if self.web_server:
                _logger.info('Terminating web server.')
                self.web_server.terminate()
        except:
            pass

        try:
            if self.graphlab_service:
                _logger.info('Terminating graphlab_service.')
                self.graphlab_service.terminate()
        except:
            pass

        try:
            if self.redis_manager:
                _logger.info('Terminating redis manager.')
                self.redis_manager.terminate()
        except:
            pass

        if delete_logs:
            _logger.info('Deleting log files.')
            try:
                s3_recursive_delete(self.log_path)
            except:
                _logger.info("Could not delete log file. Please manually delete files under: %s"
                             % self.log_path)

    @staticmethod
    def launch(predictive_service_path, log_path, num_hosts, admin_key, port):
        node_manager_dir = _os.environ['NODE_MANAGER_ROOT']

        # Start Redis for caching
        env_vars = copy(_os.environ)
        env_vars['PYTHONPATH'] = _os.pathsep.join([env_vars['PYTHONPATH'], '.'])
        env_vars['PREDICTIVE_SERVICE_STATE_PATH'] = predictive_service_path
        env_vars['LOG_PATH'] = log_path

        env_vars['REDIS_TRIB_PATH'] = _os.environ['REDIS_TRIB_PATH']
        env_vars['REDIS_SERVER_PATH'] = _os.environ['REDIS_SERVER_PATH']

        redis_manager = Popen(
            ['python', 'redismanager/redis_manager.py', str(max(3, num_hosts))],
            cwd = node_manager_dir, env = env_vars, stdout=DEVNULL, stderr=STDOUT)

        _logger.info(
            "Running Redis manager with PID: %d" % (redis_manager.pid))

        # Start the Predictive Service
        web_server = Popen(
            ['python', 'psws/ps_server.py'],
              cwd = node_manager_dir, env = env_vars, stdout=DEVNULL, stderr=STDOUT)

        _logger.info(
            "Running web server with PID: %d" % (web_server.pid))

        # Start GraphLab Service
        graphlab_service = Popen(
            ['python', 'glservice/graphlab_service.py'],
            cwd = node_manager_dir, env = env_vars,  stdout=DEVNULL, stderr=STDOUT)

        _logger.info(
            "Running GraphLab Create service with PID: %d" % (graphlab_service.pid))

        instance = LocalPredictiveServiceEnvironment(
            log_path, admin_key, port, num_hosts, redis_manager = redis_manager,
            web_server = web_server, graphlab_service = graphlab_service)

        atexit.register(functools.partial(instance.terminate, False))

        return instance

    def poke(self):
        self._poke_host('localhost')

class DockerPredictiveServiceEnvironment(PredictiveServiceEnvironment):

    def __init__(self,
        load_balancer_dns_name,
        log_path,
        admin_key,
        aws_credentials = None,
        port = PORT_DEFAULT_NUM,
        load_balancer_stats_port = PORT_DEFAULT_NUM - 1,
        metrics_server = None):
        PredictiveServiceEnvironment.__init__(self, admin_key, port, aws_credentials)
        self.load_balancer_dns_name = load_balancer_dns_name
        self.log_path = log_path
        self.ctl_port = load_balancer_stats_port
        self.metrics_server = metrics_server

    def __repr__(self):
        ret = ""
        ret += 'DockerPredictiveServiceEnvironment:\n'
        ret += '\tload_balancer_dns_name: %s\n' % str(self.load_balancer_dns_name)
        ret += '\tlog_path: %s\n' % str(self.log_path)
        return ret

    def __str__(self):
        return self.__repr__()

    def _get_state(self, schema_version):
        result = {}
        result[ENV_TYPE] = 'DockerPredictiveServiceEnvironment'
        result['log_path'] = self.log_path
        result['admin_key'] = self.admin_key
        result['load_balancer_dns_name'] = self.load_balancer_dns_name
        result['load_balancer_stats_port'] = self.ctl_port
        result['metrics_server'] = self.metrics_server
        result['port'] = self.port
        return result

    def get_status(self, _show_errors=True):
        result = []
        for host in self._get_all_hosts():
            dns_name, port = host['svname'].split(':')
            node_status = self.get_node_status(dns_name)
            result.append({
                'id': dns_name,
                'dns_name': dns_name,
                'state': host['status'],
                'reason': host['last_chk'],
                'cache': node_status.get('cache') if not node_status.has_key('error') else None,
                'models': node_status.get('models') if not node_status.has_key('error') else None,
                })

            if _show_errors and node_status.has_key('error'):
                _logger.error('Cannot get node status from %s, error: %s' % \
                    (dns_name, node_status.get('error')))

        return result

    def _check_metrics_name(self, name):
        '''
        Checks if the metrics name is valid or not
        '''
        name_list = ['requests', 'latency', 'cache::hits', 'cache::misses', 'cache::latency',
                     'cache::num_keys', 'num_hosts_in_cluster', 'num_objects_queryable',
                     'num_objects_registered', 'gls::latency']
        return name in name_list

    def _format_metrics(self, metric_name, sf):
        '''
        Format the raw metrics SFrame.
        '''
        # get rename columns (value -> sum/maximum) and list of columns to be returned
        rename_columns = {}
        columns = ['time', 'unit']
        if metric_name in ['num_hosts_in_cluster', 'num_objects_queryable',
                           'num_objects_registered']:
            rename_columns['value'] = 'maximum'
            columns.append('maximum')
        elif 'latency' not in metric_name:
            rename_columns['value'] = 'sum'
            columns.append('sum')
        else:
            columns.append('average')

        # rename columns
        sf.rename(rename_columns)

        # get only the needed columns
        sf = sf[columns]

        # convert time column to datetime column
        sf['time'] = sf['time'].apply(lambda x : datetime.strptime(x, '%Y-%m-%dT%H:%M:%SZ'))

        return sf

    def _get_metrics(self, name, ps_name, is_po, start_time, end_time, period):
        '''
        Get metrics from Docker Predictive Service.
        '''
        # check if metrics server exists
        if not self.metrics_server:
            return None

        result = {}
        url = _os.path.join('http://', self.metrics_server, 'query')
        data = {'start': start_time, 'end': end_time, 'period': period,
                'ps_name': ps_name}

        # get metrics
        if not name:
            # when name is None, return both requests and latency
            data['metrics_name'] = 'requests'
            result['requests'] = self._get(url, params=data)
            data['metrics_name'] = 'latency'
            result['latency'] = self._get(url, params=data)
        elif name and not is_po:
            # normal metrics
            data['metrics_name'] = name
            result[name] = self._get(url, params=data)
        elif name and is_po:
            # PO metrics, get both requests and latency
            data['metrics_name'] = name + '::requests'
            result[data['metrics_name']] = self._get(url, params=data)
            data['metrics_name'] = name + '::latency'
            result[data['metrics_name']] = self._get(url, params=data)

        # convert from dictionary to SFrames
        metrics = {}
        for r in result:
            response = result[r]
            if response.status_code == 200:
                content = json.loads(response.content)['result']
                sf = _gl.SArray(content).unpack('')
                # get formatted metrics
                metrics[r] = self._format_metrics(r, sf)
            else:
                metrics[r] = 'Unable to get metrics for %s.' % r

        return metrics

    def get_metrics_url(self, ps_name):
        _logger.warn("Get metrics not supported for Docker Predictive Service.")

    def terminate(self, remove_logs):
        _logger.warn("Cannot terminated your Docker Predictive Service from client.")

    def terminate_instances(self, instance_ids):
        _logger.warn("Cannot terminate instance(s) on Docker Predictive Service from client.")

    def add_instances(self, instance_ids):
        _logger.warn("Cannot add instance(s) on Docker Predictive Service from client.")

    def poke(self):
        for hostname in self._host_addrs():
            try:
                self._poke_host(hostname)
            except Exception as e:
                _logger.error('Could not notify %s: %s' % (hostname, e.message))

    def _get_all_hosts(self):
        hostname, port = self.load_balancer_dns_name.split(':')
        ctl_dns = 'http://' + hostname + ':' + str(self.ctl_port)
        stats_url = ctl_dns + '/stats;csv'
        stats_resp = self._get(stats_url)

        # clean up raw status
        stats = stats_resp.content[2:] # remove the '# ' in the start
        stats = stats.split('\n')
        column_names = stats[0].split(',')
        stats_list = []
        for row in stats[1:]:
            r = row.split(',')
            if len(r) != len(column_names):
                continue
            stat = {column_names[idx]: r[idx] for idx in range(0, len(column_names))}
            stats_list.append(stat)

        # obtain ps server status
        ps_server_status = []
        for st in stats_list:
            if st['pxname'] == 'ps_servers' and st['svname'] != 'BACKEND':
                ps_server_status.append(st)

        return ps_server_status

    def _host_addrs(self):
        result = []
        server_status = self._get_all_hosts()
        for stat in server_status:
            hostname, port = stat['svname'].split(':')
            result.append(hostname)

        return result

class Ec2PredictiveServiceEnvironment(PredictiveServiceEnvironment):

    def __init__(self, load_balancer_dns_name, region, log_path, admin_key,
                 certificate_name, certificate_is_self_signed, aws_credentials,
                 port=PORT_DEFAULT_NUM):
        PredictiveServiceEnvironment.__init__(self, admin_key,
            port=port, aws_credentials=aws_credentials)
        self.load_balancer_dns_name = load_balancer_dns_name
        self.region = region
        self.log_path = log_path
        self.certificate_name = certificate_name
        self.certificate_is_self_signed = certificate_is_self_signed

    def __repr__(self):
        ret = ""
        ret += 'Ec2PredictiveServiceEnvironment:\n'
        ret += '\tload_balancer_dns_name: %s\n' % str(self.load_balancer_dns_name)
        ret += '\tregion: %s\n' % str(self.region)
        ret += '\tlog_path: %s\n' % str(self.log_path)
        ret += '\tcertificate_name: %s\n' % str(self.certificate_name)
        if self.certificate_name:
            ret += '\tcertificate_is_self_signed: %s\n' % str(self.certificate_is_self_signed)
        return ret

    def __str__(self):
        return self.__repr__()

    @staticmethod
    def launch(name, ec2_config, s3_predictive_object_path, num_hosts, admin_key,
               ssl_credentials, aws_credentials, started, port):
        # Verify we're not overriding another predictive service.
        PredictiveServiceEnvironment._verify_state_path(s3_predictive_object_path,
                                                           aws_credentials)

        s3_log_path = "%s/logs" % (s3_predictive_object_path)
        user_data = {
            'aws_access_key': aws_credentials['aws_access_key_id'],
            'aws_secret_key': aws_credentials['aws_secret_access_key'],
            'predictive_service_state_path': s3_predictive_object_path
            }

        # add tags for all EC2 instances to indicate they are related to this Predictive Service
        tags = {}
        if ec2_config.tags:
            tags.update(ec2_config.tags)
        tags.update({'Name':name, 'predictive_service':name})

        cidr_ip = ec2_config.cidr_ip if hasattr(ec2_config, 'cidr_ip') else None

        _logger.info("[Step 1/5]: Launching EC2 instances.")

        # Start the hosts up.
        ec2_hosts, security_group, subnet_id = _ec2_factory(ec2_config.instance_type,
            region = ec2_config.region,
            CIDR_rule = cidr_ip,
            security_group_name = ec2_config.security_group,
            tags = tags, user_data = user_data,
            credentials = aws_credentials, num_hosts = num_hosts,
            ami_service_parameters = {'service': 'predictive'},
            additional_port_to_open = port,
            product_type = _ProductType.PredictiveServices,
            subnet_id = ec2_config.subnet_id,
            security_group_id = ec2_config.security_group_id)

        if num_hosts == 1:
            ec2_hosts = [ec2_hosts]

        lb = None
        try:
            # Determine host ids and availability zones used.
            zones, host_ids, is_vpc = set(), [], False
            for i in ec2_hosts:
                zones.add(i.instance.placement)
                host_ids.append(i.instance_id)

            is_vpc = bool(ec2_hosts[0].instance.vpc_id)

            certificate_name, certificate_is_self_signed = None, None
            if ssl_credentials:
                # Using HTTPS
                (private_key_path, public_certificate_path, certificate_is_self_signed) = ssl_credentials
                certificate_name = name
                certificate_id = Ec2PredictiveServiceEnvironment._upload_ssl_info(certificate_name,
                                                                                  private_key_path,
                                                                                  public_certificate_path,
                                                                                  aws_credentials)
                listener_tuple = (443, port, 'https', certificate_id)
            else:
                # Using HTTP
                _logger.info("WARNING: Launching Predictive Service without SSL certificate!")
                listener_tuple = (80, port, 'http')

            conn = boto.ec2.elb.connect_to_region(ec2_config.region, **aws_credentials)

            # Create the load balancer.
            _logger.info("[Step 2/5]: Launching Load Balancer.")

            while ((datetime.now() - started).total_seconds() < _MAX_CREATE_TIMEOUT_SECS):
                try:
                    if subnet_id is None:
                        lb = conn.create_load_balancer(name, zones, [listener_tuple])
                    else:
                        lb = conn.create_load_balancer(name, zones = None, listeners = [listener_tuple], subnets = [subnet_id])
                    break
                except BotoServerError as e:
                    # We just uploaded the certificate, so there's a good chance it will not be found, yet.
                    if "<Code>CertificateNotFound</Code>" not in str(e):
                        raise e
                    sleep(1)
            else:
                raise RuntimeError("Unable to successfully create load balancer. Please confirm in AWS Management Console")

            if is_vpc:
                lb.apply_security_groups([security_group.id])

            _logger.info("[Step 3/5]: Configuring Load Balancer.")

            # Configure healthCheck
            health_target = "HTTP:%s/control/healthcheck" % port
            health_check = boto.ec2.elb.HealthCheck(interval=20, healthy_threshold=3,
                                                    unhealthy_threshold=5,
                                                    target=health_target)
            lb.configure_health_check(health_check)

            # Add EC2 instances to the load balancer.
            lb.register_instances(host_ids)

        except Exception as e:
            _logger.error("Could not create or configure the load balancer, terminating EC2 instances." \
                "Exception: %s" % e.message)
            #TODO: _stop_instances can raise exception
            _stop_instances([h.instance_id for h in ec2_hosts], ec2_config.region,
                            credentials = aws_credentials)

            if lb:
                _logger.info("Deleting the load balancer.")
                lb.delete()

            raise

        return Ec2PredictiveServiceEnvironment(lb.dns_name, ec2_config.region, s3_log_path, admin_key,
                                               certificate_name, certificate_is_self_signed,
                                               aws_credentials, port)

    def _get_metric_info(self, name):
        info = dict()
        info['unit'] = 'Seconds' if 'latency' in name else 'Count'
        if 'latency' in name:
            info['statistics'] = 'Average'
        elif name in ['num_hosts_in_cluster', 'num_objects_queryable', 'num_objects_registered']:
            info['statistics'] = 'Maximum'
        else:
            info['statistics'] = 'Sum'
        return info

    def _check_metrics_name(self, name):
        '''
        Checks if the metrics name is valid or not
        '''
        name_list = ['requests', 'latency', 'cache::hits', 'cache::misses', 'cache::latency',
                     'cache::num_keys', 'num_hosts_in_cluster', 'num_objects_queryable',
                     'num_objects_registered', 'gls::latency', 'HealthyHostCount']
        return name in name_list

    def _rename_columns(self, name_list):
        '''
        Rename columns:
            - 'Timestamp' -> 'time'
            - convert all column names to lowercase str
        '''
        new_columns = {}
        for i in name_list:
            if i == 'Timestamp':
                new_columns[i] = 'time'
            else:
                new_columns[i] = i.lower()
        return new_columns

    @retry()
    def _get_metrics(self, name, ps_name, is_po, start_time, end_time, period=300):
        # convert unix timestamps to datetime in UTC
        start_time = datetime.utcfromtimestamp(float(start_time))
        end_time = datetime.utcfromtimestamp(float(end_time))

        # Calculate time range for period determination/sanity-check
        delta = end_time - start_time
        delta_seconds = ( delta.days * 24 * 60 * 60 ) + delta.seconds + 1 #round microseconds up

        # Determine min period as the smallest multiple of 60 that won't result in too many data points
        min_period = 60 * int(delta_seconds / CW_MAX_DATA_POINTS / 60)
        if ((delta_seconds / CW_MAX_DATA_POINTS) % 60) > 0:
            min_period += 60

        # set the min period if necessary
        if period < min_period:
            period = min_period
        if period < CW_MIN_PERIOD:
            period = CW_MIN_PERIOD

        # get cloud watch handle
        cw = self._get_cloudwatch_handle()

        result = {}
        # get metrics that are specific to the entire Predictive Service
        if not name:
            requests_info = self._get_metric_info('requests')
            result['requests'] = cw.get_metric_statistics(period, start_time, end_time,
                        'requests', ps_name, [requests_info['statistics']], {}, requests_info['unit'])
            latency_info = self._get_metric_info('latency')
            result['latency'] = cw.get_metric_statistics(period, start_time, end_time,
                        'latency', ps_name, [latency_info['statistics']], {}, latency_info['unit'])
        elif name and not is_po:
            if name == 'HealthyHostCount':
                result['HealthyHostCount'] = cw.get_metric_statistics(period, start_time, end_time,
                        'HealthyHostCount', 'AWS/ELB', ['Average'], {"LoadBalancerName": ps_name}, 'Count')
            else:
                info = self._get_metric_info(name)
                result[name] = cw.get_metric_statistics(period, start_time, end_time,
                        name, ps_name, [info['statistics']], {}, info['unit'])
        elif name and is_po: # get metrics for a specific predictive object
            metrics_list = cw.list_metrics(namespace=ps_name)
            po_name_metrics = [str(m.name) for m in metrics_list if str(m.name).startswith(name)]
            for po_name_m in po_name_metrics:
                info = self._get_metric_info(po_name_m)
                result[po_name_m] = cw.get_metric_statistics(period, start_time, end_time,
                            po_name_m, ps_name, [info['statistics']], {}, info['unit'])

        metrics = {}
        for key in result:
            m = map(dict, sorted(result[key]))
            if len(m) > 0:
                sort_column_name = "Timestamp"
                sf = _gl.SArray(m).unpack('').sort(sort_column_name)
                metrics[key] = sf.rename(self._rename_columns(sf.column_names()))

        return metrics

    def get_metrics_url(self, ps_name):
        return 'https://console.aws.amazon.com/cloudwatch/home?region=' \
               '%s#metrics:metricFilter=Pattern%%253D%s' % (self.region, quote_plus(ps_name))

    def _get_cloudwatch_handle(self):
        try:
            conn = boto.ec2.cloudwatch.connect_to_region(self.region, **self.aws_credentials)
            return conn
        except Exception:
            _logger.error("Unable to connect to to CloudWatch in region '%s'" % self.region)
        raise Exception("Cannot connect to CloudWatch in region '%s'." % self.region)

    def _get_load_balancer_handle(self):
        try:
            conn = boto.ec2.elb.connect_to_region(self.region, **self.aws_credentials)
            for i in conn.get_all_load_balancers():
                if i.dns_name == self.load_balancer_dns_name:
                    return i
        except Exception:
            _logger.error("Unable to connect to ELB with name '%s' in region '%s'"
                          % (self.load_balancer_dns_name, self.region))

        raise Exception("Cannot find load balancer with name '%s' in region '%s'."
                        % (self.load_balancer_dns_name, self.region))

    @retry()
    def get_status(self, _show_errors=True):
        result = []
        load_balancer = self._get_load_balancer_handle()

        for host in load_balancer.get_instance_health():
            host_id = host.instance_id
            host_instance = _get_ec2_instances([host_id], self.region,
                                               aws_credentials = self.aws_credentials)

            dns_name = host_instance[0].public_dns_name
            node_status = self.get_node_status(dns_name)
            result.append({
                'id': host_id,
                'dns_name': dns_name,
                'state': host.state,
                'reason': host.reason_code,
                'cache': node_status.get('cache') if not node_status.has_key('error') else None,
                'models': node_status.get('models') if not node_status.has_key('error') else None,
                })

            if _show_errors and node_status.has_key('error'):
                _logger.error('Cannot get node status from %s, error: %s' % \
                    (host_id, node_status.get('error')))

        return result

    def _get_state(self, schema_version):
        result = {}
        result[ENV_TYPE] = 'Ec2PredictiveServiceEnvironment'
        result['load_balancer_dns_name'] = self.load_balancer_dns_name
        result['region'] = self.region
        result['certificate_name'] = self.certificate_name
        result['log_path'] = self.log_path
        result['admin_key'] = self.admin_key
        result['certificate_is_self_signed'] = self.certificate_is_self_signed

        # port is added in schema_version 4
        if schema_version >= 4:
            result['port'] = self.port
        return result

    def poke(self):
        for instance in self._get_all_hosts():
            try:
                self._poke_host(instance.public_dns_name)
            except Exception as e:
                _logger.error('Could not notify %s: %s' % (instance.public_dns_name, e.message))

    @retry()
    def _get_instance_attributes(self, instance_id = None):
        conn = boto.ec2.connect_to_region(self.region, **self.aws_credentials)

        if not instance_id:
            hosts = self._get_all_hosts()
            if len(hosts) == 0:
                raise RuntimeError('There is no hosts available!')

            # pick first one
            instance_id = hosts[0].id

        reservations = conn.get_all_instances([instance_id])
        instance = reservations[0].instances[0]

        # get instance type
        instance_type = instance.instance_type

        # get instance security group
        security_groups = instance.groups
        security_group = None
        if len(security_groups) > 0:
            security_group = security_groups[0].name

        # get instance tags
        tags = instance.tags or {}

        return {"instance_type": instance_type,
                "security_group_name": security_group,
                "tags": tags}

    def terminate_instances(self, instance_ids):
        # terminate specified instances
        try:
            _stop_instances(instance_ids, self.region,
                            credentials=self.aws_credentials)
        except Exception:
            _logger.error("Could not stop instances %s. Please terminate " \
                          "the instances manually if they're still running."
                          % str(instance_ids))

        # remove specified instances from the load balancer
        try:
            load_balancer = self._get_load_balancer_handle()
            load_balancer.deregister_instances(instance_ids)
        except:
            _logger.error("Could not remove instances %s from load balancer."
                          % str(instance_ids))

    @retry()
    def add_instances(self, s3_state_path, num_hosts, instance_type,
                      security_group_name, tags, CIDR_rule=None,
                      additional_port_to_open=None):
        # get a load balancer handle, which we'll use for:
        #   1. getting an availability zone if one wasn't passed in
        #   2. getting the current cluster instances
        #   3. adding the replacement node to the load balancer
        lb = self._get_load_balancer_handle()
        availability_zone = lb.availability_zones[0]

        user_data = {
            'aws_access_key': self.aws_credentials['aws_access_key_id'],
            'aws_secret_key': self.aws_credentials['aws_secret_access_key'],
            'predictive_service_state_path': s3_state_path}

        # launch new instance
        instance_ids = []
        try:
            ec2_hosts, security_group, subnet_id  = _ec2_factory(instance_type, region=self.region,
                                     availability_zone=availability_zone,
                                     CIDR_rule=CIDR_rule,
                                     security_group_name=security_group_name,
                                     tags=tags, user_data=user_data,
                                     credentials=self.aws_credentials,
                                     num_hosts=num_hosts,
                                     ami_service_parameters={'service': 'predictive'},
                                     additional_port_to_open=additional_port_to_open,
                                     product_type=_ProductType.PredictiveServices)

            # add to the load balancer
            if not isinstance(ec2_hosts, list):
                ec2_hosts = [ec2_hosts]

            instance_ids = [i.instance_id for i in ec2_hosts]
            lb.register_instances(instance_ids)

            # poll load balancer until instances are available
            starttime = datetime.now()

            def seconds_elapsed():
                return (datetime.now() - starttime).total_seconds()

            while seconds_elapsed() < _MAX_CREATE_TIMEOUT_SECS:
                # query status, verify all InService
                nodes = self.get_status(_show_errors=False)
                nodes_in_service = [node["state"] == "InService" for node in nodes]

                if all(nodes_in_service):
                    _logger.info("Cluster is fully operational, [%d/%d] " \
                                 "instances currently in service.",
                                 nodes_in_service.count(True),
                                 len(nodes_in_service))
                    break
                else:
                    _logger.info("Cluster not fully operational yet, [%d/%d] " \
                                 "instances currently in service.",
                                 nodes_in_service.count(True),
                                 len(nodes_in_service))
                    sleep(15)

            if not all(nodes_in_service):
                raise RuntimeError("Unable to successfully add new instances to " \
                                   "load balancer. Please confirm in AWS " \
                                   "Management Console")
        except Exception as e:
            _logger.error(e.message)
            if instance_ids:
                self.terminate_instances(instance_ids)
            raise

    def terminate(self, delete_logs):
        try:
            load_balancer_handle = self._get_load_balancer_handle()
            conn = boto.ec2.elb.connect_to_region(self.region, **self.aws_credentials)
            _logger.info("Deleting load balancer: %s" % load_balancer_handle.name)
            conn.delete_load_balancer(load_balancer_handle.name)
        except:
            _logger.error("Could not delete load balancer. Please manually delete the following load "
                      "balancer: %s" % self.load_balancer_dns_name)

        # Terminate all hosts
        try:
            host_ids = [i.id for i in load_balancer_handle.instances]
            _stop_instances(host_ids, self.region, credentials = self.aws_credentials)
        except:
            _logger.error("Could not terminate hosts. Please manually terminate from the AWS console.")

        if delete_logs:
            _logger.info('Deleting log files.')
            try:
                s3_recursive_delete(self.log_path, self.aws_credentials)
            except:
                _logger.info("Could not delete log file. Please manually delete files under: %s"
                             % self.log_path)

        # Delete the server certificate
        if self.certificate_name:
            conn = boto.iam.connection.IAMConnection(**self.aws_credentials)
            conn.delete_server_cert(self.certificate_name)

    def _get_all_hosts(self):
        load_balancer_handle = self._get_load_balancer_handle()

        host_ids = [i.id for i in load_balancer_handle.instances]

        if len(host_ids) > 0:
            return _get_ec2_instances(host_ids, self.region, self.aws_credentials)

        return []

    def _host_addrs(self):
        addrs = []
        for inst in self._get_all_hosts():
            ip = inst.public_dns_name
            if not ip:
                _logger.error("Unable to get IP address for node with ID " \
                              "%s" % inst.id)
            addrs.append(ip)
        return addrs

    @staticmethod
    def _upload_ssl_info(certificate_name, private_key_path, public_certificate_path,
                         credentials):
        # Read in private key and public certificate
        with open(private_key_path, 'r') as file:
            private_key = file.read()
        with open(public_certificate_path, 'r') as file:
            cert = file.read()

        # Upload private key and public certificate
        conn = boto.iam.connection.IAMConnection(**credentials)
        resp = conn.upload_server_cert(certificate_name, cert, private_key)

        # Get the certificate id
        certificate_id =  resp['upload_server_certificate_response'] \
            ['upload_server_certificate_result']['server_certificate_metadata']['arn']

        return certificate_id

class TestEc2Env(Ec2PredictiveServiceEnvironment):

    class Host(object):
        def __init__(self):
            self.private_ip_address = 0

    def __init__(self, aws_credentials):
        Ec2PredictiveServiceEnvironment.__init__(self,
                'load_balancer_dns_name', 'us-west-2', 's3://log_path',
                'admin key', None, None, aws_credentials, PORT_DEFAULT_NUM)

    def poke(self):
        pass

    def cache_enable(self, name, restart):
        pass

    def cache_disable(self, name):
        pass

    def cache_clear(self, name):
        pass

    def _get_metrics(self, name, ps_name, is_po, start_time, end_time, period):
        # convert unix timestamps to datetime in UTC
        start_time = datetime.utcfromtimestamp(float(start_time))
        end_time = datetime.utcfromtimestamp(float(end_time))

        num_requests = []
        latency = []
        num_healthy_hosts = []
        num_hosts_in_cluster = []
        num_objects_queryable = []
        num_objects_registered = []
        cache_hits = []
        time_delta = timedelta(seconds=period)
        start = start_time
        while start < end_time:
            num_requests.append({'sum': 1.0, 'time': start, 'unit':'Count'})
            latency.append({'average': 1.0, 'time': start, 'unit':'Seconds'})
            num_healthy_hosts.append({'maximum': 1.0, 'time': start, 'unit':'Count'})
            num_objects_queryable.append({'maximum': 1.0, 'time': start, 'unit':'Count'})
            num_hosts_in_cluster.append({'maximum': 1.0, 'time': start, 'unit':'Count'})
            num_objects_registered.append({'maximum': 1.0, 'time': start, 'unit':'Count'})
            cache_hits.append({'sum': 1.0, 'time': start, 'unit':'Count'})
            start += time_delta

        result = {}
        if not name:
            result = {'requests': num_requests, 'latency': latency}
        elif name == 'num_hosts_in_cluster':
            result = {'num_hosts_in_cluster': num_hosts_in_cluster}
        elif name == 'cache::hits':
            result = {'cache::hits': cache_hits}
        else:
            result = {'test_po::requests': num_requests, 'test_po::latency': latency}

        metrics = {}
        for key in result:
            m = map(dict, sorted(result[key]))
            if len(m) > 0:
                sort_column_name = "time"
                sf = _gl.SArray(m).unpack('').sort(sort_column_name)
                metrics[key] = sf

        return metrics

    def get_metrics_url(self, ps_name):
        return 'test'

    def terminate(self, delete_logs):
        pass

    def _get_all_hosts(self):
        return [TestEc2Env.Host()]

    def launch(self, state_path):
        # Verify we're not overriding another predictive service.
        PredictiveServiceEnvironment._verify_state_path(state_path, self.aws_credentials)

    def get_status(self):
        pass

    def _host_addrs(self):
        return ["localhost"]

class TestDockerEnv(DockerPredictiveServiceEnvironment):

    def __init__(self, load_balancer_dns_name, log_path, admin_key):
        DockerPredictiveServiceEnvironment.__init__(self, load_balancer_dns_name,
            log_path, admin_key)

    def poke(self):
        pass

    def cache_enable(self, name, restart):
        pass

    def cache_disable(self, name):
        pass

    def cache_clear(self, name):
        pass

    def get_metrics_url(self, ps_name):
        return 'test'

    def terminate(self, delete_logs):
        pass

    def _get_all_hosts(self):
        return range(0, self.num_hosts)

    def launch(self, state_path):
        # Verify we're not overriding another predictive service.
        PredictiveServiceEnvironment._verify_state_path(state_path)

    def get_status(self):
        pass

    def _host_addrs(self):
        return ["localhost"]
