"""
Predictive Services provide an easy way for users to turn one or more GraphLab
Create models into a cloud service that can be used to provide predictions or
recommendatations to customer applications.
"""

# We keep track of Predictive Service and Predicitve Object schema version here.
# The way to evolve them is:
# * If Predictive Service schema changes, but Predictive Object schema does not
#   need to change, PREDICTIVE_SERVICE_SCHEMA_VERSION is increased by 1, but
#   PREDICTIVE_OBJECT_SCHEMA_VERSION stays the same.
# * If Predictive Object schema changes, then both Predictive Service and
#   Predictive Object version are bumped up to be PREDICTIVE_SERVICE_SCHEMA_VERSION
#   plus one

# This scheme allows the following:
# *  A Predictive Service with version Vs can always load Predictive Service with
#    version less than or equal to Vs, it will fail otherwise
# *  A Predictive Service with version Vs can always load Predictive Objects with
#    version less than or equal to Vs, it will fail otherwise

# PREDICTIVE OBJECT SCHEMA CHANGES:
'''
v0: Save Predict Object along with dependent GraphLab objects in a zipped file.
    User can only add dependent GraphLab from a python object. (VERSION 1)
v1: Save predictive object definition (query) and dependencies (GraphLab objects)
    separatedly. GraphLab objects are stored in its original format (not compressed)
    in a separate folder provided during save. Support adding dependent GraphLab
    objects from uri (http, s3, local path, etc.) (VERSION 1)
v2: Save Predictive Object along with dependent GraphLab objects in a zipped file.
    All GraphLab objects are pickled together with the query definition. (VERSION 2)
v3: Now saving Predictive Objects as directories with non-GL objects zipped for
    performance. (VERSION 3)
v4: Change Predicitve Service to use docker, as a result, the AMI image, the
    user_data passed and the items stored in state.ini(Execution Environment)
    section are changed. Port that the server listens on is also configurable now,
    so it is an optional parameter in state.ini.
v5: Add model experimentation support, including adding endpoints and serialize
    endpoint policy
'''

PREDICTIVE_SERVICE_SCHEMA_VERSION = 5
PREDICTIVE_OBJECT_SCHEMA_VERSION = 3
LAST_SUPPORTED_PREDICTIVE_SERVICE_SCHEMA_VERSION = 3

_MAX_CREATE_TIMEOUT_SECS = 1800 # 30m

# list of internal predictive objects used by metrics, user should not be able to
# add predictive objects with the names in this list.
INTERNAL_PREDICTIVE_OBJECT_NAMES = ["cache", "gls"]
assert(PREDICTIVE_SERVICE_SCHEMA_VERSION >=  PREDICTIVE_OBJECT_SCHEMA_VERSION)

import _predictive_object
import _model_predictive_object
import _predictive_service

import _policy

