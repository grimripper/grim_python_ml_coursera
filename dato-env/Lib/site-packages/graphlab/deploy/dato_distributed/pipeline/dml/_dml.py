import logging as _logging
import subprocess
import tempfile
import os
import sys
import logging
from copy import copy
from graphlab.util import file_util
from graphlab.util import _make_internal_url
from _dml_logging import LogPrinter, LogStream, get_log_metric_server_address
from _dml_env import DMLRemoteEnvironment
from _dml_env import change_job_logger_level

_log = _logging.getLogger(__name__)

# File indicates commander teminated
COMMANDER_COMPLETE_FILE = 'commander.complete'
COMMANDER_LOG_SERVER_ADDRESS_FILE = 'log_server_address'
COMMANDER_LOG_FILE = 'commander.log'
PROGRESS_LOG_FILE = 'progress.log'
# We scale the startup watiting timeout by 120 secs per worker
INIT_TIMEOUT_PER_WORKER = 120


def WORKER_LOG_FILE(i):
    return 'worker_%d.log' % i


def _sanitize_internal_s3_url(input_str):
    from graphlab.connect.aws import get_credentials
    sanitized_str = input_str
    aws_id, aws_key = get_credentials()
    sanitized_str = sanitized_str.replace(aws_id, '')
    sanitized_str = sanitized_str.replace(aws_key, '')
    return sanitized_str


def dml_exec(function_name, data, env='auto', verbose=True, **kwargs):
    """
    Executes a distributed ml function

    Parameters
    ----------
    function_name : str
        Name of the distributed function to be executed. The function symbol
        must exists in the unity distributed shared library.

    data : dict
        Key value arguments to the function stored in a dictionary

    env : DMLEnvironemnt
        Contains job environment parameters and a job submit function.

    **kwargs : dict
        Additional options.
        See _get_worker_args and _get_commander_args.
            - check_hdfs : {0, 1} Perform sanity check for hdfs read and write
            - startup_timeout : int Timeout in seconds for cluster setup

    Return
    ------
    (success, message, result_path) : bool, str, str
    """
    from graphlab.extensions import dml_function_invocation, init_dml_class_registry
    init_dml_class_registry()

    if env == 'auto':
        env = DMLRemoteEnvironment()

    if not file_util.exists(env.working_dir):
        _log.debug('Creating working directory: %s' % env.working_dir)
        file_util.mkdir(env.working_dir)
    else:
        _log.debug('Using existing working directory: %s' % env.working_dir)

    _log.info('Running distributed execution with %d workers. Working directory: %s' % (env.num_workers, env.working_dir))

    success = False
    message = ""
    result_path = None

    # Job function arguments
    try:
        _log.info('Serializing arguments to %s' % env.working_dir)
        args = dml_function_invocation()
        data_copy = copy(data)
        internal_working_dir = _make_internal_url(env.working_dir)
        data_copy['__base_path__'] = internal_working_dir
        args.from_dict(data_copy, internal_working_dir)
        json_data = args.to_str()

        # sanitize the base path url
    
        sanitized_json_data = json_data
        if file_util.is_s3_path(json_data): 
            sanitized_json_data = _sanitize_internal_s3_url(json_data)   

        _log.info('Serialized arguments: %s' % sanitized_json_data)
    except Exception as e:
        success = False
        message = 'Error serializing arguments. %s' % str(e)
        return (success, message, None)

    # Submit job
    try:
        job = dml_submit(function_name, json_data, env,
                         metric_server_address_file=COMMANDER_LOG_SERVER_ADDRESS_FILE,
                         logprogress_file=PROGRESS_LOG_FILE,
                         **kwargs)
    except KeyboardInterrupt:
        message = 'Canceled by user'
        return (success, message, None)

    _log.info('Waiting for workers to start ... ')
    logprinter = None
    if verbose:
        log_server_address_path = os.path.join(env.working_dir,
                                               COMMANDER_LOG_SERVER_ADDRESS_FILE)
        log_server_address = get_log_metric_server_address(log_server_address_path,
                                                           timeout=INIT_TIMEOUT_PER_WORKER * env.num_workers)
        if len(log_server_address) > 0:
            tmp_log_dir = tempfile.mkdtemp(prefix='graphlab_dml_log_')
            fd_list = []
            logprinter = LogPrinter()
            # Attach log progress stream
            logprinter.add_stream(LogStream(log_server_address + '/progress',
                                            os.path.join(env.working_dir, PROGRESS_LOG_FILE),
                                            sys.stdout))
            # Attach commander log stream
            local_commander_log = open(os.path.join(tmp_log_dir, COMMANDER_LOG_FILE), 'w')
            fd_list.append(local_commander_log)
            logprinter.add_stream(LogStream(log_server_address + '/commander',
                                            os.path.join(env.working_dir, COMMANDER_LOG_FILE),
                                            local_commander_log))
            # Attach worker log streams
            for i in range(env.num_workers):
                local_worker_log = open(os.path.join(tmp_log_dir, WORKER_LOG_FILE(i)), 'w')
                fd_list.append(local_worker_log)
                logprinter.add_stream(LogStream(log_server_address + '/worker%d' % i,
                                                os.path.join(env.working_dir, WORKER_LOG_FILE(i)),
                                                local_worker_log))
            logprinter.start()
            _log.info('Success. Worker logs are avaiable at %s ' % tmp_log_dir)

    _log.debug('Wait for job to finish')
    (success, message) = _wait_and_parse_job_result(job)

    if logprinter:
        logprinter.stop()
        for fd in fd_list:
            fd.close()

    if success:
        try:
            result_path = os.path.join(env.working_dir, env.output_name)
            ret_str = file_util.read(result_path)
            sanitized_ret_str = _sanitize_internal_s3_url(ret_str)
            _log.debug('Deserializing results: %s' % sanitized_ret_str)

            args.from_str(ret_str)
            response = args.to_dict()

            # Check toolkit response for "result" key or "exception" key.
            if 'result' in response:
                return (success, message, response['result'])
            elif 'exception' in response:
                return (False, response['exception'], None)
            else:
                raise ValueError('Invalid toolkit response. Must have "result" or \
                                 "exception" as key')
        except Exception as e:
            success = False
            message = 'Error deserializing results. %s' % str(e)
            return (success, message, None)
    else:
        return (success, message, None)

    #We leave the directory cleanup to the cluster manager


def dml_submit(function_name, str_data, env, **kwargs):
    """
    Executes a distributed ml function

    Parameters
    ----------
    function_name : str
        Name of the distributed function to be executed. The function symbol
        must exists in the unity distributed shared library.

    str_data : str
        Arguments as serialized string to be passed to the distributed
        function.

    env : DMLEnvironemnt
        Contains job environment parameters and a job submit function.

    **kwargs : dict
        Additional options. See _get_worker_args and _get_commander_args.
        - check_hdfs : {0, 1} Perform sanity check for hdfs read and write
        - startup_timeout : int Timeout in seconds for cluster setup

    Return
    ------
    job : map_job
    """
    _log.debug('Submitting job')

    if not file_util.exists(env.working_dir):
        file_util.mkdir(env.working_dir)

    map_job_args = _get_dml_exec_args(function_name, str_data, env,
                                      output_name=env.output_name,
                                      **kwargs)

    _log.debug('job arguments: %s' % str(map_job_args))

    # The following code achieve the same as
    # """return env.submit(subprocess_exec, map_job_args)"""
    # but requires one less container. (Having commander code taking one entire container is wasteful)

    # It uses group_exec and pack the commander function and the first worker
    # function into one map tasks. The rest workers stay the same.
    # group_exec returns a list of results, so the output is a nested list of results,
    # we overload the job.get_results function to flatten the results.

    def commander_exec():
        return lambda: subprocess_exe(**map_job_args[0])

    def worker_exec(i):
        return lambda: subprocess_exe(**map_job_args[i + 1])

    worker_to_function_group = [[worker_exec(i)] for i in range(env.num_workers)]
    worker_to_function_group[0].insert(0, commander_exec())
    job = env.submit(group_exec, [{'lambdas': fgroup} for fgroup in worker_to_function_group])

    # Decoreate the job get_results function to flatten the results
    def flatten_results(packed_results):
        return [item for sublist in packed_results for item in sublist]

    def decorate_with_flatten_results(f_original):
        def f_decorated():
            results = f_original()
            return flatten_results(results)
        return f_decorated
    job.get_results = decorate_with_flatten_results(job.get_results)
    return job


# /**************************************************************************/
# /*                                                                        */
# /*                            Helper function                             */
# /*                                                                        */
# /**************************************************************************/
def subprocess_exe(exe, args, setup=None, teardown=None, out_log_prefix=None, environment_variables=None):
    """
    Wrapper function to execute an external program.
    This function is exception safe, and always catches
    the error.

    Parameters
    ----------
    exe : str
        The command to run
    args : list[str]
        Arguments to passed to the command
    setup : function
        Setup function to run before executing the command
    teardown : function
        Teardown function to run after executing the command
    out_log_prefix: str
        The path prefix to the saved log file.
        If set, the logs will be save to the following locations:
            <prefix>.stdout
            <prefix>.stderr
        and the return value will contain paths to the log files.
        The path can be local or hdfs or s3.

    Return
    ------
    out : dict
        A dictionary containing the following keys:

        success : bool
            True if the command succeeded
        return_code : int
            The return code of the command
        stderr : str
            Path to the stderr log of the process
        stdout : str
            Path to the stdout log of the process
        python_exception : Exception
            Python exception
    """
    import logging
    import os
    ret = {'success': True,
           'return_code': None,
           'stdout': None,
           'stderr': None,
           'python_exception': None}

    # Creates local log files
    try:
        local_log_stdout = tempfile.NamedTemporaryFile(delete=False)
        local_log_stderr = tempfile.NamedTemporaryFile(delete=False)
    except Exception as e:
        ret['success'] = False
        ret['python_exception'] = e

   # Run setup
    try:
        if setup is not None:
            setup()
    except Exception as e:
        ret['success'] = False
        ret['python_exception'] = e

   # Executes the command
    if ret['success']:
        try:
            if environment_variables is not None:
                environment_variables = os.environ.copy().update(environment_variables)
            proc = subprocess.Popen([exe] + args,
                                    stdout=local_log_stdout,
                                    stderr=local_log_stderr,
                                    env=environment_variables)
            proc.communicate()
            ret['success'] = proc.returncode == 0
            ret['return_code'] = proc.returncode
        except Exception as e:
            ret['success'] = False
            ret['python_exception'] = e
        finally:
            try:
                local_log_stdout.close()
                local_log_stderr.close()
                if out_log_prefix is not None:
                    # persistent logfiles. When local log closed,
                    # they will be loaded to the corresponding hdfs or s3 path
                    file_log_stdout = out_log_prefix + '.stdout'
                    file_log_stderr = out_log_prefix + '.stderr'
                    # copy to target log path
                    file_util.copy_from_local(local_log_stdout.name, file_log_stdout)
                    file_util.copy_from_local(local_log_stderr.name, file_log_stderr)
                    ret['stdout'] = file_log_stdout
                    ret['stderr'] = file_log_stderr
                else:
                    ret['stdout'] = open(local_log_stdout.name).read()
                    ret['stderr'] = open(local_log_stderr.name).read()

                os.remove(local_log_stdout.name)
                os.remove(local_log_stderr.name)
            except Exception as e:
                ret['_save_log_exception'] = e
                logging.warn(str(e))

    # Teardown
    if teardown is not None:
        try:
            teardown()
        except Exception as e:
            ret['_tear_down_exception'] = e
            logging.warn(str(e))

    return ret


def group_exec(lambdas):
    """
    Execute the functions in lambdas in parallel.

    Parameters
    ----------
    lambdas : list[function]
      a list of nunary function

    Return
    ------
    out : list
      equivalent to [f() for f in lambdas]
    """
    if len(lambdas) == 0:
        return None
    elif len(lambdas) == 1:
        return [lambdas[0]()]

    from multiprocessing.pool import ThreadPool
    pool = ThreadPool(processes=len(lambdas))
    jobs = [pool.apply_async(f) for f in lambdas]
    return [j.get() for j in jobs]


@change_job_logger_level(logging.WARNING)
def _wait_and_parse_job_result(job):
    """
    Wait for job to finsh, and check the job result

    Return
    ------
    (success, message) : bool, str

      If all job succeeded, return (True, None)
      Exception can happen in the following settings
      - job.get_results() failed
      - job.get_results() succeeded, and the commander result failed
      - job.get_results() succeeded, and worker results failed
    """
    job_results = None
    success = False
    message = None

    try:
        job_results = job.get_results()
    except KeyboardInterrupt:
        try:
            job.cancel()
        except:
            pass
        message = 'Canceled by user'
        return (success, message)
    except:
        message = 'Unable to get job results. Exception message: '
        metrics = job.get_metrics()
        if metrics is not None:
            message += job.get_metrics()['exception_message']
        else:
            message += 'None'
        return (success, message)

    # unpack the results of process groups
    success = all(x['success'] for x in job_results)

    if success:
        return (success, message)

    # Here job failed. We collect the python exception messages
    # from commander and workers.
    error_messages = []
    for i in range(len(job_results)):
        result = job_results[i]
        if not result['success']:
            role = 'Commander' if i == 0 else 'Worker %d' % i
            e = result['python_exception']
            if e is not None:
                msg = '%s Python exception: %s' % (role, e)
            else:
                msg = '%s: C++ exception' % role
                msg += '. Check log file at %s' % result['stderr']
            error_messages.append(msg)
    message = "Unexpected failure\n" + '\n'.join(error_messages)
    return (success, message)


def _get_commander_args(function_name, data, env,
                        cluster_type='standalone_passive',
                        output_name='out',
                        **kwargs):
    args = dict()
    # from arguments
    args['function'] = function_name
    args['args'] = data
    args['num_nodes'] = env.num_workers
    args['working_dir'] = _make_internal_url(env.working_dir)

    # from optional arguments
    args['shared_lib'] = env.LIB_UNITY_DISTRIBUTED_PATH
    args['cluster_type'] = cluster_type
    args['output_name'] = output_name

    # from kwargs, could overwrite existing args
    accepted_args = args.keys() + ['check_hdfs', 'startup_timeout',
                                   'metric_server_address_file',
                                   'metric_server_port']
    for key in accepted_args:
        if key in kwargs:
            args[key] = kwargs[key]

    # return a formated list
    return ['--%s=%s' % (k, v) for k, v in args.iteritems()]


def _get_commander_setup(working_dir):
    """
    Return commander's setup function

    The setup function writes a dummy file to the working directory
    indicating the function started.
    """
    def setup_fun():
        file_util.touch(working_dir + '/commander.started')
    return setup_fun


def _get_commander_teardown(working_dir):
    """
    Return commander's teardown function

    The teardown function writes an empty file
    "commander.complete" to the working directory
    """
    def teardown_fun():
        commander_state_file = working_dir + '/' + COMMANDER_COMPLETE_FILE
        file_util.touch(commander_state_file)
    return teardown_fun


def _get_worker_setup(working_dir, i):
    """
    Return worker's setup function

    The setup function writes a dummy file to the working directory
    indicating the function started.
    """
    def setup_fun():
        file_util.touch(working_dir + '/worker_%d.started' % i)
    return setup_fun


def _get_worker_args(worker_id, working_dir, **kwargs):
    args = dict()
    args['worker_id'] = worker_id
    args['working_dir'] = working_dir

    accepted_args = ['check_hdfs', 'startup_timeout', 'consensus_address'] + args.keys()
    for key in accepted_args:
        if key in kwargs:
            args[key] = kwargs[key]
    return ['--%s=%s' % (k, v) for k, v in args.iteritems()]


def _get_dml_environment_variables():
    env = {}

    # Default values
    env["OMP_NUM_THREADS"] = str(8)
    env["GRAPHLAB_MEMORY_LIMIT_IN_MB"] = str(8 * 1024)
    env["GRAPHLAB_CACHE_FILE_LOCATIONS"] = ''
    env["GRAPHLAB_CACHE_FILE_HDFS_LOCATION"] = ''

    # Overwrite by current environment
    for key, val in env.iteritems():
        if key in os.environ:
            env[key] = os.environ[key]
    return env


def _get_dml_exec_args(function_name, data, env, **kwargs):
    """
    Return a list of map job arguments for distributed exec
    """
    internal_working_dir = _make_internal_url(env.working_dir)
    startup_timeout = INIT_TIMEOUT_PER_WORKER * env.num_workers
    dml_environment_variables = _get_dml_environment_variables()
    map_job_args = [{'exe': env.COMMANDER_PATH,
                     'args': _get_commander_args(function_name, data, env, startup_timeout=startup_timeout, **kwargs),
                     'setup': _get_commander_setup(env.working_dir),
                     'teardown': _get_commander_teardown(env.working_dir),
                     'out_log_prefix': os.path.join(env.working_dir, COMMANDER_LOG_FILE),
                     'environment_variables': dml_environment_variables
                     }]

    for i in range(env.num_workers):
        worker_args = {'exe': env.WORKER_PATH,
                       'setup': _get_worker_setup(env.working_dir, i),
                       'args': _get_worker_args(i, internal_working_dir, startup_timeout=startup_timeout, **kwargs),
                       'out_log_prefix': os.path.join(env.working_dir, WORKER_LOG_FILE(i)),
                       'environment_variables': dml_environment_variables}
        map_job_args.append(worker_args)
    return map_job_args
