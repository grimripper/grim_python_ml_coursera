import logging
import os

# global variable for execution environment
__DISTRIBUTED_EXECUTION_ENVIRONMENT__  = None


def get_distributed_execution_environment():
    '''
    Get current distributed execution environment

    Returns
    -------
    The default distributed execution environment, if not explicitly set by user, then the default
    is local (None), otherwise, it is a cluster execution environment

    '''
    return __DISTRIBUTED_EXECUTION_ENVIRONMENT__


def set_distributed_execution_environment(environment, timeout_in_seconds=600):
    '''
    Change default execution environment to be a Dato Distributed cluster. After this
    method is called, operations like SFrame, SArray, SGraph and model training
    may perform in the given Distributed Cluster.

    The Cluster object may be a Hadoop Cluster or EC2 Cluster. For EC2 Cluster, the
    cluster must have been started using:

        >>> cluster = gl.deploy.ec2_cluster.create(...)

    Or if the EC2 Cluster is already launched, it may be retrieved through:

        >>> cluster = gl.deploy.ec2_cluster.load(...)

    For Hadoop Cluster, the cluster must have been installed into your Hadoop
    system, and the appropriate path to a HDFS needs to be provided to load the
    cluster into GraphLab Create:

        >>> cluster = gl.deploy.hadoop_cluster.create(...)

    If the Hadoop Cluster execution environment has been created and saved before,
    it can be loaded by:

        >>> cluster = gl.deploy.hadoop_cluster.load(...)

    Parameters
    -----------

    environment : HadoopCluster | Ec2Cluster
        This can be either a EC2 Cluster or Hadoop Cluster object

    timeout_in_seconds : int, optional
        Timeout in seconds. The execution environment will shutdown automatically if
        it is in idle for given timeout. If not given, the default value is 10
        minutes (600 seconds). After the execution environment is timed out, it will
        automatically be spinned up again next time you submit job.

    See Also
    ---------
    graphlab.clear_distributed_execution_environment,
    graphlab.deploy.hadoop_cluster.create,
    graphlab.deploy.hadoop_cluster.load,
    graphlab.deploy.ec2_cluster.create,
    graphlab.deploy.ec2_cluster.load

    '''
    global __DISTRIBUTED_EXECUTION_ENVIRONMENT__

    from graphlab.deploy.ec2_cluster import Ec2Cluster
    from graphlab.deploy.hadoop_cluster import HadoopCluster
    from graphlab.deploy._dml_cluster import create

    if not isinstance(environment, Ec2Cluster) and \
       not isinstance(environment, HadoopCluster):
        raise TypeError("'environment' must be either a type of Ec2Cluster or HadoopCluster")

    if __DISTRIBUTED_EXECUTION_ENVIRONMENT__ is not None:
        raise RuntimeError('There is already a cluster set up as default execution '
            'environment. Use "graphlab.clear_distributed_execution_environment" to clear previous one.')

    if not isinstance(timeout_in_seconds, int):
        raise TypeError('timeout_in_seconds parameter has to be an integer.')

    if timeout_in_seconds <= 0:
        raise ValueError('timeout_in_seconds has to be greater than 0')

    ee = create(environment, timeout_in_seconds)

    __DISTRIBUTED_EXECUTION_ENVIRONMENT__ = ee


def clear_distributed_execution_environment():
    '''
    Clear current execution environment and revert to default execution environment,
    which is Local.

    After this method is called, all GraphLab Create operations will be performed
    in your local computer.

    See Also
    --------
    graphlab.set_distributed_execution_environment,
    graphlab.deploy.hadoop_cluster.create,
    graphlab.deploy.hadoop_cluster.load,
    graphlab.deploy.ec2_cluster.create,
    graphlab.deploy.ec2_cluster.load


    '''
    global __DISTRIBUTED_EXECUTION_ENVIRONMENT__

    if __DISTRIBUTED_EXECUTION_ENVIRONMENT__ is not None:
        __DISTRIBUTED_EXECUTION_ENVIRONMENT__.stop()

    __DISTRIBUTED_EXECUTION_ENVIRONMENT__ = None
