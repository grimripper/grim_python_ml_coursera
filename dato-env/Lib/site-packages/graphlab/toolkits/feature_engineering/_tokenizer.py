import graphlab as _gl
from graphlab.toolkits._model import _get_default_options_wrapper
from graphlab.toolkits.feature_engineering._feature_engineering import Transformer
from graphlab.toolkits._internal_utils import _toolkit_repr_print
from graphlab.toolkits._internal_utils import _precomputed_field

import _internal_utils
from _doc_utils import republish_docs

_fit_examples_doc = '''
            Our current tokenizer is regex-based and does not need to be fit.
'''

_fit_transform_examples_doc = '''
            See docs from "transform".
'''

_transform_examples_doc = '''
            >>> sf = graphlab.SFrame({'docs': ["This is a document!",
                                     "This one's also a document."]})
            >>> tokenizer = graphlab.feature_engineering.Tokenizer('docs')
            >>> tokenized_sf = tokenizer.transform(sf)
            Columns:
                    docs    string

            Rows: 2

            Data:
            +-------------------------------+
            |             docs              |
            +-------------------------------+
            | ['This', 'is', 'a', 'docum... |
            | ['This', 'one', '\'s', 'al... |
            +-------------------------------+
            [2 rows x 1 columns]
'''

@republish_docs
class Tokenizer(Transformer):
    '''
    Tokenizing is a method of breaking natural language text into its smallest 
    standalone and meaningful components (in English, usually space-delimited 
    words, but not always).

    This is often accomplished by applying regular expressions to the natural 
    language text to capture a high-recall set of valid text patterns.

    Parameters
    ----------
    features : list[str] , optional
        Column names of features to be transformed. If None, all columns are
        selected.

    exclude : list[str] | str | None, optional
        Column names of features to be ignored in transformation. Can be string
        or list of strings. Either 'exclude' or 'features' can be passed, but
        not both.

    Returns
    -------
    out : Tokenizer
        A Tokenizer object which is initialized with the defined parameters.

    Examples
    --------

    .. sourcecode:: python

        from graphlab.toolkits.feature_engineering import *

        # Construct a tokenizer with default options.
        >>> sf = graphlab.SFrame({'docs': ["This is a document!", "This one's also a document."]})
        >>> tokenizer = graphlab.feature_engineering.create(sf, Tokenizer())

        # Transform the data using the tokenizer.
        >>> tokenized_sf = tokenizer.transform(sf)

        # Tokenize only a single column 'docs'.
        >>> tokenizer = graphlab.feature_engineering.create(sf,
                                    Tokenizer(features = ['docs']))

        # Tokenize all columns except 'docs'.
        >>> tokenizer = graphlab.feature_engineering.create(sf,
                                    Tokenizer(excluded_features = ['a']))


    '''

    _fit_examples_doc = _fit_examples_doc
    _transform_examples_doc = _transform_examples_doc
    _fit_transform_examples_doc = _fit_transform_examples_doc



    get_default_options = staticmethod(_get_default_options_wrapper(
            '_Tokenizer', 'toolkits.feature_engineering._tokenizer', 'Tokenizer', True))

    def __init__(self, features=None, excluded_features=None):

        # Process and make a copy of the features, exclude.
        _features, _exclude = _internal_utils.process_features(features, excluded_features)

        # Set up options
        opts = {
          'features': features
        }
        if _exclude:
            opts['exclude'] = True
            opts['features'] = _exclude
        else:
            opts['exclude'] = False
            opts['features'] = _features

        # Initialize object
        proxy = _gl.extensions._Tokenizer()
        proxy.init_transformer(opts)
        super(Tokenizer, self).__init__(proxy, self.__class__)

    def _get_summary_struct(self):
        """
        Returns a structured description of the model, including (where relevant)
        the schema of the training data, description of the training data,
        training statistics, and model hyperparameters.

        Returns
        -------
        sections : list (of list of tuples)
            A list of summary sections.
              Each section is a list.
                Each item in a section list is a tuple of the form:
                  ('<label>','<field>')
        section_titles: list
            A list of section titles.
              The order matches that of the 'sections' object.
        """
        _features = _precomputed_field(
            _internal_utils.pretty_print_list(self.get('features')))
        _exclude = _precomputed_field(
            _internal_utils.pretty_print_list(self.get('excluded_features')))


        fields = [
            ("Features", _features),
            ("Excluded_features", _exclude)
        ]
        section_titles = ['Model fields']

        return ([fields], section_titles)

    def __repr__(self):
        (sections, section_titles) = self._get_summary_struct()
        return _toolkit_repr_print(self, sections, section_titles, width=30)
