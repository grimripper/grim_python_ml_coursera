"""
This package contains methods for evaluating the quality of predictive machine
learning models.
"""
import graphlab.connect as _mt
import graphlab as _graphlab
from graphlab.toolkits._internal_utils import _raise_error_if_not_sarray

def _supervised_evaluation_error_checking(targets, predictions):
    """
        Perform basic error checking for the evaluation metrics. Check
        types and sizes of the inputs.

    """
    _raise_error_if_not_sarray(targets, "targets")
    _raise_error_if_not_sarray(predictions, "predictions")
    assert targets.size() == predictions.size(), \
         "Input SArrays 'targets' and 'predictions' must be of the same length."

def max_error(targets, predictions):
    r"""
    Compute the maximum absolute deviation between two SArrays.

    Parameters
    ----------
    targets : SArray[float or int]
        An Sarray of ground truth target values.

    predictions : SArray[float or int]
        The prediction that corresponds to each target value.
        This vector must have the same length as ``targets``.

    Returns
    -------
    out : float
        The maximum absolute deviation error between the two SArrays.

    See Also
    --------
    rmse

    Notes
    -----
    The maximum absolute deviation between two vectors, x and y, is defined as:

    .. math::

        \textrm{max error} = \max_{i \in 1,\ldots,N} \|x_i - y_i\|

    Examples
    --------
    >>> targets = graphlab.SArray([3.14, 0.1, 50, -2.5])
    >>> predictions = graphlab.SArray([3.1, 0.5, 50.3, -5])
    >>> graphlab.evaluation.max_error(targets, predictions)
    2.5
    """

    _mt._get_metric_tracker().track('evaluation.max_error')
    _supervised_evaluation_error_checking(targets, predictions)
    return _graphlab.extensions._supervised_streaming_evaluator(targets,
                                                    predictions, "max_error")

def rmse(targets, predictions):
    r"""
    Compute the root mean squared error between two SArrays.

    Parameters
    ----------
    targets : SArray[float or int]
        An Sarray of ground truth target values.

    predictions : SArray[float or int]
        The prediction that corresponds to each target value.
        This vector must have the same length as ``targets``.

    Returns
    -------
    out : float
        The RMSE between the two SArrays.

    See Also
    --------
    max_error

    Notes
    -----
    The root mean squared error between two vectors, x and y, is defined as:

    .. math::

        RMSE = \sqrt{\frac{1}{N} \sum_{i=1}^N (x_i - y_i)^2}

    References
    ----------
    - `Wikipedia - root-mean-square deviation
      <http://en.wikipedia.org/wiki/Root-mean-square_deviation>`_

    Examples
    --------
    >>> targets = graphlab.SArray([3.14, 0.1, 50, -2.5])
    >>> predictions = graphlab.SArray([3.1, 0.5, 50.3, -5])
    >>> graphlab.evaluation.rmse(targets, predictions)
    1.2749117616525465
    """

    _mt._get_metric_tracker().track('evaluation.rmse')
    _supervised_evaluation_error_checking(targets, predictions)
    return _graphlab.extensions._supervised_streaming_evaluator(targets,
                                                          predictions, "rmse")
def confusion_matrix(targets, predictions):
    r"""
    Compute the confusion matrix for classifier predictions.

    Parameters
    ----------
    targets : SArray
        Ground truth class labels.

    predictions : SArray
        The prediction that corresponds to each target value.
        This vector must have the same length as ``targets``.

    Returns
    -------
    out : SFrame
        An SFrame containing counts for 'target_label', 'predicted_label' and
        'count' corresponding to each pair of true and predicted labels.

    See Also
    --------
    accuracy

    Examples
    --------
    >>> targets = graphlab.SArray([0, 1, 1, 0])
    >>> predictions = graphlab.SArray([0.1, 0.35, 0.7, 0.99])
    >>> graphlab.evaluation.confusion_matrix(targets, predictions)
    """

    _mt._get_metric_tracker().track('evaluation.confusion_matrix')
    _supervised_evaluation_error_checking(targets, predictions)
    return _graphlab.extensions._supervised_streaming_evaluator(targets,
                                              predictions, "confusion_matrix")

def accuracy(targets, predictions):
    r"""
    Compute the proportion of correct predictions.

    Parameters
    ----------
    targets : SArray
        Ground truth class labels.

    predictions : SArray
        The prediction that corresponds to each target value.
        This vector must have the same length as ``targets``.

    Returns
    -------
    out : float
        The ratio of the number of correct classifications and the total number
        of data points.

    See Also
    --------
    confusion_matrix

    Examples
    --------
    >>> targets = graphlab.SArray([0, 1, 1, 0])
    >>> predictions = graphlab.SArray([0.1, 0.35, 0.7, 0.99])
    >>> graphlab.evaluation.accuracy(targets, predictions)
    """
    _mt._get_metric_tracker().track('evaluation.accuracy')
    _supervised_evaluation_error_checking(targets, predictions)
    return _graphlab.extensions._supervised_streaming_evaluator(targets,
                                                    predictions, "accuracy")

def roc_curve(targets, predictions):
    r"""
    Compute an ROC curve for the given targets and predictions. This only
    supports binary classification, i.e., targets is an SArray with two unique
    values.

    Parameters
    ----------
    targets : SArray
        An SArray containing the observed values. If numeric, this should
        consist of either 0 or 1; For str type, the first unique value is
        considered a "negative" example, and the others as "positive" examples.

    predictions : SArray
        The prediction that corresponds to each target value.
        This vector must have the same length as ``targets``.

    Returns
    -------
    out : SFrame
        Each row represents the predictive performance when using a given
        cutoff threshold, where all predictions above that cutoff are considered
        "positive". Four columns are used to describe the performance:

        **tpr**: true positive rate, the number of true positives divided by
        the number of positives.
        **fpr**: false positive rate, the number of false positives divided
        by the number of negatives.
        **p**: total number of positive values.
        **n**: total number of negative values.

    See Also
    --------
    confusion_matrix

    References
    ----------
    An introduction to ROC analysis. Tom Fawcett. <https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf>

    Examples
    --------
    >>> targets = graphlab.SArray([0, 1, 1, 0])
    >>> predictions = graphlab.SArray([0.1, 0.35, 0.7, 0.99])
    >>> graphlab.evaluation.roc_curve(targets, predictions)
    """
    _mt._get_metric_tracker().track('evaluation.roc_curve')
    _supervised_evaluation_error_checking(targets, predictions)

    # For str targets, first target is negative example.
    if targets.dtype() == str:
        targets = (targets != targets[0])

    result = _graphlab.extensions._supervised_streaming_evaluator(targets,
                                                    predictions, "roc_curve")
    assert 'tpr' in result.column_names() and 'fpr' in result.column_names(), \
        "ROC curve results malformed. Please report this on the Dato forum."
    for col in ['tpr', 'fpr']:
        assert all(result[col] >= 0 * result[col] <= 1), \
        "ROC curve results malformed. Please report this on the Dato forum."

    return result
