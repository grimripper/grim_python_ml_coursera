"""
The GraphLab Create distances module provides access to the standard distance
functions and utilities for working with composite distances. Distance functions
are used in all toolkits based on a nearest neighbors search, including
:mod:`~graphlab.toolkits.nearest_neighbors` itself,
:mod:`nearest_neighbor_classifier` , and :mod:`nearest_neighbor_deduplication`.

    .. warning::

        The 'dot_product' distance is deprecated and will be removed in future
        versions of GraphLab Create. Please use 'logistic' distance instead,
        although note that this is more than a name change; it is a *different*
        transformation of the dot product of two vectors. Please see the
        distances module documentation for more details.

**Standard distance functions** measure the dissimilarity between two data
points consisting of only a single type.

    - *Euclidean*, *squared Euclidean*, *Manhattan*, *cosine*, and *dot product*
      distances work for integer and floating point data, which can be thought
      of a vectors.

    - These distances, as well as the *Jaccard* and *weighted Jaccard* distances
      work on data contained in dictionaries.

    - The *Levenshtein* distance works for string data, although another
      strategy that often works well is to turn strings into dictionaries with
      the :py:func:`graphlab.text_analytics.count_ngrams` function then use
      Jaccard or weighted Jaccard distance.

These functions may be passed to a model by specifying either the name or the
handle of the function in this module. For example, suppose we have the
following SFrame of data:

>>> sf = graphlab.SFrame({'X1': [0.98, 0.62, 0.11, 1.4, 0.88],
...                       'X2': [0.69, 0.58, 0.36, 1.23, 0.2],
...                       'species': ['cat', 'dog', 'elephant', 'fossa', 'giraffe']})

To find the nearest neighbors of each row, we create a nearest neighbors model,
and we have to indicate how we want to measure the distance between any pair of
rows. Suppose we only want to use the numeric features 'X1' and 'X2'; then we
can use any of the standard numeric distances.

>>> m = graphlab.nearest_neighbors.create(sf, features=['X1', 'X2'],
...                                       distance='euclidean')
...
>>> m2 = graphlab.nearest_neighbors.create(sf, features=['X1', 'X2'],
...                                        distance=graphlab.distances.euclidean)

**Composite distances** provide greater flexibility by defining a measure of the
dissimilarity between two data points that may contain *different* data types. A
composite distance is simply a list, each of whose elements matches some set of
features to a standard distance function. Each of these standard distances is
then multiplied by a weight factor, then summed to get a final distance score.

In our example above, suppose we want to use both the numeric features and the
string species name to measure the distance between two rows of data. In
particular, we want to again measure the numeric feature dissimilarity with
Euclidean distance, and the species name difference with Levenshtein distance.
The former will be multiplied by 2 to give it greater weight, while the string
distance is multiplied by 0.3 to reduce its impact.

>>> dist_spec = [[('X1', 'X2'), 'euclidean', 2],
...              [('species',), 'levenshtein', 0.3]]
...
>>> m3 = graphlab.nearest_neighbors.create(sf, distance=dist_spec)

Note that we no longer need to specify the features, because the composite
distance already contains that information. Models that use composite distances
store the specification so it can be retrieved, modified, and reused. For
example, suppose we decided that the Levenshtein distance on species name should
have a higher weight. We don't have to construct a composite distance from
scratch; we can modify the one we used previously.

>>> dist_spec2 = m3['composite_params']
>>> dist_spec2[1][2] = 0.7
>>> m4 = graphlab.nearest_neighbors.create(sf, distance=dist_spec2)

Specifying a composite distance can be tricky. Often we have a general sense for
which features and standard distances to use, but only a vague idea how much
each component should be weighted. The `compute_composite_distance` function can
help with this by evaluating a composite distance on two specific data points.

>>> d1 = graphlab.distances.compute_composite_distance(dist_spec, sf[0], sf[1])
>>> d2 = graphlab.distances.compute_composite_distance(dist_spec, sf[0], sf[2])
>>> print "d1:", d1, "d2:", d2
d1: 1.65286120899 d2: 3.66096749031

This tells that under our first composite distance, the 'cat' and 'dog' data
points are closer than the 'cat' and 'elephant' data points.

See Also
--------
:py:func:`graphlab.nearest_neighbors.create`
:py:func:`graphlab.nearest_neighbor_classifier.create`
:py:func:`graphlab.clustering.dbscan.create`
:py:func:`graphlab.data_matching.nearest_neighbor_deduplication.create`
"""

__all__ = ['_distances', '_util']

from _distances import euclidean, squared_euclidean, manhattan
from _distances import cosine, dot_product, transformed_dot_product, jaccard, weighted_jaccard
from _distances import levenshtein

import _util
from _util import compute_composite_distance
from _util import build_address_distance
